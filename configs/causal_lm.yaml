raw_inputs:
  - "/home/clay/research/kaggle/kaggle_llm/data/wikipedia_pages2/0_to_25000.parquet"
  - "/home/clay/research/kaggle/kaggle_llm/data/wikipedia_pages2/25000_to_50000.parquet"
  - "/home/clay/research/kaggle/kaggle_llm/data/wikipedia_pages2/50000_to_75000.parquet"
  - "/home/clay/research/kaggle/kaggle_llm/data/wikipedia_pages2/75000_to_100000.parquet"
  - "/home/clay/research/kaggle/kaggle_llm/data/wikipedia_pages2/100000_to_125000.parquet"
  - "/home/clay/research/kaggle/kaggle_llm/data/wikipedia_pages2/125000_to_131049.parquet"

inputs:
  - "/home/clay/research/kaggle/kaggle_llm/data/data_dumps/wikipedia_sentences/0_to_25000.csv"
#  - "/home/clay/research/kaggle/kaggle_llm/data/data_dumps/wikipedia_sentences/25000_to_50000.csv"
#  - "/home/clay/research/kaggle/kaggle_llm/data/data_dumps/wikipedia_sentences/50000_to_75000.csv"
#  - "/home/clay/research/kaggle/kaggle_llm/data/data_dumps/wikipedia_sentences/75000_to_100000.csv"
#  - "/home/clay/research/kaggle/kaggle_llm/data/data_dumps/wikipedia_sentences/100000_to_125000.csv"
#  - "/home/clay/research/kaggle/kaggle_llm/data/data_dumps/wikipedia_sentences/125000_to_131049.csv"

use_8bit: false
block_size: 50
cutoff_len: 120
transformer_class: "AutoModelForMaskedLM"
load_from: "microsoft/deberta-v3-large"
batch_size: 8
use_peft: false

#use_8bit: true
#block_size: 50
#cutoff_len: 120
#transformer_class: "AutoModelForCausalLM"
#load_from: "/home/clay/research/kaggle/kaggle_llm/data/pretrained_models/meta_llama2_7b"
#batch_size: 2
#use_peft: true


#report_to: ["wandb"]
report_to: []
peft_lr: 5e-5
peft_class: AdaLoraConfig
peft_kwargs:
  init_r: 1200
  target_r: 1024
  beta1: 0.85
  beta2: 0.85
  tinit: 200
  tfinal: 1000
  deltaT: 10
  lora_alpha: 256
  lora_dropout: 0.1
  target_modules: ["q_proj", "v_proj"]
  orth_reg_weight: 0.5
