{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a88132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "\n",
    "\n",
    "from kaggle_llm.core import (\n",
    "    ROOT_PATH,\n",
    "    count_words,\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import *\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pathlib import Path\n",
    "import blingfire as bf\n",
    "import argparse\n",
    "import torch\n",
    "import faiss\n",
    "import json\n",
    "import yaml\n",
    "\n",
    "\n",
    "too_long_prompt_wc = 250\n",
    "context_cutoff_len = 150\n",
    "\n",
    "\n",
    "def split_text_into_chunks(text: str, chunk_size: int) -> List[str]:\n",
    "    tokens = [i for i in text.split() if len(i) > 0]\n",
    "    start = 0\n",
    "    end = chunk_size\n",
    "    chunks = []\n",
    "    while end < len(tokens):\n",
    "        chunks.append(\" \".join(tokens[start: end]))\n",
    "        start += chunk_size\n",
    "        end += chunk_size\n",
    "    return chunks\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_sentence_embeddings(\n",
    "        wiki_df_path: Union[str, Path],\n",
    "        model: SentenceTransformer\n",
    "):\n",
    "    wiki_df = pd.read_csv(wiki_df_path, index_col=0)\n",
    "\n",
    "    wc_per_page = wiki_df.groupby(\"page\")[[\"word_count\"]].sum().sort_values(\"word_count\", ascending=False)\n",
    "    black_list = list(wc_per_page.loc[\n",
    "        (wc_per_page[\"word_count\"] > 10000)\n",
    "        | (wc_per_page.index.map(lambda x: \"list of equations\" in x.lower()))\n",
    "    ].index)\n",
    "    print(json.dumps(black_list, indent=4))\n",
    "\n",
    "    filtered_wiki_df = wiki_df.loc[~wiki_df[\"page\"].isin(black_list), :].copy()\n",
    "    print(len(wiki_df), len(filtered_wiki_df))\n",
    "\n",
    "    batch_size = 16\n",
    "    sentences_df = []\n",
    "\n",
    "    print(\"extracting sentences:\")\n",
    "    for _, row in tqdm(filtered_wiki_df.iterrows(), total=len(filtered_wiki_df)):\n",
    "        _, sentence_offsets = bf.text_to_sentences_and_offsets(row[\"text\"])\n",
    "        for start_idx, end_idx in sentence_offsets:\n",
    "            is_long_enough = (end_idx - start_idx) > 3\n",
    "            is_math = \"\\\\\" in row[\"text\"][start_idx: end_idx]  # leads to excessive tokens\n",
    "            if is_long_enough and (not is_math):\n",
    "                sentences_df.append({\n",
    "                    \"page\": row[\"page\"],\n",
    "                    \"i_sentence\": len(sentences_df),\n",
    "                    \"text\": row[\"text\"][start_idx: end_idx],\n",
    "                    \"topic\": row[\"page\"],\n",
    "                })\n",
    "\n",
    "    sentences_df = pd.DataFrame.from_records(sentences_df)\n",
    "    print(f\"extracted: {len(sentences_df)} sentences\")\n",
    "\n",
    "    print(f\"dropping too long sentences\")\n",
    "    pass_indices = sentences_df.loc[sentences_df[\"text\"].apply(count_words) < context_cutoff_len, \"text\"].index\n",
    "    print(f\"keeping {len(pass_indices) / len(sentences_df) * 100} % at cutoff {context_cutoff_len}\")\n",
    "    sentences_df = sentences_df.loc[pass_indices, :].reset_index().copy()\n",
    "\n",
    "    print(\"computing wiki embeddings:\")\n",
    "    sentence_embeddings = model.encode(\n",
    "        sentences_df[\"text\"].values,\n",
    "        batch_size=batch_size,\n",
    "        device=\"cuda\",\n",
    "        show_progress_bar=True,\n",
    "        convert_to_tensor=True,\n",
    "        normalize_embeddings=True,\n",
    "    ).half()\n",
    "    sentence_embeddings = sentence_embeddings.detach().cpu().numpy()\n",
    "\n",
    "    sentence_index = faiss.IndexFlatIP(sentence_embeddings.shape[1])\n",
    "    sentence_index.add(sentence_embeddings)\n",
    "    print(f\"{sentence_index.ntotal = }\")\n",
    "    return sentences_df, sentence_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837c07f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_path = \"/home/clay/research/kaggle/kaggle_llm/data/data_dumps/context_df/more_questions_raw_questions.csv\"\n",
    "wiki_df_path = \"/home/clay/research/kaggle/kaggle_llm/data/physics_pages_list/physics_pages_formatted.csv\"\n",
    "sentence_model = \"/home/clay/research/kaggle/kaggle_llm/data/sentence_transformer_model\"\n",
    "k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70866b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(sentence_model, device=\"cuda\")\n",
    "model.max_seq_length = 384\n",
    "model = model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2457b6de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wiki_df_path = Path(wiki_df_path)\n",
    "print(f\"computing wiki embeddings\")\n",
    "sentences_df, sentence_index = get_sentence_embeddings(wiki_df_path, model)\n",
    "batch_size = 16\n",
    "print(f\"computed wiki embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c379eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_df = pd.read_csv(train_df_path)\n",
    "if \"id\" in train_df.columns:\n",
    "    train_df = train_df.drop(\"id\", axis=1)\n",
    "\n",
    "train_df[\"prompt_and_answer\"] = (\n",
    "        train_df[\"prompt\"]\n",
    "        + \" \" + train_df[\"A\"]\n",
    "        + \" \" + train_df[\"B\"]\n",
    "        + \" \" + train_df[\"C\"]\n",
    "        + \" \" + train_df[\"D\"]\n",
    "        + \" \" + train_df[\"E\"]\n",
    ")\n",
    "question_embeddings = model.encode(\n",
    "    train_df[\"prompt_and_answer\"].values,\n",
    "    batch_size=batch_size,\n",
    "    device=\"cuda\",\n",
    "    show_progress_bar=True,\n",
    "    convert_to_tensor=True,\n",
    "    normalize_embeddings=True,\n",
    ").half()\n",
    "question_embeddings = question_embeddings.detach().cpu().numpy()\n",
    "\n",
    "distance, indices = sentence_index.search(question_embeddings, k)\n",
    "\n",
    "for i in range(k):\n",
    "    train_df[f\"context_{i}_idx\"] = indices[:, i]\n",
    "\n",
    "for i in range(k):\n",
    "    # this is different, we just join the topic here\n",
    "    train_df[f\"context_topic_{i}\"] = train_df.join(\n",
    "        sentences_df[\"topic\"],\n",
    "        on=f\"context_{i}_idx\",\n",
    "        how=\"left\",\n",
    "        rsuffix=f\"_context_{i}\"\n",
    "    )[f\"topic_context_{i}\"]\n",
    "    train_df[f\"context_topic_{i}\"] = train_df[f\"context_topic_{i}\"].apply(\n",
    "        lambda x: x.lower().replace(\" \", \"_\")\\\n",
    "            .replace(\"/\", \"_\")\\\n",
    "            .replace(\"'\", \"_\")\\\n",
    "            .replace(\"(\", \"_\")\\\n",
    "            .replace(\")\", \"_\")\\\n",
    "            .replace(\":\", \"_\")\n",
    "    )\n",
    "\n",
    "assert not train_df[\"prompt\"].isna().any(), f\"{train_df_path} contains {train_df['prompt'].isna().sum()} dumbass prompts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bf3795",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a79c7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "for i in range(k):\n",
    "    accs.append((train_df[\"topic\"] == train_df[f\"context_topic_{i}\"]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b89eb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ccb545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "ConfusionMatrixDisplay(\n",
    "    confusion_matrix(\n",
    "        train_df[\"topic\"],\n",
    "        train_df[\"context_topic_0\"]\n",
    "    )\n",
    ").plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
