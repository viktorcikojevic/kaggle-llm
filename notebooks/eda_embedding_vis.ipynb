{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389c485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "\n",
    "\n",
    "from kaggle_llm.core import (\n",
    "    ROOT_PATH,\n",
    "    count_words,\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import *\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pathlib import Path\n",
    "import blingfire as bf\n",
    "import argparse\n",
    "import torch\n",
    "import faiss\n",
    "import json\n",
    "import yaml\n",
    "\n",
    "\n",
    "too_long_prompt_wc = 250\n",
    "context_cutoff_len = 150\n",
    "\n",
    "\n",
    "def split_text_into_chunks(text: str, chunk_size: int) -> List[str]:\n",
    "    tokens = [i for i in text.split() if len(i) > 0]\n",
    "    start = 0\n",
    "    end = chunk_size\n",
    "    chunks = []\n",
    "    while end < len(tokens):\n",
    "        chunks.append(\" \".join(tokens[start: end]))\n",
    "        start += chunk_size\n",
    "        end += chunk_size\n",
    "    return chunks\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_sentence_embeddings(\n",
    "        wiki_df_path: Union[str, Path],\n",
    "        model: SentenceTransformer\n",
    "):\n",
    "    wiki_df = pd.read_csv(wiki_df_path, index_col=0)\n",
    "\n",
    "    wc_per_page = wiki_df.groupby(\"page\")[[\"word_count\"]].sum().sort_values(\"word_count\", ascending=False)\n",
    "    black_list = list(wc_per_page.loc[\n",
    "        (wc_per_page[\"word_count\"] > 10000)\n",
    "        | (wc_per_page.index.map(lambda x: \"list of equations\" in x.lower()))\n",
    "    ].index)\n",
    "    print(json.dumps(black_list, indent=4))\n",
    "\n",
    "    filtered_wiki_df = wiki_df.loc[~wiki_df[\"page\"].isin(black_list), :].copy()\n",
    "    print(len(wiki_df), len(filtered_wiki_df))\n",
    "\n",
    "    batch_size = 16\n",
    "    sentences_df = []\n",
    "\n",
    "    print(\"extracting sentences:\")\n",
    "    for _, row in tqdm(filtered_wiki_df.iterrows(), total=len(filtered_wiki_df)):\n",
    "        _, sentence_offsets = bf.text_to_sentences_and_offsets(row[\"text\"])\n",
    "        for start_idx, end_idx in sentence_offsets:\n",
    "            is_long_enough = (end_idx - start_idx) > 3\n",
    "            is_math = \"\\\\\" in row[\"text\"][start_idx: end_idx]  # leads to excessive tokens\n",
    "            if is_long_enough and (not is_math):\n",
    "                sentences_df.append({\n",
    "                    \"page\": row[\"page\"],\n",
    "                    \"i_sentence\": len(sentences_df),\n",
    "                    \"text\": row[\"text\"][start_idx: end_idx],\n",
    "                })\n",
    "\n",
    "    sentences_df = pd.DataFrame.from_records(sentences_df)\n",
    "    print(f\"extracted: {len(sentences_df)} sentences\")\n",
    "\n",
    "    print(f\"dropping too long sentences\")\n",
    "    pass_indices = sentences_df.loc[sentences_df[\"text\"].apply(count_words) < context_cutoff_len, \"text\"].index\n",
    "    print(f\"keeping {len(pass_indices) / len(sentences_df) * 100} % at cutoff {context_cutoff_len}\")\n",
    "    sentences_df = sentences_df.loc[pass_indices, :].reset_index().copy()\n",
    "\n",
    "    print(\"computing wiki embeddings:\")\n",
    "    sentence_embeddings = model.encode(\n",
    "        sentences_df[\"text\"].values,\n",
    "        batch_size=batch_size,\n",
    "        device=\"cuda\",\n",
    "        show_progress_bar=True,\n",
    "        convert_to_tensor=True,\n",
    "        normalize_embeddings=True,\n",
    "    ).half()\n",
    "    sentence_embeddings = sentence_embeddings.detach().cpu().numpy()\n",
    "\n",
    "    sentence_index = faiss.IndexFlatIP(sentence_embeddings.shape[1])\n",
    "    sentence_index.add(sentence_embeddings)\n",
    "    print(f\"{sentence_index.ntotal = }\")\n",
    "    return sentences_df, sentence_index, sentence_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c679f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_path = \"/home/clay/research/kaggle/kaggle_llm/data/physics_pages_list/physics_pages_formatted.csv\"\n",
    "sentence_model = \"/home/clay/research/kaggle/kaggle_llm/data/sentence_transformer_model\"\n",
    "model = SentenceTransformer(sentence_model, device=\"cuda\")\n",
    "model.max_seq_length = 384\n",
    "model = model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8deb809",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_df, s_idx, s_embs = get_sentence_embeddings(\n",
    "    wiki_path,\n",
    "    model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49e4024",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b89428",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "niter = 200\n",
    "verbose = True\n",
    "kmeans = faiss.Kmeans(\n",
    "    s_embs.shape[1], \n",
    "    k, \n",
    "    niter=niter, \n",
    "    verbose=verbose\n",
    ")\n",
    "kmeans.train(s_embs)\n",
    "kmeans_index = faiss.IndexFlatIP(s_embs.shape[1])\n",
    "kmeans_index.add(kmeans.centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106aaf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, neighbours = kmeans_index.search(s_embs, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23933794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "\n",
    "plt.close()\n",
    "plt.figure()\n",
    "for i in range(k):\n",
    "    _ = plt.hist(scores[:, i], bins=50, alpha=0.8, label=f\"k={i}\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43207ff9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89825cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e627bb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "\n",
    "dummy_data, y = sklearn.datasets.make_blobs(\n",
    "    n_samples=1000, \n",
    "    n_features=2\n",
    ")\n",
    "\n",
    "dummy_kmeans = faiss.Kmeans(\n",
    "    dummy_data.shape[1], \n",
    "    3,\n",
    "    niter=20, \n",
    "    verbose=True,\n",
    ")\n",
    "dummy_kmeans.train(dummy_data)\n",
    "\n",
    "plt.scatter(dummy_data[:, 0], dummy_data[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d826ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "reducer = umap.UMAP(random_state=42, metric=\"cosine\")\n",
    "reduced_embs = reducer.fit_transform(s_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6876156c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    reduced_embs[::10, 0],\n",
    "    reduced_embs[::10, 1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cb2f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9684e84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = HDBSCAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b78b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer.fit(s_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1947f81e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
