{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b1585d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "\n",
    "\n",
    "from kaggle_llm.core import (\n",
    "    ROOT_PATH,\n",
    "    count_words,\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import *\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pathlib import Path\n",
    "import blingfire as bf\n",
    "import argparse\n",
    "import torch\n",
    "import faiss\n",
    "import json\n",
    "import yaml\n",
    "\n",
    "\n",
    "too_long_prompt_wc = 250\n",
    "context_cutoff_len = 150\n",
    "\n",
    "\n",
    "def split_text_into_chunks(text: str, chunk_size: int) -> List[str]:\n",
    "    tokens = [i for i in text.split() if len(i) > 0]\n",
    "    start = 0\n",
    "    end = chunk_size\n",
    "    chunks = []\n",
    "    while end < len(tokens):\n",
    "        chunks.append(\" \".join(tokens[start: end]))\n",
    "        start += chunk_size\n",
    "        end += chunk_size\n",
    "    return chunks\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_sentence_df(\n",
    "        wiki_df_path: Union[str, Path],\n",
    "):\n",
    "    wiki_df = pd.read_csv(wiki_df_path, index_col=0)\n",
    "\n",
    "    wc_per_page = wiki_df.groupby(\"page\")[[\"word_count\"]].sum().sort_values(\"word_count\", ascending=False)\n",
    "    black_list = list(wc_per_page.loc[\n",
    "        (wc_per_page[\"word_count\"] > 10000)\n",
    "        | (wc_per_page.index.map(lambda x: \"list of equations\" in x.lower()))\n",
    "    ].index)\n",
    "    print(json.dumps(black_list, indent=4))\n",
    "\n",
    "    filtered_wiki_df = wiki_df.loc[~wiki_df[\"page\"].isin(black_list), :].copy()\n",
    "    print(len(wiki_df), len(filtered_wiki_df))\n",
    "\n",
    "    batch_size = 16\n",
    "    sentences_df = []\n",
    "\n",
    "    print(\"extracting sentences:\")\n",
    "    for _, row in tqdm(filtered_wiki_df.iterrows(), total=len(filtered_wiki_df)):\n",
    "        _, sentence_offsets = bf.text_to_sentences_and_offsets(row[\"text\"])\n",
    "        for start_idx, end_idx in sentence_offsets:\n",
    "            is_long_enough = (end_idx - start_idx) > 3\n",
    "            is_math = \"\\\\\" in row[\"text\"][start_idx: end_idx]  # leads to excessive tokens\n",
    "            if is_long_enough and (not is_math):\n",
    "                sentences_df.append({\n",
    "                    \"page\": row[\"page\"],\n",
    "                    \"i_sentence\": len(sentences_df),\n",
    "                    \"text\": row[\"text\"][start_idx: end_idx],\n",
    "                })\n",
    "\n",
    "    sentences_df = pd.DataFrame.from_records(sentences_df)\n",
    "    print(f\"extracted: {len(sentences_df)} sentences\")\n",
    "\n",
    "    print(f\"dropping too long sentences\")\n",
    "    pass_indices = sentences_df.loc[sentences_df[\"text\"].apply(count_words) < context_cutoff_len, \"text\"].index\n",
    "    print(f\"keeping {len(pass_indices) / len(sentences_df) * 100} % at cutoff {context_cutoff_len}\")\n",
    "    sentences_df = sentences_df.loc[pass_indices, :].reset_index().copy()\n",
    "\n",
    "    return sentences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db71707",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_model = \"/home/clay/research/kaggle/kaggle_llm/data/sentence_transformer_model\"\n",
    "wiki_df_path = \"/home/clay/research/kaggle/kaggle_llm/data/physics_pages_list/physics_pages_formatted.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9cf6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_df = get_sentence_df(wiki_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a22f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sentence_df))\n",
    "sentence_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383533af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = sorted(list(sentence_df[\"page\"].unique()))\n",
    "train_rv_idx = {v: i for i, v in enumerate(train_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486ee971",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model = SentenceTransformer(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533ef8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "train_examples = [\n",
    "    InputExample(texts=['Sentence from class 0'], label=0), \n",
    "    InputExample(texts=['Another sentence from class 0'], label=0),\n",
    "    InputExample(texts=['Sentence from class 1'], label=1), \n",
    "    InputExample(texts=['Sentence from class 2'], label=2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1698adec",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_sentences1 = []\n",
    "eval_sentences2 = []\n",
    "eval_labels = []\n",
    "\n",
    "\n",
    "# same group\n",
    "for name, group in sentence_df.groupby(\"page\"):\n",
    "    sampled_sentences = group.sample(min(5, len(group)))\n",
    "    for i in range(len(sampled_sentences)):\n",
    "        for j in range(i, len(sampled_sentences)):\n",
    "            eval_sentences1.append(sampled_sentences.iloc[i][\"text\"])\n",
    "            eval_sentences2.append(sampled_sentences.iloc[j][\"text\"])\n",
    "            eval_labels.append(1.0)\n",
    "\n",
    "\n",
    "# different group\n",
    "n_samples = len(eval_sentences1)\n",
    "sampled1 = sentence_df.sample(n_samples).reset_index()[\"text\"].values\n",
    "sampled2 = sentence_df.sample(n_samples).reset_index()[\"text\"].values\n",
    "sampled_labels = (sampled1 == sampled2).astype(float)\n",
    "\n",
    "eval_sentences1 += list(sampled1)\n",
    "eval_sentences2 += list(sampled2)\n",
    "eval_labels += list(sampled_labels)\n",
    "\n",
    "print(f\"{len(eval_sentences1) = }: ratio: {sum(eval_labels) / len(eval_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98898137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035bfeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import InputExample\n",
    "\n",
    "\n",
    "train_examples = []\n",
    "for _, row in sentence_df.iterrows():\n",
    "    train_examples.append(InputExample(\n",
    "        texts=[row[\"text\"]],\n",
    "        label=train_rv_idx[row[\"page\"]],\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01de93f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86370f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import losses, evaluation\n",
    "\n",
    "\n",
    "# train_loss = losses.TripletLoss(model=model)\n",
    "# train_loss = losses.BatchHardSoftMarginTripletLoss(model=model)\n",
    "train_loss = losses.BatchAllTripletLoss(model=model)\n",
    "evaluator = evaluation.EmbeddingSimilarityEvaluator(\n",
    "    eval_sentences1, \n",
    "    eval_sentences2, \n",
    "    eval_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd761d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback(_score, _epoch, _step):\n",
    "    print(f\"score={_score}, epoch={_epoch}, step={_step}\")\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)], \n",
    "    epochs=10,\n",
    "    output_path=\"/home/clay/research/kaggle/kaggle_llm/data/data_dumps/sentence_embedder_modules/\",\n",
    "    evaluator=evaluator,\n",
    "    evaluation_steps=len(train_dataloader),\n",
    "    callback=callback,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a920eda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dfcafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        # if using DS Zero 3 and the weights are initialized empty\n",
    "        if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
    "            num_params = param.ds_numel\n",
    "\n",
    "        all_param += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params:,d} || all params: {all_param:,d} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a869f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_trainable_parameters(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
