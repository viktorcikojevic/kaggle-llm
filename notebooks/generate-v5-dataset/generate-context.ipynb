{"cells":[{"cell_type":"markdown","metadata":{},"source":["# get context\n","\n","- context is obtained from the [https://www.kaggle.com/code/mbanaei/86-2-with-only-270k-articles](https://www.kaggle.com/code/mbanaei/86-2-with-only-270k-articles) notebook"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-10-05T20:11:28.363863Z","iopub.status.busy":"2023-10-05T20:11:28.363487Z","iopub.status.idle":"2023-10-05T20:11:28.373667Z","shell.execute_reply":"2023-10-05T20:11:28.372615Z","shell.execute_reply.started":"2023-10-05T20:11:28.363835Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting get_context.py\n"]}],"source":["\n","%%writefile get_context.py\n","\n","RUN_ON_KAGGLE = False\n","DEBUG = False\n","\n","import numpy as np\n","import pandas as pd \n","from datasets import load_dataset, load_from_disk\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import torch\n","from transformers import LongformerTokenizer, LongformerForMultipleChoice\n","import transformers\n","import pandas as pd\n","import pickle\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import unicodedata\n","import gc\n","import os\n","\n","stop_words = ['each', 'you', 'the', 'use', 'used',\n","                  'where', 'themselves', 'nor', \"it's\", 'how', \"don't\", 'just', 'your',\n","                  'about', 'himself', 'with', \"weren't\", 'hers', \"wouldn't\", 'more', 'its', 'were',\n","                  'his', 'their', 'then', 'been', 'myself', 're', 'not',\n","                  'ours', 'will', 'needn', 'which', 'here', 'hadn', 'it', 'our', 'there', 'than',\n","                  'most', \"couldn't\", 'both', 'some', 'for', 'up', 'couldn', \"that'll\",\n","                  \"she's\", 'over', 'this', 'now', 'until', 'these', 'few', 'haven',\n","                  'of', 'wouldn', 'into', 'too', 'to', 'very', 'shan', 'before', 'the', 'they',\n","                  'between', \"doesn't\", 'are', 'was', 'out', 'we', 'me',\n","                  'after', 'has', \"isn't\", 'have', 'such', 'should', 'yourselves', 'or', 'during', 'herself',\n","                  'doing', 'in', \"shouldn't\", \"won't\", 'when', 'do', 'through', 'she',\n","                  'having', 'him', \"haven't\", 'against', 'itself', 'that',\n","                  'did', 'theirs', 'can', 'those',\n","                  'own', 'so', 'and', 'who', \"you've\", 'yourself', 'her', 'he', 'only',\n","                  'what', 'ourselves', 'again', 'had', \"you'd\", 'is', 'other',\n","                  'why', 'while', 'from', 'them', 'if', 'above', 'does', 'whom',\n","                  'yours', 'but', 'being', \"wasn't\", 'be']\n","\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import unicodedata\n","\n","\n","def SplitList(mylist, chunk_size):\n","    return [mylist[offs:offs+chunk_size] for offs in range(0, len(mylist), chunk_size)]\n","\n","\n","def get_relevant_documents(df_valid):\n","    df_chunk_size=800\n","    if RUN_ON_KAGGLE:\n","        cohere_dataset_filtered = load_from_disk(\"/kaggle/working/stem-wiki-cohere-no-emb\")\n","    else:\n","        cohere_dataset_filtered = load_from_disk(\"/home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/wiki-stem-cohere\")\n","    modified_texts = cohere_dataset_filtered.map(lambda example:\n","                                             {'temp_text':\n","                                              unicodedata.normalize(\"NFKD\", f\"{example['title']} {example['text']}\").replace('\"',\"\")},\n","                                             num_proc=2)[\"temp_text\"]\n","    \n","    all_articles_indices = []\n","    all_articles_values = []\n","    for idx in tqdm(range(0, df_valid.shape[0], df_chunk_size)):\n","        df_valid_ = df_valid.iloc[idx: idx+df_chunk_size]\n","    \n","        articles_indices, merged_top_scores = retrieval(df_valid_, modified_texts)\n","        all_articles_indices.append(articles_indices)\n","        all_articles_values.append(merged_top_scores)\n","        \n","    article_indices_array =  np.concatenate(all_articles_indices, axis=0)\n","    articles_values_array = np.concatenate(all_articles_values, axis=0).reshape(-1)\n","    \n","    top_per_query = article_indices_array.shape[1]\n","    articles_flatten = [(\n","                         articles_values_array[index],\n","                         cohere_dataset_filtered[idx.item()][\"title\"],\n","                         unicodedata.normalize(\"NFKD\", cohere_dataset_filtered[idx.item()][\"text\"]),\n","                        )\n","                        for index,idx in enumerate(article_indices_array.reshape(-1))]\n","    retrieved_articles = SplitList(articles_flatten, top_per_query)\n","    return retrieved_articles\n","\n","\n","\n","def retrieval(df_valid, modified_texts):\n","    \n","    corpus_df_valid = df_valid.apply(lambda row:\n","                                     f'{row[\"prompt\"]}\\n{row[\"prompt\"]}\\n{row[\"prompt\"]}\\n{row[\"A\"]}\\n{row[\"B\"]}\\n{row[\"C\"]}\\n{row[\"D\"]}\\n{row[\"E\"]}',\n","                                     axis=1).values\n","    vectorizer1 = TfidfVectorizer(ngram_range=(1,2),\n","                                 token_pattern=r\"(?u)\\b[\\w/.-]+\\b|!|/|\\?|\\\"|\\'\",\n","                                 stop_words=stop_words)\n","    vectorizer1.fit(corpus_df_valid)\n","    vocab_df_valid = vectorizer1.get_feature_names_out()\n","    vectorizer = TfidfVectorizer(ngram_range=(1,2),\n","                                 token_pattern=r\"(?u)\\b[\\w/.-]+\\b|!|/|\\?|\\\"|\\'\",\n","                                 stop_words=stop_words,\n","                                 vocabulary=vocab_df_valid)\n","    vectorizer.fit(modified_texts[:500000])\n","    corpus_tf_idf = vectorizer.transform(corpus_df_valid)\n","    \n","    print(f\"length of vectorizer vocab is {len(vectorizer.get_feature_names_out())}\")\n","\n","    chunk_size = 100000\n","    top_per_chunk = 30\n","    top_per_query = 30\n","\n","    all_chunk_top_indices = []\n","    all_chunk_top_values = []\n","\n","    for idx in tqdm(range(0, len(modified_texts), chunk_size)):\n","        wiki_vectors = vectorizer.transform(modified_texts[idx: idx+chunk_size])\n","        temp_scores = (corpus_tf_idf * wiki_vectors.T).toarray()\n","        chunk_top_indices = temp_scores.argpartition(-top_per_chunk, axis=1)[:, -top_per_chunk:]\n","        chunk_top_values = temp_scores[np.arange(temp_scores.shape[0])[:, np.newaxis], chunk_top_indices]\n","\n","        all_chunk_top_indices.append(chunk_top_indices + idx)\n","        all_chunk_top_values.append(chunk_top_values)\n","\n","    top_indices_array = np.concatenate(all_chunk_top_indices, axis=1)\n","    top_values_array = np.concatenate(all_chunk_top_values, axis=1)\n","    \n","    merged_top_scores = np.sort(top_values_array, axis=1)[:,-top_per_query:]\n","    merged_top_indices = top_values_array.argsort(axis=1)[:,-top_per_query:]\n","    articles_indices = top_indices_array[np.arange(top_indices_array.shape[0])[:, np.newaxis], merged_top_indices]\n","    \n","    return articles_indices, merged_top_scores\n","\n","if RUN_ON_KAGGLE:\n","    if DEBUG:\n","        df = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/train.csv\", index_col=\"id\")\n","    else:\n","        df = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\", index_col=\"id\")\n","else:\n","    \n","    df = pd.read_csv(\"/home/viktor/Documents/kaggle/kaggle_llm/data/data_dumps/more_questions/more_questions_raw_questions_wiki_sci_4.csv\")\n","\n","\n","retrieved_articles = get_relevant_documents(df)\n","gc.collect()\n","\n","\n","contexts = []\n","\n","for index in tqdm(range(df.shape[0])):\n","    row = df.iloc[index]\n","    # question is 'prompt'\n","    question = row['prompt']\n","    options = [row['A'], row['B'], row['C'], row['D'], row['E']]\n","    context = f\"{retrieved_articles[index][-4][2]}\\n{retrieved_articles[index][-3][2]}\\n{retrieved_articles[index][-2][2]}\\n{retrieved_articles[index][-1][2]}\"\n","    contexts.append(context)\n","    \n","df['context'] = contexts\n","df.to_parquet(\"train_with_context.parquet\")"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-10-05T20:11:30.789190Z","iopub.status.busy":"2023-10-05T20:11:30.788557Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading cached processed dataset at /home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/wiki-stem-cohere/cache-e94d488c6798573e_*_of_00002.arrow\n","  0%|                                                    | 0/34 [00:00<?, ?it/s]/home/viktor/miniconda3/envs/torch-env/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'd', 'doesn', 'don', 'isn', 'll', 's', 'shouldn', 't', 've', 'wasn', 'weren', 'won'] not in stop_words.\n","  warnings.warn(\n","length of vectorizer vocab is 20238\n","\n","  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\n","  4%|█▌                                          | 1/28 [00:03<01:43,  3.84s/it]\u001b[A\n","  7%|███▏                                        | 2/28 [00:07<01:39,  3.83s/it]\u001b[A\n"," 11%|████▋                                       | 3/28 [00:11<01:34,  3.80s/it]\u001b[A\n"," 14%|██████▎                                     | 4/28 [00:15<01:30,  3.78s/it]\u001b[A\n"," 18%|███████▊                                    | 5/28 [00:18<01:25,  3.72s/it]\u001b[A\n"," 21%|█████████▍                                  | 6/28 [00:22<01:21,  3.69s/it]\u001b[A\n"," 25%|███████████                                 | 7/28 [00:26<01:17,  3.67s/it]\u001b[A\n"," 29%|████████████▌                               | 8/28 [00:29<01:12,  3.65s/it]\u001b[A\n"," 32%|██████████████▏                             | 9/28 [00:33<01:09,  3.64s/it]\u001b[A\n"," 36%|███████████████▎                           | 10/28 [00:36<01:05,  3.62s/it]\u001b[A\n"," 39%|████████████████▉                          | 11/28 [00:40<01:01,  3.62s/it]\u001b[A\n"," 43%|██████████████████▍                        | 12/28 [00:43<00:57,  3.58s/it]\u001b[A\n"," 46%|███████████████████▉                       | 13/28 [00:47<00:53,  3.54s/it]\u001b[A\n"," 50%|█████████████████████▌                     | 14/28 [00:50<00:49,  3.54s/it]\u001b[A\n"," 54%|███████████████████████                    | 15/28 [00:54<00:46,  3.54s/it]\u001b[A\n"," 57%|████████████████████████▌                  | 16/28 [00:57<00:42,  3.53s/it]\u001b[A\n"," 61%|██████████████████████████                 | 17/28 [01:01<00:38,  3.53s/it]\u001b[A\n"," 64%|███████████████████████████▋               | 18/28 [01:05<00:35,  3.52s/it]\u001b[A\n"," 68%|█████████████████████████████▏             | 19/28 [01:08<00:31,  3.50s/it]\u001b[A\n"," 71%|██████████████████████████████▋            | 20/28 [01:11<00:27,  3.49s/it]\u001b[A^C\n"," 71%|██████████████████████████████▋            | 20/28 [01:13<00:29,  3.67s/it]\n","  0%|                                                    | 0/34 [01:27<?, ?it/s]\n","\u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/home/viktor/Documents/kaggle/kaggle_llm/notebooks/generate-v5-dataset/\u001b[0m\u001b[1;33mget_c\u001b[0m \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[1;33montext.py\u001b[0m:\u001b[94m137\u001b[0m in \u001b[92m<module>\u001b[0m                                                    \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m134 \u001b[0m\u001b[2m│   \u001b[0mdf = pd.read_csv(\u001b[33m\"\u001b[0m\u001b[33m/home/viktor/Documents/kaggle/kaggle_llm/data/da\u001b[0m \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m135 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m136 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m137 retrieved_articles = get_relevant_documents(df)                        \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m138 \u001b[0mgc.collect()                                                           \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m139 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m140 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/home/viktor/Documents/kaggle/kaggle_llm/notebooks/generate-v5-dataset/\u001b[0m\u001b[1;33mget_c\u001b[0m \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[1;33montext.py\u001b[0m:\u001b[94m64\u001b[0m in \u001b[92mget_relevant_documents\u001b[0m                                       \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 61 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mfor\u001b[0m idx \u001b[95min\u001b[0m tqdm(\u001b[96mrange\u001b[0m(\u001b[94m0\u001b[0m, df_valid.shape[\u001b[94m0\u001b[0m], df_chunk_size)):       \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 62 \u001b[0m\u001b[2m│   │   \u001b[0mdf_valid_ = df_valid.iloc[idx: idx+df_chunk_size]              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 63 \u001b[0m\u001b[2m│   │   \u001b[0m                                                               \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 64 \u001b[2m│   │   \u001b[0marticles_indices, merged_top_scores = retrieval(df_valid_, mod \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 65 \u001b[0m\u001b[2m│   │   \u001b[0mall_articles_indices.append(articles_indices)                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 66 \u001b[0m\u001b[2m│   │   \u001b[0mall_articles_values.append(merged_top_scores)                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 67 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/home/viktor/Documents/kaggle/kaggle_llm/notebooks/generate-v5-dataset/\u001b[0m\u001b[1;33mget_c\u001b[0m \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[1;33montext.py\u001b[0m:\u001b[94m110\u001b[0m in \u001b[92mretrieval\u001b[0m                                                   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m107 \u001b[0m\u001b[2m│   \u001b[0mall_chunk_top_values = []                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m108 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m109 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mfor\u001b[0m idx \u001b[95min\u001b[0m tqdm(\u001b[96mrange\u001b[0m(\u001b[94m0\u001b[0m, \u001b[96mlen\u001b[0m(modified_texts), chunk_size)):        \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m110 \u001b[2m│   │   \u001b[0mwiki_vectors = vectorizer.transform(modified_texts[idx: idx+ch \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m111 \u001b[0m\u001b[2m│   │   \u001b[0mtemp_scores = (corpus_tf_idf * wiki_vectors.T).toarray()       \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m112 \u001b[0m\u001b[2m│   │   \u001b[0mchunk_top_indices = temp_scores.argpartition(-top_per_chunk, a \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m113 \u001b[0m\u001b[2m│   │   \u001b[0mchunk_top_values = temp_scores[np.arange(temp_scores.shape[\u001b[94m0\u001b[0m]) \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/home/viktor/miniconda3/envs/torch-env/lib/python3.9/site-packages/sklearn/f\u001b[0m \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33meature_extraction/\u001b[0m\u001b[1;33mtext.py\u001b[0m:\u001b[94m2157\u001b[0m in \u001b[92mtransform\u001b[0m                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2154 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                           \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2155 \u001b[0m\u001b[2m│   │   \u001b[0mcheck_is_fitted(\u001b[96mself\u001b[0m, msg=\u001b[33m\"\u001b[0m\u001b[33mThe TF-IDF vectorizer is not fitte\u001b[0m \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2156 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2157 \u001b[2m│   │   \u001b[0mX = \u001b[96msuper\u001b[0m().transform(raw_documents)                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2158 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._tfidf.transform(X, copy=\u001b[94mFalse\u001b[0m)                   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2159 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2160 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_more_tags\u001b[0m(\u001b[96mself\u001b[0m):                                             \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/home/viktor/miniconda3/envs/torch-env/lib/python3.9/site-packages/sklearn/f\u001b[0m \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33meature_extraction/\u001b[0m\u001b[1;33mtext.py\u001b[0m:\u001b[94m1433\u001b[0m in \u001b[92mtransform\u001b[0m                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1430 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m._check_vocabulary()                                      \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1431 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1432 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# use the same matrix-building strategy as fit_transform\u001b[0m      \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1433 \u001b[2m│   │   \u001b[0m_, X = \u001b[96mself\u001b[0m._count_vocab(raw_documents, fixed_vocab=\u001b[94mTrue\u001b[0m)     \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1434 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.binary:                                               \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1435 \u001b[0m\u001b[2m│   │   │   \u001b[0mX.data.fill(\u001b[94m1\u001b[0m)                                            \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1436 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m X                                                      \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/home/viktor/miniconda3/envs/torch-env/lib/python3.9/site-packages/sklearn/f\u001b[0m \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33meature_extraction/\u001b[0m\u001b[1;33mtext.py\u001b[0m:\u001b[94m1275\u001b[0m in \u001b[92m_count_vocab\u001b[0m                               \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1272 \u001b[0m\u001b[2m│   │   \u001b[0mindptr.append(\u001b[94m0\u001b[0m)                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1273 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m doc \u001b[95min\u001b[0m raw_documents:                                     \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1274 \u001b[0m\u001b[2m│   │   │   \u001b[0mfeature_counter = {}                                      \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1275 \u001b[2m│   │   │   \u001b[0m\u001b[94mfor\u001b[0m feature \u001b[95min\u001b[0m analyze(doc):                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1276 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1277 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mfeature_idx = vocabulary[feature]                 \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1278 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mif\u001b[0m feature_idx \u001b[95mnot\u001b[0m \u001b[95min\u001b[0m feature_counter:            \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/home/viktor/miniconda3/envs/torch-env/lib/python3.9/site-packages/sklearn/f\u001b[0m \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33meature_extraction/\u001b[0m\u001b[1;33mtext.py\u001b[0m:\u001b[94m116\u001b[0m in \u001b[92m_analyze\u001b[0m                                    \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 113 \u001b[0m\u001b[2m│   │   │   \u001b[0mdoc = tokenizer(doc)                                      \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 114 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m ngrams \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                        \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 115 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m stop_words \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 116 \u001b[2m│   │   │   │   \u001b[0mdoc = ngrams(doc, stop_words)                         \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 117 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                     \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 118 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mdoc = ngrams(doc)                                     \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 119 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m doc                                                        \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/home/viktor/miniconda3/envs/torch-env/lib/python3.9/site-packages/sklearn/f\u001b[0m \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33meature_extraction/\u001b[0m\u001b[1;33mtext.py\u001b[0m:\u001b[94m271\u001b[0m in \u001b[92m_word_ngrams\u001b[0m                                \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 268 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 269 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mfor\u001b[0m n \u001b[95min\u001b[0m \u001b[96mrange\u001b[0m(min_n, \u001b[96mmin\u001b[0m(max_n + \u001b[94m1\u001b[0m, n_original_tokens +  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 270 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mfor\u001b[0m i \u001b[95min\u001b[0m \u001b[96mrange\u001b[0m(n_original_tokens - n + \u001b[94m1\u001b[0m):            \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 271 \u001b[2m│   │   │   │   │   \u001b[0mtokens_append(space_join(original_tokens[i : i +  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 272 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 273 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m tokens                                                 \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 274 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n","\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n","\u001b[1;91mKeyboardInterrupt\u001b[0m\n"]}],"source":["!python get_context.py"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import pandas as pd\n","\n","\n","df = pd.read_parquet(\"train_with_context.parquet\")\n","# save as csv\n","df.to_csv(\"train_with_context.csv\", index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":4}
