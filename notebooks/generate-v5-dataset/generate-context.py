#!/usr/bin/env python
# coding: utf-8

# # get context
# 
# - context is obtained from the [https://www.kaggle.com/code/mbanaei/86-2-with-only-270k-articles](https://www.kaggle.com/code/mbanaei/86-2-with-only-270k-articles) notebook

# In[1]:


get_ipython().run_cell_magic('writefile', 'get_context.py', '\nRUN_ON_KAGGLE = False\nDEBUG = False\n\nimport numpy as np\nimport pandas as pd \nfrom datasets import load_dataset, load_from_disk\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport torch\nfrom transformers import LongformerTokenizer, LongformerForMultipleChoice\nimport transformers\nimport pandas as pd\nimport pickle\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport unicodedata\nimport gc\nimport os\n\nstop_words = [\'each\', \'you\', \'the\', \'use\', \'used\',\n                  \'where\', \'themselves\', \'nor\', "it\'s", \'how\', "don\'t", \'just\', \'your\',\n                  \'about\', \'himself\', \'with\', "weren\'t", \'hers\', "wouldn\'t", \'more\', \'its\', \'were\',\n                  \'his\', \'their\', \'then\', \'been\', \'myself\', \'re\', \'not\',\n                  \'ours\', \'will\', \'needn\', \'which\', \'here\', \'hadn\', \'it\', \'our\', \'there\', \'than\',\n                  \'most\', "couldn\'t", \'both\', \'some\', \'for\', \'up\', \'couldn\', "that\'ll",\n                  "she\'s", \'over\', \'this\', \'now\', \'until\', \'these\', \'few\', \'haven\',\n                  \'of\', \'wouldn\', \'into\', \'too\', \'to\', \'very\', \'shan\', \'before\', \'the\', \'they\',\n                  \'between\', "doesn\'t", \'are\', \'was\', \'out\', \'we\', \'me\',\n                  \'after\', \'has\', "isn\'t", \'have\', \'such\', \'should\', \'yourselves\', \'or\', \'during\', \'herself\',\n                  \'doing\', \'in\', "shouldn\'t", "won\'t", \'when\', \'do\', \'through\', \'she\',\n                  \'having\', \'him\', "haven\'t", \'against\', \'itself\', \'that\',\n                  \'did\', \'theirs\', \'can\', \'those\',\n                  \'own\', \'so\', \'and\', \'who\', "you\'ve", \'yourself\', \'her\', \'he\', \'only\',\n                  \'what\', \'ourselves\', \'again\', \'had\', "you\'d", \'is\', \'other\',\n                  \'why\', \'while\', \'from\', \'them\', \'if\', \'above\', \'does\', \'whom\',\n                  \'yours\', \'but\', \'being\', "wasn\'t", \'be\']\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport unicodedata\n\n\ndef SplitList(mylist, chunk_size):\n    return [mylist[offs:offs+chunk_size] for offs in range(0, len(mylist), chunk_size)]\n\n\ndef get_relevant_documents(df_valid):\n    df_chunk_size=800\n    if RUN_ON_KAGGLE:\n        cohere_dataset_filtered = load_from_disk("/kaggle/working/stem-wiki-cohere-no-emb")\n    else:\n        cohere_dataset_filtered = load_from_disk("/home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/wiki-stem-cohere")\n    modified_texts = cohere_dataset_filtered.map(lambda example:\n                                             {\'temp_text\':\n                                              unicodedata.normalize("NFKD", f"{example[\'title\']} {example[\'text\']}").replace(\'"\',"")},\n                                             num_proc=2)["temp_text"]\n    \n    all_articles_indices = []\n    all_articles_values = []\n    for idx in tqdm(range(0, df_valid.shape[0], df_chunk_size)):\n        df_valid_ = df_valid.iloc[idx: idx+df_chunk_size]\n    \n        articles_indices, merged_top_scores = retrieval(df_valid_, modified_texts)\n        all_articles_indices.append(articles_indices)\n        all_articles_values.append(merged_top_scores)\n        \n    article_indices_array =  np.concatenate(all_articles_indices, axis=0)\n    articles_values_array = np.concatenate(all_articles_values, axis=0).reshape(-1)\n    \n    top_per_query = article_indices_array.shape[1]\n    articles_flatten = [(\n                         articles_values_array[index],\n                         cohere_dataset_filtered[idx.item()]["title"],\n                         unicodedata.normalize("NFKD", cohere_dataset_filtered[idx.item()]["text"]),\n                        )\n                        for index,idx in enumerate(article_indices_array.reshape(-1))]\n    retrieved_articles = SplitList(articles_flatten, top_per_query)\n    return retrieved_articles\n\n\n\ndef retrieval(df_valid, modified_texts):\n    \n    corpus_df_valid = df_valid.apply(lambda row:\n                                     f\'{row["prompt"]}\\n{row["prompt"]}\\n{row["prompt"]}\\n{row["A"]}\\n{row["B"]}\\n{row["C"]}\\n{row["D"]}\\n{row["E"]}\',\n                                     axis=1).values\n    vectorizer1 = TfidfVectorizer(ngram_range=(1,2),\n                                 token_pattern=r"(?u)\\b[\\w/.-]+\\b|!|/|\\?|\\"|\\\'",\n                                 stop_words=stop_words)\n    vectorizer1.fit(corpus_df_valid)\n    vocab_df_valid = vectorizer1.get_feature_names_out()\n    vectorizer = TfidfVectorizer(ngram_range=(1,2),\n                                 token_pattern=r"(?u)\\b[\\w/.-]+\\b|!|/|\\?|\\"|\\\'",\n                                 stop_words=stop_words,\n                                 vocabulary=vocab_df_valid)\n    vectorizer.fit(modified_texts[:500000])\n    corpus_tf_idf = vectorizer.transform(corpus_df_valid)\n    \n    print(f"length of vectorizer vocab is {len(vectorizer.get_feature_names_out())}")\n\n    chunk_size = 100000\n    top_per_chunk = 30\n    top_per_query = 30\n\n    all_chunk_top_indices = []\n    all_chunk_top_values = []\n\n    for idx in tqdm(range(0, len(modified_texts), chunk_size)):\n        wiki_vectors = vectorizer.transform(modified_texts[idx: idx+chunk_size])\n        temp_scores = (corpus_tf_idf * wiki_vectors.T).toarray()\n        chunk_top_indices = temp_scores.argpartition(-top_per_chunk, axis=1)[:, -top_per_chunk:]\n        chunk_top_values = temp_scores[np.arange(temp_scores.shape[0])[:, np.newaxis], chunk_top_indices]\n\n        all_chunk_top_indices.append(chunk_top_indices + idx)\n        all_chunk_top_values.append(chunk_top_values)\n\n    top_indices_array = np.concatenate(all_chunk_top_indices, axis=1)\n    top_values_array = np.concatenate(all_chunk_top_values, axis=1)\n    \n    merged_top_scores = np.sort(top_values_array, axis=1)[:,-top_per_query:]\n    merged_top_indices = top_values_array.argsort(axis=1)[:,-top_per_query:]\n    articles_indices = top_indices_array[np.arange(top_indices_array.shape[0])[:, np.newaxis], merged_top_indices]\n    \n    return articles_indices, merged_top_scores\n\nif RUN_ON_KAGGLE:\n    if DEBUG:\n        df = pd.read_csv("/kaggle/input/kaggle-llm-science-exam/train.csv", index_col="id")\n    else:\n        df = pd.read_csv("/kaggle/input/kaggle-llm-science-exam/test.csv", index_col="id")\nelse:\n    \n    df = pd.read_csv("/home/viktor/Documents/kaggle/kaggle_llm/data/data_dumps/more_questions/more_questions_raw_questions_wiki_sci_4.csv")\n\n\nretrieved_articles = get_relevant_documents(df)\ngc.collect()\n\n\ncontexts = []\n\nfor index in tqdm(range(df.shape[0])):\n    row = df.iloc[index]\n    # question is \'prompt\'\n    question = row[\'prompt\']\n    options = [row[\'A\'], row[\'B\'], row[\'C\'], row[\'D\'], row[\'E\']]\n    context = f"{retrieved_articles[index][-4][2]}\\n{retrieved_articles[index][-3][2]}\\n{retrieved_articles[index][-2][2]}\\n{retrieved_articles[index][-1][2]}"\n    contexts.append(context)\n    \ndf[\'context\'] = contexts\ndf.to_parquet("train_with_context.parquet")\n')


# In[2]:


get_ipython().system('python get_context.py')

