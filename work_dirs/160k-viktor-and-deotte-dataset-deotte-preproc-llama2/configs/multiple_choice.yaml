inputs:
  - "/home/viktor/Documents/kaggle/kaggle_llm/work_dirs/reproduce-mgoksu-deotte/test_data/train_context_0.csv"
  # - "/home/viktor/Documents/kaggle/kaggle_llm/work_dirs/reproduce-mgoksu-deotte/train_data_1_and_2_final/train_data_final.csv"
  # - "/home/viktor/Documents/kaggle/kaggle_llm/data/cdeotte-60k-data-with-context-v2/all_12_with_context2.csv"
  # - "/home/viktor/Documents/kaggle/kaggle_llm/data/cdeotte-40k/MMLU_17k_with_context2.csv"
  # - "/home/viktor/Documents/kaggle/kaggle_llm/data/cdeotte-40k/OpenBook_with_context2.csv"
  # - "/home/viktor/Documents/kaggle/kaggle_llm/data/cdeotte-40k/ScienceQA_with_context2.csv"

load_from: "/home/viktor/Documents/kaggle/kaggle_llm/data/llama2-7b-stem" # "/home/viktor/Documents/kaggle/kaggle_llm/work_dirs/deberta-v3-data-wiki_sci-with-wiki-sentence-context-eval-kaggle-all-folds-grad-accum-128/deberta-v3-large-2023-08-30-13-15-21/checkpoint-1143"

report_to: []
lr: 2e-6
use_peft: True
peft_class: "AdaLoraConfig"
use_8bit: True
gradient_accumulation_steps: 32
separate_prompt_and_context: False
preprocess_type: "deotte"
max_input: 450
total_epochs: 30
early_stopping_patience: 10
peft_lr: 5e-6
peft_kwargs:
  init_r: 2048
  target_r: 1024
  beta1: 0.85
  beta2: 0.85
  tinit: 200
  tfinal: 1000
  deltaT: 10
  lora_alpha: 256
  lora_dropout: 0.1
  target_modules: ["q_proj", "v_proj"]
  orth_reg_weight: 0.5


eval_on: 
  - "/home/viktor/Documents/kaggle/kaggle_llm/work_dirs/reproduce-mgoksu-deotte/test_data/train_context_0.csv"
eval_all_folds: True
save_total_limit: 10
# fold:
#   num: 0
#   of: 10
