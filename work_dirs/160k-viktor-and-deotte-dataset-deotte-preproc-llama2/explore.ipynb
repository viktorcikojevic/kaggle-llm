{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# read /home/viktor/Documents/kaggle/kaggle_llm/work_dirs/160k-viktor-and-deotte-dataset-deotte-preproc-llama2/llama2-7b-stem-2023-09-19-19-26-03/train_tokenized_dataset.pkl to train_tokenized_dataset\n",
    "with open('/home/viktor/Documents/kaggle/kaggle_llm/work_dirs/160k-viktor-and-deotte-dataset-deotte-preproc-llama2/llama2-7b-stem-2023-09-19-19-26-03/train_tokenized_dataset.pkl', 'rb') as f:\n",
    "    train_tokenized_dataset = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['context', '__index_level_0__', 'input_ids', 'attention_mask', 'label'],\n",
       "    num_rows: 200\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples with empty input_ids: 0\n"
     ]
    }
   ],
   "source": [
    "empty_input_ids_indices = []\n",
    "\n",
    "for i, sample in enumerate(train_tokenized_dataset):\n",
    "    if not sample[\"input_ids\"]:\n",
    "        empty_input_ids_indices.append(i)\n",
    "\n",
    "print(f\"Number of samples with empty input_ids: {len(empty_input_ids_indices)}\")\n",
    "\n",
    "# If you want to see the indices of samples with empty input_ids:\n",
    "if empty_input_ids_indices:\n",
    "    print(f\"Indices of samples with empty input_ids: {empty_input_ids_indices}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# read /home/viktor/Documents/kaggle/kaggle_llm/work_dirs/160k-viktor-and-deotte-dataset-deotte-preproc-llama2/llama2-7b-stem-2023-09-19-19-26-03/train_tokenized_dataset.pkl to train_tokenized_dataset\n",
    "with open('/home/viktor/Documents/kaggle/kaggle_llm/work_dirs/160k-viktor-and-deotte-dataset-deotte-preproc-llama2/llama2-7b-stem-2023-09-19-19-26-03/val_tokenized_dataset.pkl', 'rb') as f:\n",
    "    val_tokenized_dataset = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['context', '__index_level_0__', 'input_ids', 'attention_mask', 'label'],\n",
       "    num_rows: 200\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['context', '__index_level_0__', 'input_ids', 'attention_mask', 'label'],\n",
       "    num_rows: 200\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_llm.adapted_models import (\n",
    "    LlamaModelForMultipleChoice, \n",
    "    DebertaV2ForMultipleChoice2\n",
    ")\n",
    "from kaggle_llm.core import (\n",
    "    DataCollatorForMultipleChoice,\n",
    "    DataCollatorForMultipleChoicePrompting,\n",
    "    WORK_DIRS_PATH,\n",
    "    ROOT_PATH,\n",
    "    compute_map3_hf,\n",
    "    build_peft_model,\n",
    "    load_train_and_val_df,\n",
    "    get_tokenize_dataset_from_df,\n",
    "    get_mcp_tokenize_dataset_from_df,\n",
    "    train_and_save_best_model_on_error,\n",
    "    add_context\n",
    ")\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from loguru import logger\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import json\n",
    "import yaml\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "logger.add(sys.stdout, format=\"{time} {level} {message}\", level=\"INFO\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def main(config_path: str,\n",
    "         work_dir_path: str = None,\n",
    "         ):\n",
    "    with open(config_path, \"rb\") as f:\n",
    "        config = yaml.load(f, yaml.FullLoader)\n",
    "\n",
    "    load_from = config[\"load_from\"]\n",
    "    input_paths = config[\"inputs\"]\n",
    "    logger.info(json.dumps(config, indent=4))\n",
    "    logger.info(\"loading data\")\n",
    "    \n",
    "    if \"eval_on\" in config.keys():\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Load training data \n",
    "        train_df_1, val_df_1 = load_train_and_val_df(\n",
    "            input_paths=config[\"inputs\"],\n",
    "            i_fold=config[\"fold\"][\"num\"] if \"fold\" in config else 0,\n",
    "            total_fold=config[\"fold\"][\"of\"] if \"fold\" in config else 10,\n",
    "        )\n",
    "        train_df = pd.concat([train_df_1, val_df_1])\n",
    "        \n",
    "        \n",
    "        # Load validation data \n",
    "        train_df_2, val_df_2 = load_train_and_val_df(\n",
    "            input_paths=config[\"eval_on\"],\n",
    "            i_fold=config[\"fold\"][\"num\"] if \"fold\" in config else 0,\n",
    "            total_fold=config[\"fold\"][\"of\"] if \"fold\" in config else 10,\n",
    "        )\n",
    "        if \"eval_all_folds\" in config and config[\"eval_all_folds\"]:\n",
    "            val_df = pd.concat([train_df_2, val_df_2])\n",
    "        else:\n",
    "            val_df = val_df_2\n",
    "    else:\n",
    "        train_df, val_df = load_train_and_val_df(\n",
    "            input_paths=input_paths,\n",
    "            i_fold=config[\"fold\"][\"num\"],\n",
    "            total_fold=config[\"fold\"][\"of\"],\n",
    "        )  \n",
    "        \n",
    "        \n",
    "    print(\"train_df.dtypes\", train_df.dtypes)\n",
    "    \n",
    "    print(\"train_df:\")\n",
    "    print(train_df.iloc[0])\n",
    "    print(\"val_df:\")\n",
    "    print(val_df.iloc[0])\n",
    "    if \"add_context\" in config and config[\"add_context\"]:\n",
    "        train_df = add_context(train_df)\n",
    "        val_df = add_context(val_df)\n",
    "\n",
    "        print(f\"New train_df size: {len(train_df)}\")\n",
    "        print(f\"New val_df size: {len(val_df)}\")\n",
    "    \n",
    "        print(train_df.sample(1)['new_prompt'].values[0])\n",
    "        \n",
    "    print(f\"[INFO] train df size is {len(train_df)}\")\n",
    "    print(f\"[INFO] val df size is {len(val_df)}\")\n",
    "    \n",
    "    if \"train_size\" in config:\n",
    "        train_df = train_df.sample(config[\"train_size\"], replace=False).reset_index(drop=True)\n",
    "        print(f\"[INFO] Resampled df. New train df size is {len(train_df)}\")\n",
    "\n",
    "    \n",
    "    model_name = load_from.split(\"/\")[-1]\n",
    "    print(\"model_name:\", model_name)\n",
    "    if work_dir_path is None:\n",
    "        work_dir_path = WORK_DIRS_PATH\n",
    "    model_output_dir = os.path.join(work_dir_path, f\"{model_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\")\n",
    "    model_output_dir = Path(model_output_dir)\n",
    "    model_output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    logger.info(f\"splitted dataset of size {len(train_df) + len(val_df)} -> {len(train_df)} & {len(val_df)}\")\n",
    "    logger.info(\"loaded data\")\n",
    "\n",
    "    logger.info(\"initting models\")\n",
    "    model, tokenizer = build_peft_model(\n",
    "        config[\"load_from\"],\n",
    "        use_peft=config[\"use_peft\"],\n",
    "        peft_class=config[\"peft_class\"],\n",
    "        transformer_class=\"AutoModelForMultipleChoice\",\n",
    "        use_8bit=config[\"use_8bit\"],\n",
    "        **config[\"peft_kwargs\"] if \"peft_kwargs\" in config else {},\n",
    "    )\n",
    "    \n",
    "    if 'freeze_embeddings' in config and config['freeze_embeddings'] and 'deberta' in config['load_from']:\n",
    "        print('Freezing embeddings.')\n",
    "        for param in model.deberta.embeddings.parameters():\n",
    "            param.requires_grad = False\n",
    "    if 'freeze_layers' in config and config['freeze_layers'] and 'deberta' in config['load_from']:\n",
    "        freeze_layers = config['freeze_layers']\n",
    "        print(f'Freezing first {freeze_layers} layers.')\n",
    "        for layer in model.deberta.encoder.layer[:freeze_layers]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    \n",
    "    logger.info(f\"model.num_parameters() = {model.num_parameters() * 1e-6} Million\")\n",
    "    logger.info(f\"model.num_parameters() = {model.num_parameters() * 1e-9} Billion\")\n",
    "    logger.info(\"initted models\")\n",
    " \n",
    "    logger.info(\"initting dataset\")\n",
    "    \n",
    "    if 'separate_prompt_and_context' in config and config['separate_prompt_and_context']:\n",
    "        def get_context(text):\n",
    "            x = text.split(\" ### \")\n",
    "            # remove empty strings\n",
    "            x = [x for x in x if len(x) > 0]\n",
    "            \n",
    "            assert len(x) == 2, f\"Unsuccesful prompt splitting . len(x) = {len(x)}, x={x}\"\n",
    "            return x[0]\n",
    "        \n",
    "        \n",
    "        train_df['context'] = train_df['prompt'].apply(lambda x: get_context(x))\n",
    "        val_df['context'] = val_df['prompt'].apply(lambda x: get_context(x))\n",
    "        \n",
    "        def get_prompt(text):\n",
    "            x = text.split(\" ### \")\n",
    "            # remove empty strings\n",
    "            x = [x for x in x if len(x) > 0]\n",
    "            assert len(x) == 2, f\"Unsuccesful prompt splitting . len(x) = {len(x)}\"\n",
    "            return x[1]\n",
    "        \n",
    "        train_df['prompt'] = train_df['prompt'].apply(lambda x: get_prompt(x))\n",
    "        val_df['prompt'] = val_df['prompt'].apply(lambda x: get_prompt(x))\n",
    "        \n",
    "    \n",
    "    if \"max_context_size\" in config:\n",
    "        max_context_size = config[\"max_context_size\"]\n",
    "        def limit_context(x, max_context_size):\n",
    "            x = x[:max_context_size]\n",
    "            return x\n",
    "        train_df['context'] = train_df['context'].apply(lambda x: limit_context(x, max_context_size))\n",
    "        val_df['context'] = val_df['context'].apply(lambda x: limit_context(x, max_context_size))\n",
    "    \n",
    "    preprocess_type = 'sumo' if 'preprocess_type' not in config else config['preprocess_type']\n",
    "    print(f\"preprocess_type: {preprocess_type}\")\n",
    "    max_input = 512 if 'max_input' not in config else config['max_input']\n",
    "    \n",
    "    # Trim context length to reasonable size\n",
    "    train_df['context'] = train_df['context'].apply(lambda x: x[:12000])\n",
    "    val_df['context'] = val_df['context'].apply(lambda x: x[:12000])\n",
    "    \n",
    "    train_df['context_len'] = train_df['context'].apply(lambda x: len(x))\n",
    "    train_df['prompt_len'] = train_df['prompt'].apply(lambda x: len(x))\n",
    "    \n",
    "    val_df['context_len'] = val_df['context'].apply(lambda x: len(x))\n",
    "    val_df['prompt_len'] = val_df['prompt'].apply(lambda x: len(x))\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"train_df['context_len'].min()\", train_df['context_len'].min())\n",
    "    print(\"train_df['context_len'].max()\", train_df['context_len'].max())\n",
    "    print(\"val_df['context_len'].min()\", val_df['context_len'].min())\n",
    "    print(\"val_df['context_len'].max()\", val_df['context_len'].max())\n",
    "    print(\"train_df['prompt_len'].min()\", train_df['prompt_len'].min())\n",
    "    print(\"train_df['prompt_len'].max()\", train_df['prompt_len'].max())\n",
    "    print(\"val_df['prompt_len'].min()\", val_df['prompt_len'].min())\n",
    "    print(\"val_df['prompt_len'].max()\", val_df['prompt_len'].max())\n",
    "    \n",
    "    # print unique answers\n",
    "    print(\"train_df['answer'].unique()\", train_df['answer'].unique())\n",
    "    print(\"val_df['answer'].unique()\", val_df['answer'].unique())\n",
    "    \n",
    "    # take only prompt, context, A, B, C, D, E and answer (if there's an answer)\n",
    "    train_df = train_df[['prompt', 'context', 'A', 'B', 'C', 'D', 'E', 'answer']]\n",
    "    val_df = val_df[['prompt', 'context', 'A', 'B', 'C', 'D', 'E', 'answer']]\n",
    "    \n",
    "    train_df['prompt'] = train_df['prompt'].astype(str)\n",
    "    val_df['prompt'] = val_df['prompt'].astype(str)\n",
    "    train_df['context'] = train_df['context'].astype(str)\n",
    "    val_df['context'] = val_df['context'].astype(str)\n",
    "    train_df['answer'] = train_df['answer'].astype(str)\n",
    "    val_df['answer'] = val_df['answer'].astype(str)\n",
    "\n",
    "    options = 'ABCDE'\n",
    "    for option in options:\n",
    "        train_df[option] = train_df[option].astype(str)\n",
    "        val_df[option] = val_df[option].astype(str)\n",
    "    \n",
    "    \n",
    "    train_df.to_csv(model_output_dir / \"train_df.csv\")\n",
    "    val_df.to_csv(model_output_dir / \"val_df.csv\")\n",
    "    \n",
    "    train_tokenized_dataset = get_tokenize_dataset_from_df(train_df, tokenizer, preprocess_type, max_input)\n",
    "    val_tokenized_dataset = get_tokenize_dataset_from_df(val_df, tokenizer, preprocess_type, max_input)\n",
    "    # train_tokenized_dataset = get_mcp_tokenize_dataset_from_df(train_df, tokenizer)\n",
    "    # val_tokenized_dataset = get_mcp_tokenize_dataset_from_df(val_df, tokenizer)\n",
    "    logger.info(\"initted dataset\")\n",
    "    \n",
    "    # save train_tokenized_dataset and val_tokenized_dataset as pickle files\n",
    "    import pickle\n",
    "    with open(model_output_dir / \"train_tokenized_dataset.pkl\", 'wb') as f:\n",
    "        pickle.dump(train_tokenized_dataset, f)\n",
    "    \n",
    "    with open(model_output_dir / \"val_tokenized_dataset.pkl\", 'wb') as f:\n",
    "        pickle.dump(val_tokenized_dataset, f)\n",
    "    \n",
    "    return\n",
    "\n",
    "    logger.info(\"initting trainer\")\n",
    "    warmup_epochs = 1\n",
    "    total_epochs = config[\"total_epochs\"]\n",
    "    warmup_ratio = warmup_epochs / total_epochs\n",
    "    training_args = TrainingArguments(\n",
    "        metric_for_best_model=\"map3\",\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        greater_is_better=True,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        learning_rate=float(config[\"lr\"]),\n",
    "        per_device_train_batch_size=1,\n",
    "        load_best_model_at_end=False,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=50,\n",
    "        save_steps=50,\n",
    "        logging_steps=50,\n",
    "        per_device_eval_batch_size=2,\n",
    "        num_train_epochs=total_epochs,\n",
    "        save_total_limit=config[\"save_total_limit\"] if \"save_total_limit\" in config else 10,\n",
    "        report_to=config[\"report_to\"],\n",
    "        output_dir=str(model_output_dir),\n",
    "        # fp16=False if 'use_8bit' in config and config['use_8bit'] else True,\n",
    "        # gradient_checkpointing=True,\n",
    "        gradient_accumulation_steps=config[\"gradient_accumulation_steps\"],\n",
    "        # deepspeed=str((ROOT_PATH / \"configs\" / \"deepspeed.json\").resolve().absolute()),\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n",
    "        # data_collator=DataCollatorForMultipleChoicePrompting(tokenizer=tokenizer),\n",
    "        train_dataset=train_tokenized_dataset,\n",
    "        eval_dataset=val_tokenized_dataset,\n",
    "        compute_metrics=compute_map3_hf,\n",
    "        # callbacks=[\n",
    "        #     EarlyStoppingCallback(early_stopping_patience=config[\"early_stopping_patience\"]),\n",
    "        # ],\n",
    "    )\n",
    "    logger.info(\"initting trainer\")\n",
    "\n",
    "    trainer.train()\n",
    "    # train_and_save_best_model_on_error(\n",
    "    #     trainer,\n",
    "    #     model_output_dir,\n",
    "    #     \"best_map3_peft\" if config[\"use_peft\"] else \"best_map3\",\n",
    "    # )\n",
    "    \n",
    "    if config[\"report_to\"] == \"wandb\":\n",
    "        wandb.finish()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"/home/viktor/Documents/kaggle/kaggle_llm/work_dirs/160k-viktor-and-deotte-dataset-deotte-preproc-llama2/configs/multiple_choice.yaml\"\n",
    "work_dir_path = \"./\"\n",
    "\n",
    "\n",
    "peft_kwargs = {\n",
    "        \"init_r\": 2048,\n",
    "        \"target_r\": 1024,\n",
    "        \"beta1\": 0.85,\n",
    "        \"beta2\": 0.85,\n",
    "        \"tinit\": 200,\n",
    "        \"tfinal\": 1000,\n",
    "        \"deltaT\": 10,\n",
    "        \"lora_alpha\": 256,\n",
    "        \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\n",
    "            \"q_proj\",\n",
    "            \"v_proj\"\n",
    "        ],\n",
    "        \"orth_reg_weight\": 0.5\n",
    "    }\n",
    "\n",
    "model, tokenizer = build_peft_model(\n",
    "        \"/home/viktor/Documents/kaggle/kaggle_llm/data/llama2-7b-stem\",\n",
    "        use_peft=True,\n",
    "        peft_class=\"AdaLoraConfig\",\n",
    "        transformer_class=\"AutoModelForMultipleChoice\",\n",
    "        use_8bit=True,\n",
    "        **peft_kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1 </span>text = <span style=\"color: #808000; text-decoration-color: #808000\">\"ahha\"</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2 tokenized_text = tokenizer(text, return_tensors=<span style=\"color: #808000; text-decoration-color: #808000\">\"pt\"</span>)                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span>tokenized_text                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'tokenizer'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m2\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1 \u001b[0mtext = \u001b[33m\"\u001b[0m\u001b[33mahha\u001b[0m\u001b[33m\"\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2 tokenized_text = tokenizer(text, return_tensors=\u001b[33m\"\u001b[0m\u001b[33mpt\u001b[0m\u001b[33m\"\u001b[0m)                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3 \u001b[0mtokenized_text                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m4 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'tokenizer'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"ahha\"\n",
    "tokenized_text = tokenizer(text, return_tensors=\"pt\")\n",
    "tokenized_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
