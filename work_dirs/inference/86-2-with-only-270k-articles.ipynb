{"cells":[{"cell_type":"markdown","metadata":{},"source":["I previously shared a [notebook](url) (see the discussion [here](https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/441128)) that found a cluster of relevant Wikipedia STEM articles, resulting in around 270K STEM articles for which the resulting dataset is released [here.](https://www.kaggle.com/datasets/mbanaei/stem-wiki-cohere-no-emb)\n","\n","However, due to issues with WikiExtractor, there're cases in which some numbers or even paragraphs are missing from the final Wiki parsing. Therefore,  for the same set of  articles, I used Wiki API to gather the articles' contexts (see discussion [here](https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/442483)), for which the resulting dataset is released [here](https://www.kaggle.com/datasets/mbanaei/all-paraphs-parsed-expanded).\n","\n","In order to show that the found articles cover not only the train dataset articles but also a majority of LB gold articles, I release this notebook that uses a simple retrieval model (without any prior indexing) together with a model that is trained only on the RACE dataset. (not fine-tuned on any competition-similar dataset).\n","\n","The main design choices for the notebook are:\n","- Using a simple TF-IDF to retrieve contexts from both datasets for every given question.\n","- Although the majority of high-performing public models use DeBERTa-V3 to do the inference in their pipeline, I used a LongFormer Large model, which enables us to have a much longer prefix context given limited GPU memory. More specifically, as opposed to many public notebooks, there's no splitting to sentence level, and the whole paragraph is retrieved and passed to the classifier as a context (the main reason that we don't get OOM and also have relatively fast inference is that in LongFormer full attention is not computed as opposed to standard models like BERT).\n","- I use a fall-back model (based on a public notebook that uses an openbook approach and performs 81.5 on LB) that is used for prediction when there's low confidence in the main model's output for the top choice.\n","\n","P.S: Although the model's performance is relatively good compared to other public notebooks, many design choices can be revised to improve both inference time and performance. (e.g., currently, context retrieval seems to be the inference bottleneck as no prior indexing is used)."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-29T13:41:10.47071Z","iopub.status.busy":"2023-08-29T13:41:10.470251Z","iopub.status.idle":"2023-08-29T13:41:46.496147Z","shell.execute_reply":"2023-08-29T13:41:46.494943Z","shell.execute_reply.started":"2023-08-29T13:41:10.470675Z"},"trusted":true},"outputs":[],"source":["# !cp /kaggle/input/datasets-wheel/datasets-2.14.4-py3-none-any.whl /kaggle/working\n","# !pip install  /kaggle/working/datasets-2.14.4-py3-none-any.whl\n","# !cp /kaggle/input/backup-806/util_openbook.py ."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2023-08-29T13:41:52.418247Z","iopub.status.busy":"2023-08-29T13:41:52.41787Z","iopub.status.idle":"2023-08-29T13:43:54.355399Z","shell.execute_reply":"2023-08-29T13:43:54.354189Z","shell.execute_reply.started":"2023-08-29T13:41:52.418215Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["# # installing offline dependencies\n","# !pip install -U /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n","# !cp -rf /kaggle/input/sentence-transformers-222/sentence-transformers /kaggle/working/sentence-transformers\n","# !pip install -U /kaggle/working/sentence-transformers\n","# !pip install -U /kaggle/input/blingfire-018/blingfire-0.1.8-py3-none-any.whl\n","\n","# !pip install --no-index --no-deps /kaggle/input/llm-whls/transformers-4.31.0-py3-none-any.whl\n","# !pip install --no-index --no-deps /kaggle/input/llm-whls/peft-0.4.0-py3-none-any.whl\n","# !pip install --no-index --no-deps /kaggle/input/llm-whls/trl-0.5.0-py3-none-any.whl"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["RUN_ON_KAGGLE = False\n","DEBUG = False\n","\n","VAL_SIZE = 10 if DEBUG else 2000\n","\n","# file_for_local_cv = \"/home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-llm-science-exam/test.csv\"\n","file_for_local_cv = \"/home/viktor/Documents/kaggle/kaggle_llm/data/data_dumps/more_questions/more_questions_raw_questions_wiki_sci_3_shuffled.csv\""]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2023-08-29T13:44:53.234278Z","iopub.status.busy":"2023-08-29T13:44:53.233905Z","iopub.status.idle":"2023-08-29T13:55:59.072609Z","shell.execute_reply":"2023-08-29T13:55:59.071407Z","shell.execute_reply.started":"2023-08-29T13:44:53.234248Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["from util_openbook import get_contexts\n","import pickle\n","\n","get_contexts(RUN_ON_KAGGLE, VAL_SIZE, file_for_local_cv)\n","# generate_openbook_output()\n","\n","import gc\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-29T14:10:13.830365Z","iopub.status.busy":"2023-08-29T14:10:13.829974Z","iopub.status.idle":"2023-08-29T14:10:13.839005Z","shell.execute_reply":"2023-08-29T14:10:13.838006Z","shell.execute_reply.started":"2023-08-29T14:10:13.830336Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","backup_model_predictions = pd.read_csv(\"submission_backup.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-08-11T16:44:53.204295Z","iopub.status.busy":"2023-08-11T16:44:53.203928Z","iopub.status.idle":"2023-08-11T16:44:53.360032Z","shell.execute_reply":"2023-08-11T16:44:53.359072Z","shell.execute_reply.started":"2023-08-11T16:44:53.204267Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd \n","from datasets import load_dataset, load_from_disk\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import torch\n","from transformers import LongformerTokenizer, LongformerForMultipleChoice\n","import transformers\n","import pandas as pd\n","import pickle\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import unicodedata\n","\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-11T16:45:00.736996Z","iopub.status.busy":"2023-08-11T16:45:00.735581Z","iopub.status.idle":"2023-08-11T16:45:15.400251Z","shell.execute_reply":"2023-08-11T16:45:15.398987Z","shell.execute_reply.started":"2023-08-11T16:45:00.736955Z"},"trusted":true},"outputs":[],"source":["!cp -r /kaggle/input/stem-wiki-cohere-no-emb /kaggle/working\n","!cp -r /kaggle/input/all-paraphs-parsed-expanded /kaggle/working/"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-11T16:45:37.825306Z","iopub.status.busy":"2023-08-11T16:45:37.824922Z","iopub.status.idle":"2023-08-11T16:45:37.850706Z","shell.execute_reply":"2023-08-11T16:45:37.84937Z","shell.execute_reply.started":"2023-08-11T16:45:37.825276Z"},"trusted":true},"outputs":[],"source":["def SplitList(mylist, chunk_size):\n","    return [mylist[offs:offs+chunk_size] for offs in range(0, len(mylist), chunk_size)]\n","\n","def get_relevant_documents_parsed(df_valid):\n","    df_chunk_size=600\n","    paraphs_parsed_dataset = load_from_disk(\"/kaggle/working/all-paraphs-parsed-expanded\")\n","    modified_texts = paraphs_parsed_dataset.map(lambda example:\n","                                             {'temp_text':\n","                                              f\"{example['title']} {example['section']} {example['text']}\".replace('\\n',\" \").replace(\"'\",\"\")},\n","                                             num_proc=2)[\"temp_text\"]\n","    \n","    all_articles_indices = []\n","    all_articles_values = []\n","    for idx in tqdm(range(0, df_valid.shape[0], df_chunk_size)):\n","        df_valid_ = df_valid.iloc[idx: idx+df_chunk_size]\n","    \n","        articles_indices, merged_top_scores = retrieval(df_valid_, modified_texts)\n","        all_articles_indices.append(articles_indices)\n","        all_articles_values.append(merged_top_scores)\n","        \n","    article_indices_array =  np.concatenate(all_articles_indices, axis=0)\n","    articles_values_array = np.concatenate(all_articles_values, axis=0).reshape(-1)\n","    \n","    top_per_query = article_indices_array.shape[1]\n","    articles_flatten = [(\n","                         articles_values_array[index],\n","                         paraphs_parsed_dataset[idx.item()][\"title\"],\n","                         paraphs_parsed_dataset[idx.item()][\"text\"],\n","                        )\n","                        for index,idx in enumerate(article_indices_array.reshape(-1))]\n","    retrieved_articles = SplitList(articles_flatten, top_per_query)\n","    return retrieved_articles\n","\n","\n","\n","def get_relevant_documents(df_valid):\n","    df_chunk_size=800\n","    \n","    cohere_dataset_filtered = load_from_disk(\"/kaggle/working/stem-wiki-cohere-no-emb\")\n","    modified_texts = cohere_dataset_filtered.map(lambda example:\n","                                             {'temp_text':\n","                                              unicodedata.normalize(\"NFKD\", f\"{example['title']} {example['text']}\").replace('\"',\"\")},\n","                                             num_proc=2)[\"temp_text\"]\n","    \n","    all_articles_indices = []\n","    all_articles_values = []\n","    for idx in tqdm(range(0, df_valid.shape[0], df_chunk_size)):\n","        df_valid_ = df_valid.iloc[idx: idx+df_chunk_size]\n","    \n","        articles_indices, merged_top_scores = retrieval(df_valid_, modified_texts)\n","        all_articles_indices.append(articles_indices)\n","        all_articles_values.append(merged_top_scores)\n","        \n","    article_indices_array =  np.concatenate(all_articles_indices, axis=0)\n","    articles_values_array = np.concatenate(all_articles_values, axis=0).reshape(-1)\n","    \n","    top_per_query = article_indices_array.shape[1]\n","    articles_flatten = [(\n","                         articles_values_array[index],\n","                         cohere_dataset_filtered[idx.item()][\"title\"],\n","                         unicodedata.normalize(\"NFKD\", cohere_dataset_filtered[idx.item()][\"text\"]),\n","                        )\n","                        for index,idx in enumerate(article_indices_array.reshape(-1))]\n","    retrieved_articles = SplitList(articles_flatten, top_per_query)\n","    return retrieved_articles\n","\n","\n","\n","def retrieval(df_valid, modified_texts):\n","    \n","    corpus_df_valid = df_valid.apply(lambda row:\n","                                     f'{row[\"prompt\"]}\\n{row[\"prompt\"]}\\n{row[\"prompt\"]}\\n{row[\"A\"]}\\n{row[\"B\"]}\\n{row[\"C\"]}\\n{row[\"D\"]}\\n{row[\"E\"]}',\n","                                     axis=1).values\n","    vectorizer1 = TfidfVectorizer(ngram_range=(1,2),\n","                                 token_pattern=r\"(?u)\\b[\\w/.-]+\\b|!|/|\\?|\\\"|\\'\",\n","                                 stop_words=stop_words)\n","    vectorizer1.fit(corpus_df_valid)\n","    vocab_df_valid = vectorizer1.get_feature_names_out()\n","    vectorizer = TfidfVectorizer(ngram_range=(1,2),\n","                                 token_pattern=r\"(?u)\\b[\\w/.-]+\\b|!|/|\\?|\\\"|\\'\",\n","                                 stop_words=stop_words,\n","                                 vocabulary=vocab_df_valid)\n","    vectorizer.fit(modified_texts[:500000])\n","    corpus_tf_idf = vectorizer.transform(corpus_df_valid)\n","    \n","    print(f\"length of vectorizer vocab is {len(vectorizer.get_feature_names_out())}\")\n","\n","    chunk_size = 100000\n","    top_per_chunk = 10\n","    top_per_query = 10\n","\n","    all_chunk_top_indices = []\n","    all_chunk_top_values = []\n","\n","    for idx in tqdm(range(0, len(modified_texts), chunk_size)):\n","        wiki_vectors = vectorizer.transform(modified_texts[idx: idx+chunk_size])\n","        temp_scores = (corpus_tf_idf * wiki_vectors.T).toarray()\n","        chunk_top_indices = temp_scores.argpartition(-top_per_chunk, axis=1)[:, -top_per_chunk:]\n","        chunk_top_values = temp_scores[np.arange(temp_scores.shape[0])[:, np.newaxis], chunk_top_indices]\n","\n","        all_chunk_top_indices.append(chunk_top_indices + idx)\n","        all_chunk_top_values.append(chunk_top_values)\n","\n","    top_indices_array = np.concatenate(all_chunk_top_indices, axis=1)\n","    top_values_array = np.concatenate(all_chunk_top_values, axis=1)\n","    \n","    merged_top_scores = np.sort(top_values_array, axis=1)[:,-top_per_query:]\n","    merged_top_indices = top_values_array.argsort(axis=1)[:,-top_per_query:]\n","    articles_indices = top_indices_array[np.arange(top_indices_array.shape[0])[:, np.newaxis], merged_top_indices]\n","    \n","    return articles_indices, merged_top_scores\n","\n","\n","def prepare_answering_input(\n","        tokenizer, \n","        question,  \n","        options,   \n","        context,   \n","        max_seq_length=4096,\n","    ):\n","    c_plus_q   = context + ' ' + tokenizer.bos_token + ' ' + question\n","    c_plus_q_4 = [c_plus_q] * len(options)\n","    tokenized_examples = tokenizer(\n","        c_plus_q_4, options,\n","        max_length=max_seq_length,\n","        padding=\"longest\",\n","        truncation=False,\n","        return_tensors=\"pt\",\n","    )\n","    input_ids = tokenized_examples['input_ids'].unsqueeze(0)\n","    attention_mask = tokenized_examples['attention_mask'].unsqueeze(0)\n","    example_encoded = {\n","        \"input_ids\": input_ids.to(model.device.index),\n","        \"attention_mask\": attention_mask.to(model.device.index),\n","    }\n","    return example_encoded\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-11T16:45:45.645019Z","iopub.status.busy":"2023-08-11T16:45:45.644645Z","iopub.status.idle":"2023-08-11T16:45:45.654919Z","shell.execute_reply":"2023-08-11T16:45:45.653722Z","shell.execute_reply.started":"2023-08-11T16:45:45.644991Z"},"trusted":true},"outputs":[],"source":["stop_words = ['each', 'you', 'the', 'use', 'used',\n","                  'where', 'themselves', 'nor', \"it's\", 'how', \"don't\", 'just', 'your',\n","                  'about', 'himself', 'with', \"weren't\", 'hers', \"wouldn't\", 'more', 'its', 'were',\n","                  'his', 'their', 'then', 'been', 'myself', 're', 'not',\n","                  'ours', 'will', 'needn', 'which', 'here', 'hadn', 'it', 'our', 'there', 'than',\n","                  'most', \"couldn't\", 'both', 'some', 'for', 'up', 'couldn', \"that'll\",\n","                  \"she's\", 'over', 'this', 'now', 'until', 'these', 'few', 'haven',\n","                  'of', 'wouldn', 'into', 'too', 'to', 'very', 'shan', 'before', 'the', 'they',\n","                  'between', \"doesn't\", 'are', 'was', 'out', 'we', 'me',\n","                  'after', 'has', \"isn't\", 'have', 'such', 'should', 'yourselves', 'or', 'during', 'herself',\n","                  'doing', 'in', \"shouldn't\", \"won't\", 'when', 'do', 'through', 'she',\n","                  'having', 'him', \"haven't\", 'against', 'itself', 'that',\n","                  'did', 'theirs', 'can', 'those',\n","                  'own', 'so', 'and', 'who', \"you've\", 'yourself', 'her', 'he', 'only',\n","                  'what', 'ourselves', 'again', 'had', \"you'd\", 'is', 'other',\n","                  'why', 'while', 'from', 'them', 'if', 'above', 'does', 'whom',\n","                  'yours', 'but', 'being', \"wasn't\", 'be']"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-11T16:45:50.623548Z","iopub.status.busy":"2023-08-11T16:45:50.623189Z","iopub.status.idle":"2023-08-11T16:45:50.641881Z","shell.execute_reply":"2023-08-11T16:45:50.640966Z","shell.execute_reply.started":"2023-08-11T16:45:50.623519Z"},"trusted":true},"outputs":[],"source":["df_valid = pd.read_csv(file_for_local_cv)\n","if DEBUG and not RUN_ON_KAGGLE:\n","    df_valid = df_valid.sample(n=VAL_SIZE, random_state=42).reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","import unicodedata\n","\n","\n","def SplitList(mylist, chunk_size):\n","    return [mylist[offs:offs+chunk_size] for offs in range(0, len(mylist), chunk_size)]\n","\n","def get_relevant_documents_parsed(df_valid):\n","    df_chunk_size=600\n","    if RUN_ON_KAGGLE:\n","        paraphs_parsed_dataset = load_from_disk(\"/kaggle/working/all-paraphs-parsed-expanded\")\n","    else:\n","        paraphs_parsed_dataset = load_from_disk(\"/home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/all-paraphs-parsed-expanded\")\n","    \n","    modified_texts = paraphs_parsed_dataset.map(lambda example:\n","                                             {'temp_text':\n","                                              f\"{example['title']} {example['section']} {example['text']}\".replace('\\n',\" \").replace(\"'\",\"\")},\n","                                             num_proc=2)[\"temp_text\"]\n","    \n","    all_articles_indices = []\n","    all_articles_values = []\n","    for idx in tqdm(range(0, df_valid.shape[0], df_chunk_size)):\n","        df_valid_ = df_valid.iloc[idx: idx+df_chunk_size]\n","    \n","        articles_indices, merged_top_scores = retrieval(df_valid_, modified_texts)\n","        all_articles_indices.append(articles_indices)\n","        all_articles_values.append(merged_top_scores)\n","        \n","    article_indices_array =  np.concatenate(all_articles_indices, axis=0)\n","    articles_values_array = np.concatenate(all_articles_values, axis=0).reshape(-1)\n","    \n","    top_per_query = article_indices_array.shape[1]\n","    articles_flatten = [(\n","                         articles_values_array[index],\n","                         paraphs_parsed_dataset[idx.item()][\"title\"],\n","                         paraphs_parsed_dataset[idx.item()][\"text\"],\n","                        )\n","                        for index,idx in enumerate(article_indices_array.reshape(-1))]\n","    retrieved_articles = SplitList(articles_flatten, top_per_query)\n","    return retrieved_articles\n","\n","\n","\n","def get_relevant_documents(df_valid):\n","    df_chunk_size=800\n","    if RUN_ON_KAGGLE:\n","        cohere_dataset_filtered = load_from_disk(\"/kaggle/working/stem-wiki-cohere-no-emb\")\n","    else:\n","        cohere_dataset_filtered = load_from_disk(\"/home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/wiki-stem-cohere\")\n","    modified_texts = cohere_dataset_filtered.map(lambda example:\n","                                             {'temp_text':\n","                                              unicodedata.normalize(\"NFKD\", f\"{example['title']} {example['text']}\").replace('\"',\"\")},\n","                                             num_proc=2)[\"temp_text\"]\n","    \n","    all_articles_indices = []\n","    all_articles_values = []\n","    for idx in tqdm(range(0, df_valid.shape[0], df_chunk_size)):\n","        df_valid_ = df_valid.iloc[idx: idx+df_chunk_size]\n","    \n","        articles_indices, merged_top_scores = retrieval(df_valid_, modified_texts)\n","        all_articles_indices.append(articles_indices)\n","        all_articles_values.append(merged_top_scores)\n","        \n","    article_indices_array =  np.concatenate(all_articles_indices, axis=0)\n","    articles_values_array = np.concatenate(all_articles_values, axis=0).reshape(-1)\n","    \n","    top_per_query = article_indices_array.shape[1]\n","    articles_flatten = [(\n","                         articles_values_array[index],\n","                         cohere_dataset_filtered[idx.item()][\"title\"],\n","                         unicodedata.normalize(\"NFKD\", cohere_dataset_filtered[idx.item()][\"text\"]),\n","                        )\n","                        for index,idx in enumerate(article_indices_array.reshape(-1))]\n","    retrieved_articles = SplitList(articles_flatten, top_per_query)\n","    return retrieved_articles\n","\n","\n","\n","def retrieval(df_valid, modified_texts):\n","    \n","    corpus_df_valid = df_valid.apply(lambda row:\n","                                     f'{row[\"prompt\"]}\\n{row[\"prompt\"]}\\n{row[\"prompt\"]}\\n{row[\"A\"]}\\n{row[\"B\"]}\\n{row[\"C\"]}\\n{row[\"D\"]}\\n{row[\"E\"]}',\n","                                     axis=1).values\n","    vectorizer1 = TfidfVectorizer(ngram_range=(1,2),\n","                                 token_pattern=r\"(?u)\\b[\\w/.-]+\\b|!|/|\\?|\\\"|\\'\",\n","                                 stop_words=stop_words)\n","    vectorizer1.fit(corpus_df_valid)\n","    vocab_df_valid = vectorizer1.get_feature_names_out()\n","    vectorizer = TfidfVectorizer(ngram_range=(1,2),\n","                                 token_pattern=r\"(?u)\\b[\\w/.-]+\\b|!|/|\\?|\\\"|\\'\",\n","                                 stop_words=stop_words,\n","                                 vocabulary=vocab_df_valid)\n","    vectorizer.fit(modified_texts[:500000])\n","    corpus_tf_idf = vectorizer.transform(corpus_df_valid)\n","    \n","    print(f\"length of vectorizer vocab is {len(vectorizer.get_feature_names_out())}\")\n","\n","    chunk_size = 100000\n","    top_per_chunk = 10\n","    top_per_query = 10\n","\n","    all_chunk_top_indices = []\n","    all_chunk_top_values = []\n","\n","    for idx in tqdm(range(0, len(modified_texts), chunk_size)):\n","        wiki_vectors = vectorizer.transform(modified_texts[idx: idx+chunk_size])\n","        temp_scores = (corpus_tf_idf * wiki_vectors.T).toarray()\n","        chunk_top_indices = temp_scores.argpartition(-top_per_chunk, axis=1)[:, -top_per_chunk:]\n","        chunk_top_values = temp_scores[np.arange(temp_scores.shape[0])[:, np.newaxis], chunk_top_indices]\n","\n","        all_chunk_top_indices.append(chunk_top_indices + idx)\n","        all_chunk_top_values.append(chunk_top_values)\n","\n","    top_indices_array = np.concatenate(all_chunk_top_indices, axis=1)\n","    top_values_array = np.concatenate(all_chunk_top_values, axis=1)\n","    \n","    merged_top_scores = np.sort(top_values_array, axis=1)[:,-top_per_query:]\n","    merged_top_indices = top_values_array.argsort(axis=1)[:,-top_per_query:]\n","    articles_indices = top_indices_array[np.arange(top_indices_array.shape[0])[:, np.newaxis], merged_top_indices]\n","    \n","    return articles_indices, merged_top_scores\n","\n","\n","def prepare_answering_input(\n","        tokenizer, \n","        question,  \n","        options,   \n","        context,   \n","        max_seq_length=4096,\n","    ):\n","    c_plus_q   = context + ' ' + tokenizer.bos_token + ' ' + question\n","    c_plus_q_4 = [c_plus_q] * len(options)\n","    tokenized_examples = tokenizer(\n","        c_plus_q_4, options,\n","        max_length=max_seq_length,\n","        padding=\"longest\",\n","        truncation=False,\n","        return_tensors=\"pt\",\n","    )\n","    input_ids = tokenized_examples['input_ids'].unsqueeze(0)\n","    attention_mask = tokenized_examples['attention_mask'].unsqueeze(0)\n","    example_encoded = {\n","        \"input_ids\": input_ids.to(model.device.index),\n","        \"attention_mask\": attention_mask.to(model.device.index),\n","    }\n","    return example_encoded"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["retrieved_articles_parsed = get_relevant_documents_parsed(df_valid)\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-11T16:45:52.93561Z","iopub.status.busy":"2023-08-11T16:45:52.935248Z","iopub.status.idle":"2023-08-11T17:03:23.254145Z","shell.execute_reply":"2023-08-11T17:03:23.25295Z","shell.execute_reply.started":"2023-08-11T16:45:52.935582Z"},"trusted":true},"outputs":[],"source":["retrieved_articles = get_relevant_documents(df_valid)\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-11T17:03:23.259633Z","iopub.status.busy":"2023-08-11T17:03:23.2586Z","iopub.status.idle":"2023-08-11T17:03:45.509905Z","shell.execute_reply":"2023-08-11T17:03:45.508872Z","shell.execute_reply.started":"2023-08-11T17:03:23.259593Z"},"trusted":true},"outputs":[],"source":["tokenizer = LongformerTokenizer.from_pretrained(\"/home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/longformer-race-model/longformer_qa_model\")\n","model = LongformerForMultipleChoice.from_pretrained(\"/home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/longformer-race-model/longformer_qa_model\").cuda()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-11T17:11:57.829758Z","iopub.status.busy":"2023-08-11T17:11:57.829337Z","iopub.status.idle":"2023-08-11T17:13:53.780911Z","shell.execute_reply":"2023-08-11T17:13:53.779904Z","shell.execute_reply.started":"2023-08-11T17:11:57.829727Z"},"trusted":true},"outputs":[],"source":["\n","contexts = []\n","predictions = []\n","submit_ids = []\n","\n","for index in tqdm(range(df_valid.shape[0])):\n","    row = df_valid.iloc[index]\n","    # question is 'prompt'\n","    question = row['prompt']\n","    options = [row['A'], row['B'], row['C'], row['D'], row['E']]\n","    context1 = f\"{retrieved_articles[index][-4][2]}\\n{retrieved_articles[index][-3][2]}\\n{retrieved_articles[index][-2][2]}\\n{retrieved_articles[index][-1][2]}\"\n","    contexts.append(context1)\n","#     inputs1 = prepare_answering_input(\n","#         tokenizer=tokenizer, question=question,\n","#         options=options, context=context1,\n","#         )\n","#     with torch.no_grad():\n","#         outputs1 = model(**inputs1)    \n","#         losses1 = -outputs1.logits[0].detach().cpu().numpy()\n","#         probability1 = torch.softmax(torch.tensor(-losses1), dim=-1)\n","        \n","#     with torch.no_grad():\n","#         outputs2 = model(**inputs2)\n","#         losses2 = -outputs2.logits[0].detach().cpu().numpy()\n","#         probability2 = torch.softmax(torch.tensor(-losses2), dim=-1)\n","        \n","#     probability_ = (probability1 + probability2)/2\n","\n","    \n","#     predict = np.array(list(\"ABCDE\"))[np.argsort(probability_)][-3:].tolist()[::-1]\n","    \n","    \n","#     # if probability_.max() > 0.4:\n","#     #     predict = np.array(list(\"ABCDE\"))[np.argsort(probability_)][-3:].tolist()[::-1]\n","#     # else:\n","#     #     predict = backup_model_predictions.iloc[index].prediction.replace(\" \",\"\")\n","#     predictions.append(predict)\n","\n","# predictions = [\" \".join(i) for i in predictions]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_valid['context'] = contexts\n","# save as parquet\n","df_valid.to_parquet(\"df_valid.parquet\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","predictions = []\n","submit_ids = []\n","\n","df_valid_small = df_valid.sample(n=100).reset_index(drop=True)\n","\n","for index in tqdm(range(df_valid_small.shape[0])):\n","    row = df_valid_small.iloc[index]\n","    # question is 'prompt'\n","    question = row['prompt']\n","    options = [row['A'], row['B'], row['C'], row['D'], row['E']]\n","    context1 = f\"{retrieved_articles[index][-4][2]}\\n{retrieved_articles[index][-3][2]}\\n{retrieved_articles[index][-2][2]}\\n{retrieved_articles[index][-1][2]}\"\n","    # contexts.append(context1)\n","    inputs1 = prepare_answering_input(\n","        tokenizer=tokenizer, question=question,\n","        options=options, context=context1,\n","        )\n","    with torch.no_grad():\n","        outputs1 = model(**inputs1)    \n","        losses1 = -outputs1.logits[0].detach().cpu().numpy()\n","        probability1 = torch.softmax(torch.tensor(-losses1), dim=-1)\n","\n","    probability_ = probability1\n","\n","    \n","    predict = np.array(list(\"ABCDE\"))[np.argsort(probability_)][-3:].tolist()[::-1]\n","    \n","    \n","    # if probability_.max() > 0.4:\n","    #     predict = np.array(list(\"ABCDE\"))[np.argsort(probability_)][-3:].tolist()[::-1]\n","    # else:\n","    #     predict = backup_model_predictions.iloc[index].prediction.replace(\" \",\"\")\n","    predictions.append(predict)\n","\n","predictions = [\" \".join(i) for i in predictions]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-11T17:09:27.495432Z","iopub.status.busy":"2023-08-11T17:09:27.495047Z","iopub.status.idle":"2023-08-11T17:09:27.510631Z","shell.execute_reply":"2023-08-11T17:09:27.509601Z","shell.execute_reply.started":"2023-08-11T17:09:27.495397Z"},"trusted":true},"outputs":[],"source":["df = pd.DataFrame({'prediction':predictions})\n","df['targets'] = df_valid['answer']\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df['correct_1'] = df[['prediction', 'targets']].apply(lambda x: 1 if x[0].split(\" \")[0] == x[1] else 0, axis=1)\n","df['correct_2'] = df[['prediction', 'targets']].apply(lambda x: 1 if x[0].split(\" \")[1] == x[1] else 0, axis=1)\n","df['correct_3'] = df[['prediction', 'targets']].apply(lambda x: 1 if x[0].split(\" \")[2] == x[1] else 0, axis=1)\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["(df['correct_1'].sum() + 0.5 * df['correct_2'].sum() +  1./3 * df['correct_3'].sum()) / len(df)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_valid"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","df.to_csv('submission.csv', index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":4}
