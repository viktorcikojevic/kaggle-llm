{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-09-25T21:32:16.823181Z","iopub.status.busy":"2023-09-25T21:32:16.822903Z","iopub.status.idle":"2023-09-25T21:32:16.836270Z","shell.execute_reply":"2023-09-25T21:32:16.835164Z","shell.execute_reply.started":"2023-09-25T21:32:16.823155Z"},"trusted":true},"outputs":[],"source":["RUN_ON_KAGGLE = False\n","DEBUG = False"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-09-25T21:32:16.838813Z","iopub.status.busy":"2023-09-25T21:32:16.838485Z"},"trusted":true},"outputs":[],"source":["%%capture\n","if RUN_ON_KAGGLE:\n","    # installing offline dependencies\n","    !pip install -U /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n","    !cp -rf /kaggle/input/sentence-transformers-222/sentence-transformers /kaggle/working/sentence-transformers\n","    !pip install -U /kaggle/working/sentence-transformers\n","    !pip install -U /kaggle/input/blingfire-018/blingfire-0.1.8-py3-none-any.whl\n","\n","    !pip install --no-index --no-deps /kaggle/input/llm-whls/transformers-4.31.0-py3-none-any.whl\n","    !pip install --no-index --no-deps /kaggle/input/llm-whls/peft-0.4.0-py3-none-any.whl\n","    !pip install --no-index --no-deps /kaggle/input/llm-whls/datasets-2.14.3-py3-none-any.whl\n","    !pip install --no-index --no-deps /kaggle/input/llm-whls/trl-0.5.0-py3-none-any.whl\n","    \n","    !pip install \"/kaggle/input/blingfire-018/blingfire-0.1.8-py3-none-any.whl\" --no-index\n","    !pip install \"/kaggle/input/peft-native/peft-0.4.0-py3-none-any.whl\" --no-index --find-links=\"/kaggle/input/peft-native/\"\n","    !pip install \"/kaggle/input/bitsandbytes-0410/bitsandbytes-0.41.0-py3-none-any.whl\" --no-index\n","    !pip install \"/kaggle/input/accelerate-native/accelerate-0.21.0-py3-none-any.whl\" --no-index --find-links=\"/kaggle/input/accelerate-native/\"\n","    !pip install \"/kaggle/input/transformers-native/transformers-4.31.0-py3-none-any.whl\" --no-index --find-links=\"/kaggle/input/transformers-native/\""]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","===================================BUG REPORT===================================\n","Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n","================================================================================\n","CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n","CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n","CUDA SETUP: Highest compute capability among GPUs detected: 8.9\n","CUDA SETUP: Detected CUDA version 121\n","CUDA SETUP: Loading binary /home/viktor/miniconda3/envs/torch-env/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda121.so...\n"]},{"name":"stderr","output_type":"stream","text":["/home/viktor/miniconda3/envs/torch-env/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /home/viktor/miniconda3/envs/torch-env did not contain libcudart.so as expected! Searching further paths...\n","  warn(msg)\n","/home/viktor/miniconda3/envs/torch-env/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n","  warn(msg)\n","/home/viktor/miniconda3/envs/torch-env/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n","  warn(msg)\n"]}],"source":["import os, time\n","import gc\n","import pandas as pd\n","import numpy as np\n","import re\n","from tqdm.auto import tqdm\n","import blingfire as bf\n","# from __future__ import annotations\n","\n","from collections.abc import Iterable\n","\n","import faiss\n","from faiss import write_index, read_index\n","\n","from sentence_transformers import SentenceTransformer\n","\n","import torch\n","import ctypes\n","libc = ctypes.CDLL(\"libc.so.6\")\n","\n","from dataclasses import dataclass\n","from typing import Optional, Union\n","\n","import torch\n","import numpy as np\n","import pandas as pd\n","from datasets import Dataset\n","from transformers import AutoTokenizer\n","from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n","from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n","from torch.utils.data import DataLoader\n","\n","from scipy.special import softmax"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[],"source":["if RUN_ON_KAGGLE:\n","    SIM_MODEL = '/kaggle/input/sentencetransformers-allminilml6v2/sentence-transformers_all-MiniLM-L6-v2'\n","else:\n","    SIM_MODEL = 'sentence-transformers/all-MiniLM-L6-v2'\n","DEVICE = 0\n","MAX_LENGTH = 384\n","BATCH_SIZE = 1\n","\n","\n","FILTER_LEN = 1 if DEBUG else 9\n","IND_SEARCH = 1 if DEBUG else 7\n","NUM_SENTENCES_INCLUDE = 1 if DEBUG else 25\n","CONTEXT_LEN = 1000 if DEBUG else 2305\n","CONTEXT_LEN_8BIT = 1000 if DEBUG else 4096\n","VAL_SIZE = 20 if DEBUG else 2000\n","\n","if RUN_ON_KAGGLE:\n","    WIKI_PATH = \"/kaggle/input/wikipedia-20230701\"\n","else:\n","    WIKI_PATH = \"/home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/wikipedia-2023-07-faiss-index\"\n","    \n","# file_for_local_cv = \"/home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-llm-science-exam/test.csv\"    \n","file_for_local_cv = \"/home/viktor/Documents/kaggle/kaggle_llm/data/data_dumps/more_questions/more_questions_raw_questions_wiki_sci_3_shuffled.csv\"    \n","\n","wiki_files = os.listdir(WIKI_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def process_documents(documents: Iterable[str],\n","                      document_ids: Iterable,\n","                      split_sentences: bool = True,\n","                      filter_len: int = FILTER_LEN,\n","                      disable_progress_bar: bool = False) -> pd.DataFrame:\n","    \n","    df = sectionize_documents(documents, document_ids, disable_progress_bar)\n","\n","    if split_sentences:\n","        df = sentencize(df.text.values, \n","                        df.document_id.values,\n","                        df.offset.values, \n","                        filter_len, \n","                        disable_progress_bar)\n","    return df\n","\n","\n","def sectionize_documents(documents: Iterable[str],\n","                         document_ids: Iterable,\n","                         disable_progress_bar: bool = False) -> pd.DataFrame:\n","\n","    processed_documents = []\n","    for document_id, document in tqdm(zip(document_ids, documents), total=len(documents), disable=disable_progress_bar):\n","        row = {}\n","        text, start, end = (document, 0, len(document))\n","        row['document_id'] = document_id\n","        row['text'] = text\n","        row['offset'] = (start, end)\n","\n","        processed_documents.append(row)\n","\n","    _df = pd.DataFrame(processed_documents)\n","    if _df.shape[0] > 0:\n","        return _df.sort_values(['document_id', 'offset']).reset_index(drop=True)\n","    else:\n","        return _df\n","\n","\n","def sentencize(documents: Iterable[str],\n","               document_ids: Iterable,\n","               offsets: Iterable[tuple[int, int]],\n","               filter_len: int = FILTER_LEN,\n","               disable_progress_bar: bool = False) -> pd.DataFrame:\n","\n","    document_sentences = []\n","    for document, document_id, offset in tqdm(zip(documents, document_ids, offsets), total=len(documents), disable=disable_progress_bar):\n","        try:\n","            _, sentence_offsets = bf.text_to_sentences_and_offsets(document)\n","            for o in sentence_offsets:\n","                if o[1]-o[0] > filter_len:\n","                    sentence = document[o[0]:o[1]]\n","                    abs_offsets = (o[0]+offset[0], o[1]+offset[0])\n","                    row = {}\n","                    row['document_id'] = document_id\n","                    row['text'] = sentence\n","                    row['offset'] = abs_offsets\n","                    document_sentences.append(row)\n","        except:\n","            continue\n","    return pd.DataFrame(document_sentences)"]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>prompt</th>\n","      <th>A</th>\n","      <th>B</th>\n","      <th>C</th>\n","      <th>D</th>\n","      <th>E</th>\n","      <th>answer</th>\n","      <th>cluster number</th>\n","      <th>round</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>13884</td>\n","      <td>What is the relationship between Adiantum and ...</td>\n","      <td>Adiantum is a variant of AES</td>\n","      <td>Adiantum is a cryptographic message authentica...</td>\n","      <td>Adiantum uses AES for disk encryption</td>\n","      <td>Adiantum is a low-powered mobile device runnin...</td>\n","      <td>Adiantum uses a different construction for the...</td>\n","      <td>C</td>\n","      <td>590</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>20379</td>\n","      <td>What is the cause of the painful bump in Osgoo...</td>\n","      <td>Excessive bone growth at the tibial tuberosity</td>\n","      <td>Fluid accumulation in the patellar ligament</td>\n","      <td>Inflammation of the quadriceps muscle</td>\n","      <td>Degeneration of the knee cartilage</td>\n","      <td>Dislocation of the tibiofemoral joint</td>\n","      <td>A</td>\n","      <td>827</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>24313</td>\n","      <td>What are the two most common types of tarsal c...</td>\n","      <td>Calcaneo-cuboid and calcaneo-navicular</td>\n","      <td>Talo-cuneiform and talo-tarsal</td>\n","      <td>Navicular-cuboid and calcaneo-tarsal</td>\n","      <td>Talo-calcaneal and talo-navicular</td>\n","      <td>Calcaneo-navicular and talo-calcaneal</td>\n","      <td>E</td>\n","      <td>988</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>22258</td>\n","      <td>What is the significance of dietary restrictio...</td>\n","      <td>Dietary restrictions can worsen the symptoms o...</td>\n","      <td>Dietary restrictions can completely cure ocula...</td>\n","      <td>Dietary restrictions have no impact on the sym...</td>\n","      <td>Dietary restrictions may reduce or eliminate t...</td>\n","      <td>Dietary restrictions only impact the symptoms ...</td>\n","      <td>D</td>\n","      <td>902</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>16046</td>\n","      <td>What is the purpose of isolating Parvibaculum ...</td>\n","      <td>To remove it from wastewater treatment plants ...</td>\n","      <td>To improve the aesthetics of activated sludge</td>\n","      <td>To study its metabolic abilities and potential...</td>\n","      <td>To increase the production of linear alkylbenz...</td>\n","      <td>To enhance the growth of other bacteria in the...</td>\n","      <td>C</td>\n","      <td>668</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0                                             prompt   \n","0       13884  What is the relationship between Adiantum and ...  \\\n","1       20379  What is the cause of the painful bump in Osgoo...   \n","2       24313  What are the two most common types of tarsal c...   \n","3       22258  What is the significance of dietary restrictio...   \n","4       16046  What is the purpose of isolating Parvibaculum ...   \n","\n","                                                   A   \n","0                       Adiantum is a variant of AES  \\\n","1     Excessive bone growth at the tibial tuberosity   \n","2             Calcaneo-cuboid and calcaneo-navicular   \n","3  Dietary restrictions can worsen the symptoms o...   \n","4  To remove it from wastewater treatment plants ...   \n","\n","                                                   B   \n","0  Adiantum is a cryptographic message authentica...  \\\n","1        Fluid accumulation in the patellar ligament   \n","2                     Talo-cuneiform and talo-tarsal   \n","3  Dietary restrictions can completely cure ocula...   \n","4      To improve the aesthetics of activated sludge   \n","\n","                                                   C   \n","0              Adiantum uses AES for disk encryption  \\\n","1              Inflammation of the quadriceps muscle   \n","2               Navicular-cuboid and calcaneo-tarsal   \n","3  Dietary restrictions have no impact on the sym...   \n","4  To study its metabolic abilities and potential...   \n","\n","                                                   D   \n","0  Adiantum is a low-powered mobile device runnin...  \\\n","1                 Degeneration of the knee cartilage   \n","2                  Talo-calcaneal and talo-navicular   \n","3  Dietary restrictions may reduce or eliminate t...   \n","4  To increase the production of linear alkylbenz...   \n","\n","                                                   E answer  cluster number   \n","0  Adiantum uses a different construction for the...      C             590  \\\n","1              Dislocation of the tibiofemoral joint      A             827   \n","2              Calcaneo-navicular and talo-calcaneal      E             988   \n","3  Dietary restrictions only impact the symptoms ...      D             902   \n","4  To enhance the growth of other bacteria in the...      C             668   \n","\n","   round  \n","0      0  \n","1      1  \n","2      0  \n","3      0  \n","4      0  "]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["if RUN_ON_KAGGLE:\n","    trn = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\").drop(\"id\", axis=1)\n","else:\n","    trn = pd.read_csv(file_for_local_cv).drop(\"id\", axis=1).sample(n=VAL_SIZE, random_state=42).reset_index(drop=True)\n","trn.head()"]},{"cell_type":"markdown","metadata":{},"source":["# Inference"]},{"cell_type":"code","execution_count":7,"metadata":{"trusted":true},"outputs":[],"source":["test_df = pd.read_csv(\"test_context.csv\")\n","test_df.index = list(range(len(test_df)))\n","test_df['id'] = list(range(len(test_df)))\n","# test_df[\"prompt\"] = test_df[\"context\"].apply(lambda x: x[:CONTEXT_LEN]) + \" #### \" +  test_df[\"prompt\"]"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[],"source":["\n","options = 'ABCDE'\n","indices = list(range(5))\n","\n","option_to_index = {option: index for option, index in zip(options, indices)}\n","index_to_option = {index: option for option, index in zip(options, indices)}\n","\n","def preprocess(example):\n","  \n","    \n","    first_sentence = [ \"[CLS] \" + example['context'] ] * 5\n","    second_sentences = [\" #### \" + example['prompt'] + \" [SEP] \" + example[option] + \" [SEP]\" for option in 'ABCDE']\n","    tokenized_example = tokenizer(first_sentence, \n","                                    second_sentences, \n","                                    truncation='only_first', \n","                                    max_length=CONTEXT_LEN, \n","                                    add_special_tokens=False)        \n","    if \"answer\" in example:\n","        tokenized_example[\"label\"] = option_to_index[example[\"answer\"]]\n","    \n","    return tokenized_example\n","            \n","    # first_sentence = [example['prompt']] * 5\n","    # second_sentence = []\n","    # for option in options:\n","    #     second_sentence.append(example[option])\n","    \n","    # tokenized_example = tokenizer(first_sentence, second_sentence, truncation='only_first')\n","    # tokenized_example['label'] = option_to_index[example['answer']]\n","    # return tokenized_example\n","    \n","def preprocess_for_8bit(example):\n","  \n","    \n","    first_sentence = [ \"[CLS] \" + example['context'] ] * 5\n","    second_sentences = [\" #### \" + example['prompt'] + \" [SEP] \" + example[option] + \" [SEP]\" for option in 'ABCDE']\n","    tokenized_example = tokenizer(first_sentence, \n","                                    second_sentences, \n","                                    truncation='only_first', \n","                                    max_length=CONTEXT_LEN_8BIT, \n","                                    add_special_tokens=False)        \n","    if \"answer\" in example:\n","        tokenized_example[\"label\"] = option_to_index[example[\"answer\"]]\n","    \n","    return tokenized_example\n","            \n","    # first_sentence = [example['prompt']] * 5\n","    # second_sentence = []\n","    # for option in options:\n","    #     second_sentence.append(example[option])\n","    \n","    # tokenized_example = tokenizer(first_sentence, second_sentence, truncation='only_first')\n","    # tokenized_example['label'] = option_to_index[example['answer']]\n","    # return tokenized_example"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[],"source":["@dataclass\n","class DataCollatorForMultipleChoice:\n","    tokenizer: PreTrainedTokenizerBase\n","    padding: Union[bool, str, PaddingStrategy] = True\n","    max_length: Optional[int] = None\n","    pad_to_multiple_of: Optional[int] = None\n","    \n","    def __call__(self, features):\n","        # label_name = \"label\" if 'label' in features[0].keys() else 'labels'\n","        # labels = [feature.pop(label_name) for feature in features]\n","        # batch_size = len(features)\n","        # num_choices = len(features[0]['input_ids'])\n","        # flattened_features = [\n","        #     [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n","        # ]\n","        # flattened_features = sum(flattened_features, [])\n","        \n","        # batch = self.tokenizer.pad(\n","        #     flattened_features,\n","        #     padding=self.padding,\n","        #     max_length=self.max_length,\n","        #     pad_to_multiple_of=self.pad_to_multiple_of,\n","        #     return_tensors='pt',\n","        # )\n","        # batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n","        # batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n","        # return batch\n","        \n","        batch_size = len(features)\n","        num_choices = len(features[0][\"input_ids\"])\n","        flattened_features = [\n","            [\n","                {k: v[i] for k, v in feature.items() if k not in (\"context\", \"__index_level_0__\", \"label\", \"labels\")}\n","                for i in range(num_choices)\n","            ] for feature in features\n","        ]\n","        flattened_features = sum(flattened_features, [])\n","\n","        # _max_length = max([len(x[\"input_ids\"]) for x in flattened_features])\n","        # print(f\"{_max_length = }\")\n","\n","        batch = self.tokenizer.pad(\n","            flattened_features,\n","            padding=self.padding,\n","            max_length=self.max_length,\n","            pad_to_multiple_of=self.pad_to_multiple_of,\n","            return_tensors=\"pt\",\n","        )\n","        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n","        if \"label\" in features[0].keys() or \"labels\" in features[0].keys():\n","            label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n","            labels = [feature.pop(label_name) for feature in features]\n","            batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n","        return batch\n","        \n","        "]},{"cell_type":"markdown","metadata":{},"source":["# viktor deberta 160k 8bit"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# model_dir = \"/kaggle/input/how-to-train-open-book-model-part-1/model_v2\"\n","if RUN_ON_KAGGLE:\n","    model_dir = \"/kaggle/input/llm-submissions-viktor/work_dirs/deberta-v3-data-wiki_sci-with-wiki-sentence-context-eval-kaggle-all-folds-grad-accum-128-60k/deberta-v3-large-2023-09-05-07-35-55/checkpoint-3281\"\n","else:\n","    model_dir = \"/home/viktor/Documents/kaggle/kaggle_llm/work_dirs/160k-viktor-and-deotte-dataset-deotte-preproc-deberta/deberta-v3-large-2023-09-17-10-00-20/checkpoint-14400\"\n","    \n","tokenizer = AutoTokenizer.from_pretrained(model_dir)\n","model = AutoModelForMultipleChoice.from_pretrained(model_dir, load_in_8bit=True)\n","model.eval()"]},{"cell_type":"markdown","metadata":{},"source":["# longformer (kaggle)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if RUN_ON_KAGGLE:\n","    !cp /kaggle/input/datasets-wheel/datasets-2.14.4-py3-none-any.whl /kaggle/working\n","    !pip install  /kaggle/working/datasets-2.14.4-py3-none-any.whl\n","    !cp /kaggle/input/backup-806-viktor/util_openbook.py ."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import gc\n","import pandas as pd\n","import numpy as np\n","import re\n","from tqdm.auto import tqdm\n","import blingfire as bf\n","\n","from collections.abc import Iterable\n","\n","import faiss\n","from faiss import write_index, read_index\n","\n","from sentence_transformers import SentenceTransformer\n","\n","import torch\n","import ctypes\n","libc = ctypes.CDLL(\"libc.so.6\")\n","\n","from dataclasses import dataclass\n","from typing import Optional, Union\n","\n","import torch\n","import numpy as np\n","import pandas as pd\n","from datasets import Dataset\n","from transformers import AutoTokenizer\n","from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n","from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n","from torch.utils.data import DataLoader\n","\n","\n","def process_documents(documents: Iterable[str],\n","                      document_ids: Iterable,\n","                      split_sentences: bool = True,\n","                      filter_len: int = 3,\n","                      disable_progress_bar: bool = False) -> pd.DataFrame:\n","    \"\"\"\n","    Main helper function to process documents from the EMR.\n","\n","    :param documents: Iterable containing documents which are strings\n","    :param document_ids: Iterable containing document unique identifiers\n","    :param document_type: String denoting the document type to be processed\n","    :param document_sections: List of sections for a given document type to process\n","    :param split_sentences: Flag to determine whether to further split sections into sentences\n","    :param filter_len: Minimum character length of a sentence (otherwise filter out)\n","    :param disable_progress_bar: Flag to disable tqdm progress bar\n","    :return: Pandas DataFrame containing the columns `document_id`, `text`, `section`, `offset`\n","    \"\"\"\n","\n","    df = sectionize_documents(documents, document_ids, disable_progress_bar)\n","\n","    if split_sentences:\n","        df = sentencize(df.text.values,\n","                        df.document_id.values,\n","                        df.offset.values,\n","                        filter_len,\n","                        disable_progress_bar)\n","    return df\n","\n","\n","def sectionize_documents(documents: Iterable[str],\n","                         document_ids: Iterable,\n","                         disable_progress_bar: bool = False) -> pd.DataFrame:\n","    \"\"\"\n","    Obtains the sections of the imaging reports and returns only the\n","    selected sections (defaults to FINDINGS, IMPRESSION, and ADDENDUM).\n","\n","    :param documents: Iterable containing documents which are strings\n","    :param document_ids: Iterable containing document unique identifiers\n","    :param disable_progress_bar: Flag to disable tqdm progress bar\n","    :return: Pandas DataFrame containing the columns `document_id`, `text`, `offset`\n","    \"\"\"\n","    processed_documents = []\n","    for document_id, document in tqdm(zip(document_ids, documents), total=len(documents), disable=disable_progress_bar):\n","        row = {}\n","        text, start, end = (document, 0, len(document))\n","        row['document_id'] = document_id\n","        row['text'] = text\n","        row['offset'] = (start, end)\n","\n","        processed_documents.append(row)\n","\n","    _df = pd.DataFrame(processed_documents)\n","    if _df.shape[0] > 0:\n","        return _df.sort_values(['document_id', 'offset']).reset_index(drop=True)\n","    else:\n","        return _df\n","\n","\n","def sentencize(documents: Iterable[str],\n","               document_ids: Iterable,\n","               offsets: Iterable[tuple[int, int]],\n","               filter_len: int = 3,\n","               disable_progress_bar: bool = False) -> pd.DataFrame:\n","    \"\"\"\n","    Split a document into sentences. Can be used with `sectionize_documents`\n","    to further split documents into more manageable pieces. Takes in offsets\n","    to ensure that after splitting, the sentences can be matched to the\n","    location in the original documents.\n","\n","    :param documents: Iterable containing documents which are strings\n","    :param document_ids: Iterable containing document unique identifiers\n","    :param offsets: Iterable tuple of the start and end indices\n","    :param filter_len: Minimum character length of a sentence (otherwise filter out)\n","    :return: Pandas DataFrame containing the columns `document_id`, `text`, `section`, `offset`\n","    \"\"\"\n","\n","    document_sentences = []\n","    for document, document_id, offset in tqdm(zip(documents, document_ids, offsets), total=len(documents),\n","                                              disable=disable_progress_bar):\n","        try:\n","            _, sentence_offsets = bf.text_to_sentences_and_offsets(document)\n","            for o in sentence_offsets:\n","                if o[1] - o[0] > filter_len:\n","                    sentence = document[o[0]:o[1]]\n","                    abs_offsets = (o[0] + offset[0], o[1] + offset[0])\n","                    row = {}\n","                    row['document_id'] = document_id\n","                    row['text'] = sentence\n","                    row['offset'] = abs_offsets\n","                    document_sentences.append(row)\n","        except:\n","            continue\n","    return pd.DataFrame(document_sentences)\n","\n","\n","def get_contexts(RUN_ON_KAGGLE, VAL_SIZE, file_for_local_cv):\n","    if RUN_ON_KAGGLE:\n","        SIM_MODEL = '/kaggle/input/sentencetransformers-allminilml6v2/sentence-transformers_all-MiniLM-L6-v2'\n","    else:\n","        SIM_MODEL = 'sentence-transformers/all-MiniLM-L6-v2'\n","        \n","    DEVICE = 0\n","    MAX_LENGTH = 384\n","    BATCH_SIZE = 16\n","\n","    if RUN_ON_KAGGLE:\n","        WIKI_PATH = \"/kaggle/input/wikipedia-20230701\"\n","    else:\n","        WIKI_PATH = \"/home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/wikipedia-2023-07-faiss-index\"\n","    wiki_files = os.listdir(WIKI_PATH)\n","\n","\n","    if RUN_ON_KAGGLE:\n","        trn = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\").drop(\"id\", axis=1)\n","    else:\n","        trn = pd.read_csv(file_for_local_cv,index_col=0).sample(n=VAL_SIZE, random_state=42).reset_index(drop=True)\n","\n","    model = SentenceTransformer(SIM_MODEL, device='cuda')\n","    model.max_seq_length = MAX_LENGTH\n","    model = model.half()\n","\n","    if RUN_ON_KAGGLE:\n","        sentence_index = read_index(\"/kaggle/input/wikipedia-2023-07-faiss-index/wikipedia_202307.index\")\n","    else:\n","        sentence_index = read_index(\"/home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/wikipedia-2023-07-faiss-index/wikipedia_202307.index\")\n","\n","    # prompt_embeddings = model.encode(trn.prompt.values, batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\n","    prompt_embeddings = model.encode(\n","        trn.apply(lambda row: f\"{row['prompt']}\\n{row['A']}\\n{row['B']}\\n{row['C']}\\n{row['D']}\\n{row['E']}\",\n","                  axis=1).values,\n","        batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\n","\n","    prompt_embeddings = prompt_embeddings.detach().cpu().numpy()\n","    _ = gc.collect()\n","\n","    # Get the top 20 pages that are likely to contain the topic of interest\n","    search_score, search_index = sentence_index.search(prompt_embeddings, 20)\n","\n","    # Save memory - delete sentence_index since it is no longer necessary\n","    del sentence_index\n","    del prompt_embeddings\n","    _ = gc.collect()\n","    libc.malloc_trim(0)\n","\n","    if RUN_ON_KAGGLE:\n","        df = pd.read_parquet(\"/kaggle/input/wikipedia-20230701/wiki_2023_index.parquet\", columns=['id', 'file'])\n","    else:\n","        df = pd.read_parquet(\"/home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/wikipedia-2023-07-faiss-index/wiki_2023_index.parquet\", columns=['id', 'file'])\n","\n","    # Get the article and associated file location using the index\n","    wikipedia_file_data = []\n","\n","    for i, (scr, idx) in tqdm(enumerate(zip(search_score, search_index)), total=len(search_score)):\n","        scr_idx = idx\n","        _df = df.loc[scr_idx].copy()\n","        _df['prompt_id'] = i\n","        wikipedia_file_data.append(_df)\n","    wikipedia_file_data = pd.concat(wikipedia_file_data).reset_index(drop=True)\n","    wikipedia_file_data = wikipedia_file_data[['id', 'prompt_id', 'file']].drop_duplicates().sort_values(\n","        ['file', 'id']).reset_index(drop=True)\n","\n","    # Save memory - delete df since it is no longer necessary\n","    del df\n","    _ = gc.collect()\n","    libc.malloc_trim(0)\n","\n","    # Get the full text data\n","    wiki_text_data = []\n","\n","    for file in tqdm(wikipedia_file_data.file.unique(), total=len(wikipedia_file_data.file.unique())):\n","        _id = [str(i) for i in wikipedia_file_data[wikipedia_file_data['file'] == file]['id'].tolist()]\n","        _df = pd.read_parquet(f\"{WIKI_PATH}/{file}\", columns=['id', 'text', 'title'])\n","\n","        _df_temp = _df[_df['id'].isin(_id)].copy()\n","        del _df\n","        _ = gc.collect()\n","        libc.malloc_trim(0)\n","        wiki_text_data.append(_df_temp)\n","    wiki_text_data = pd.concat(wiki_text_data).drop_duplicates().reset_index(drop=True)\n","    _ = gc.collect()\n","\n","    # Parse documents into sentences\n","    processed_wiki_text_data = process_documents(wiki_text_data.text.values, wiki_text_data.id.values)\n","\n","    # Get embeddings of the wiki text data\n","    wiki_data_embeddings = model.encode(processed_wiki_text_data.text,\n","                                        batch_size=BATCH_SIZE,\n","                                        device=DEVICE,\n","                                        show_progress_bar=True,\n","                                        convert_to_tensor=True,\n","                                        normalize_embeddings=True)  # .half()\n","    wiki_data_embeddings = wiki_data_embeddings.detach().cpu().numpy()\n","\n","    _ = gc.collect()\n","\n","    # Combine all answers\n","    trn['answer_all'] = trn.apply(lambda x: \" \".join([x['A'], x['B'], x['C'], x['D'], x['E']]), axis=1)\n","\n","    # Search using the prompt and answers to guide the search\n","    trn['prompt_answer_stem'] = trn['prompt'] + \" \" + trn['answer_all']\n","\n","    question_embeddings = model.encode(trn.prompt_answer_stem.values, batch_size=BATCH_SIZE, device=DEVICE,\n","                                       show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\n","    question_embeddings = question_embeddings.detach().cpu().numpy()\n","\n","    # Parameter to determine how many relevant sentences to include\n","    NUM_SENTENCES_INCLUDE = 6\n","\n","    # List containing just Context\n","    contexts = []\n","\n","    for r in tqdm(trn.itertuples(), total=len(trn)):\n","\n","        prompt_id = r.Index\n","\n","        prompt_indices = processed_wiki_text_data[processed_wiki_text_data['document_id'].isin(\n","            wikipedia_file_data[wikipedia_file_data['prompt_id'] == prompt_id]['id'].values)].index.values\n","\n","        if prompt_indices.shape[0] > 0:\n","            prompt_index = faiss.index_factory(wiki_data_embeddings.shape[1], \"Flat\")\n","            prompt_index.add(wiki_data_embeddings[prompt_indices])\n","\n","            context = \"\"\n","\n","            # Get the top matches\n","            ss, ii = prompt_index.search(question_embeddings, NUM_SENTENCES_INCLUDE)\n","            for _s, _i in zip(ss[prompt_id], ii[prompt_id]):\n","                context += processed_wiki_text_data.loc[prompt_indices]['text'].iloc[_i] + \" \"\n","        contexts.append(context)\n","\n","    trn['context'] = contexts\n","\n","    trn[[\"prompt\", \"context\", \"A\", \"B\", \"C\", \"D\", \"E\"]].to_csv(\"./test_context.csv\", index=False)\n","\n","\n","# def generate_openbook_output(RUN_ON_KAGGLE):\n","#     test_df = pd.read_csv(\"test_context.csv\")\n","#     test_df.index = list(range(len(test_df)))\n","#     test_df['id'] = list(range(len(test_df)))\n","#     test_df[\"prompt\"] = test_df[\"context\"].apply(lambda x: x[:1750]) + \" #### \" + test_df[\"prompt\"]\n","#     test_df['answer'] = 'A'\n","#     if RUN_ON_KAGGLE:\n","#         model_dir = \"/kaggle/input/llm-science-run-context-2\"\n","#     else:\n","#         model_dir = \"/home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/llm-science-run-context-2\"\n","#     tokenizer = AutoTokenizer.from_pretrained(model_dir)\n","#     model = AutoModelForMultipleChoice.from_pretrained(model_dir).cuda()\n","#     model.eval()\n","\n","#     # We'll create a dictionary to convert option names (A, B, C, D, E) into indices and back again\n","#     options = 'ABCDE'\n","#     indices = list(range(5))\n","\n","#     option_to_index = {option: index for option, index in zip(options, indices)}\n","#     index_to_option = {index: option for option, index in zip(options, indices)}\n","\n","#     def preprocess(example):\n","#         # The AutoModelForMultipleChoice class expects a set of question/answer pairs\n","#         # so we'll copy our question 5 times before tokenizing\n","#         first_sentence = [example['prompt']] * 5\n","#         second_sentence = []\n","#         for option in options:\n","#             second_sentence.append(example[option])\n","#         # Our tokenizer will turn our text into token IDs BERT can understand\n","#         tokenized_example = tokenizer(first_sentence, second_sentence, truncation=True)\n","#         tokenized_example['label'] = option_to_index[example['answer']]\n","#         return tokenized_example\n","\n","#     tokenized_test_dataset = Dataset.from_pandas(test_df[['id', 'prompt', 'A', 'B', 'C', 'D', 'E', 'answer']].drop(columns=['id'])).map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])\n","#     tokenized_test_dataset = tokenized_test_dataset.remove_columns([\"__index_level_0__\"])\n","#     data_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\n","#     test_dataloader = DataLoader(tokenized_test_dataset, batch_size=1, shuffle=False, collate_fn=data_collator)\n","\n","#     test_predictions = []\n","#     for batch in test_dataloader:\n","#         for k in batch.keys():\n","#             batch[k] = batch[k].cuda()\n","#         with torch.no_grad():\n","#             outputs = model(**batch)\n","#         test_predictions.append(outputs.logits.cpu().detach())\n","\n","#     test_predictions = torch.cat(test_predictions)\n","\n","#     predictions_as_ids = np.argsort(-test_predictions, 1)\n","\n","#     predictions_as_answer_letters = np.array(list('ABCDE'))[predictions_as_ids]\n","#     # predictions_as_answer_letters[:3]\n","\n","#     predictions_as_string = test_df['prediction'] = [\n","#         ' '.join(row) for row in predictions_as_answer_letters[:, :3]\n","#     ]\n","\n","#     submission = test_df[['id', 'prediction']]\n","#     submission.to_csv('submission_backup.csv', index=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pickle\n","\n","get_contexts(RUN_ON_KAGGLE, VAL_SIZE, file_for_local_cv)\n","# generate_openbook_output(RUN_ON_KAGGLE)\n","\n","import gc\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if RUN_ON_KAGGLE:\n","    !cp -r /kaggle/input/stem-wiki-cohere-no-emb /kaggle/working\n","    !cp -r /kaggle/input/all-paraphs-parsed-expanded /kaggle/working/"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from datasets import load_dataset, load_from_disk"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","import unicodedata\n","\n","\n","def SplitList(mylist, chunk_size):\n","    return [mylist[offs:offs+chunk_size] for offs in range(0, len(mylist), chunk_size)]\n","\n","def get_relevant_documents_parsed(df_test):\n","    df_chunk_size=600\n","    if RUN_ON_KAGGLE:\n","        paraphs_parsed_dataset = load_from_disk(\"/kaggle/working/all-paraphs-parsed-expanded\")\n","    else:\n","        paraphs_parsed_dataset = load_from_disk(\"/home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/all-paraphs-parsed-expanded\")\n","    \n","    modified_texts = paraphs_parsed_dataset.map(lambda example:\n","                                             {'temp_text':\n","                                              f\"{example['title']} {example['section']} {example['text']}\".replace('\\n',\" \").replace(\"'\",\"\")},\n","                                             num_proc=2)[\"temp_text\"]\n","    \n","    all_articles_indices = []\n","    all_articles_values = []\n","    for idx in tqdm(range(0, df_test.shape[0], df_chunk_size)):\n","        df_test_ = df_test.iloc[idx: idx+df_chunk_size]\n","    \n","        articles_indices, merged_top_scores = retrieval(df_test_, modified_texts)\n","        all_articles_indices.append(articles_indices)\n","        all_articles_values.append(merged_top_scores)\n","        \n","    article_indices_array =  np.concatenate(all_articles_indices, axis=0)\n","    articles_values_array = np.concatenate(all_articles_values, axis=0).reshape(-1)\n","    \n","    top_per_query = article_indices_array.shape[1]\n","    articles_flatten = [(\n","                         articles_values_array[index],\n","                         paraphs_parsed_dataset[idx.item()][\"title\"],\n","                         paraphs_parsed_dataset[idx.item()][\"text\"],\n","                        )\n","                        for index,idx in enumerate(article_indices_array.reshape(-1))]\n","    retrieved_articles = SplitList(articles_flatten, top_per_query)\n","    return retrieved_articles\n","\n","\n","\n","def get_relevant_documents(df_test):\n","    df_chunk_size=800\n","    if RUN_ON_KAGGLE:\n","        cohere_dataset_filtered = load_from_disk(\"/kaggle/working/stem-wiki-cohere-no-emb\")\n","    else:\n","        cohere_dataset_filtered = load_from_disk(\"/home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/wiki-stem-cohere\")\n","    modified_texts = cohere_dataset_filtered.map(lambda example:\n","                                             {'temp_text':\n","                                              unicodedata.normalize(\"NFKD\", f\"{example['title']} {example['text']}\").replace('\"',\"\")},\n","                                             num_proc=2)[\"temp_text\"]\n","    \n","    all_articles_indices = []\n","    all_articles_values = []\n","    for idx in tqdm(range(0, df_test.shape[0], df_chunk_size)):\n","        df_test_ = df_test.iloc[idx: idx+df_chunk_size]\n","    \n","        articles_indices, merged_top_scores = retrieval(df_test_, modified_texts)\n","        all_articles_indices.append(articles_indices)\n","        all_articles_values.append(merged_top_scores)\n","        \n","    article_indices_array =  np.concatenate(all_articles_indices, axis=0)\n","    articles_values_array = np.concatenate(all_articles_values, axis=0).reshape(-1)\n","    \n","    top_per_query = article_indices_array.shape[1]\n","    articles_flatten = [(\n","                         articles_values_array[index],\n","                         cohere_dataset_filtered[idx.item()][\"title\"],\n","                         unicodedata.normalize(\"NFKD\", cohere_dataset_filtered[idx.item()][\"text\"]),\n","                        )\n","                        for index,idx in enumerate(article_indices_array.reshape(-1))]\n","    retrieved_articles = SplitList(articles_flatten, top_per_query)\n","    return retrieved_articles\n","\n","\n","\n","def retrieval(df_test, modified_texts):\n","    \n","    corpus_df_test = df_test.apply(lambda row:\n","                                     f'{row[\"prompt\"]}\\n{row[\"prompt\"]}\\n{row[\"prompt\"]}\\n{row[\"A\"]}\\n{row[\"B\"]}\\n{row[\"C\"]}\\n{row[\"D\"]}\\n{row[\"E\"]}',\n","                                     axis=1).values\n","    vectorizer1 = TfidfVectorizer(ngram_range=(1,2),\n","                                 token_pattern=r\"(?u)\\b[\\w/.-]+\\b|!|/|\\?|\\\"|\\'\",\n","                                 stop_words=stop_words)\n","    vectorizer1.fit(corpus_df_test)\n","    vocab_df_test = vectorizer1.get_feature_names_out()\n","    vectorizer = TfidfVectorizer(ngram_range=(1,2),\n","                                 token_pattern=r\"(?u)\\b[\\w/.-]+\\b|!|/|\\?|\\\"|\\'\",\n","                                 stop_words=stop_words,\n","                                 vocabulary=vocab_df_test)\n","    vectorizer.fit(modified_texts[:500000])\n","    corpus_tf_idf = vectorizer.transform(corpus_df_test)\n","    \n","    print(f\"length of vectorizer vocab is {len(vectorizer.get_feature_names_out())}\")\n","\n","    chunk_size = 100000\n","    top_per_chunk = 10\n","    top_per_query = 10\n","\n","    all_chunk_top_indices = []\n","    all_chunk_top_values = []\n","\n","    for idx in tqdm(range(0, len(modified_texts), chunk_size)):\n","        wiki_vectors = vectorizer.transform(modified_texts[idx: idx+chunk_size])\n","        temp_scores = (corpus_tf_idf * wiki_vectors.T).toarray()\n","        chunk_top_indices = temp_scores.argpartition(-top_per_chunk, axis=1)[:, -top_per_chunk:]\n","        chunk_top_values = temp_scores[np.arange(temp_scores.shape[0])[:, np.newaxis], chunk_top_indices]\n","\n","        all_chunk_top_indices.append(chunk_top_indices + idx)\n","        all_chunk_top_values.append(chunk_top_values)\n","\n","    top_indices_array = np.concatenate(all_chunk_top_indices, axis=1)\n","    top_values_array = np.concatenate(all_chunk_top_values, axis=1)\n","    \n","    merged_top_scores = np.sort(top_values_array, axis=1)[:,-top_per_query:]\n","    merged_top_indices = top_values_array.argsort(axis=1)[:,-top_per_query:]\n","    articles_indices = top_indices_array[np.arange(top_indices_array.shape[0])[:, np.newaxis], merged_top_indices]\n","    \n","    return articles_indices, merged_top_scores\n","\n","\n","def prepare_answering_input(\n","        tokenizer, \n","        question,  \n","        options,   \n","        context,   \n","        max_seq_length=4096,\n","    ):\n","    c_plus_q   = context + ' ' + tokenizer.bos_token + ' ' + question\n","    c_plus_q_4 = [c_plus_q] * len(options)\n","    tokenized_examples = tokenizer(\n","        c_plus_q_4, options,\n","        max_length=max_seq_length,\n","        padding=\"longest\",\n","        truncation=False,\n","        return_tensors=\"pt\",\n","    )\n","    input_ids = tokenized_examples['input_ids'].unsqueeze(0)\n","    attention_mask = tokenized_examples['attention_mask'].unsqueeze(0)\n","    example_encoded = {\n","        \"input_ids\": input_ids.to(model.device.index),\n","        \"attention_mask\": attention_mask.to(model.device.index),\n","    }\n","    return example_encoded"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["stop_words = ['each', 'you', 'the', 'use', 'used',\n","                  'where', 'themselves', 'nor', \"it's\", 'how', \"don't\", 'just', 'your',\n","                  'about', 'himself', 'with', \"weren't\", 'hers', \"wouldn't\", 'more', 'its', 'were',\n","                  'his', 'their', 'then', 'been', 'myself', 're', 'not',\n","                  'ours', 'will', 'needn', 'which', 'here', 'hadn', 'it', 'our', 'there', 'than',\n","                  'most', \"couldn't\", 'both', 'some', 'for', 'up', 'couldn', \"that'll\",\n","                  \"she's\", 'over', 'this', 'now', 'until', 'these', 'few', 'haven',\n","                  'of', 'wouldn', 'into', 'too', 'to', 'very', 'shan', 'before', 'the', 'they',\n","                  'between', \"doesn't\", 'are', 'was', 'out', 'we', 'me',\n","                  'after', 'has', \"isn't\", 'have', 'such', 'should', 'yourselves', 'or', 'during', 'herself',\n","                  'doing', 'in', \"shouldn't\", \"won't\", 'when', 'do', 'through', 'she',\n","                  'having', 'him', \"haven't\", 'against', 'itself', 'that',\n","                  'did', 'theirs', 'can', 'those',\n","                  'own', 'so', 'and', 'who', \"you've\", 'yourself', 'her', 'he', 'only',\n","                  'what', 'ourselves', 'again', 'had', \"you'd\", 'is', 'other',\n","                  'why', 'while', 'from', 'them', 'if', 'above', 'does', 'whom',\n","                  'yours', 'but', 'being', \"wasn't\", 'be']"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[],"source":["if RUN_ON_KAGGLE:\n","    df_valid = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\")\n","else:\n","    df_valid = pd.read_csv(file_for_local_cv).sample(n=VAL_SIZE, random_state=42).reset_index(drop=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["retrieved_articles_parsed = get_relevant_documents_parsed(df_valid)\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_valid"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["retrieved_articles = get_relevant_documents(df_valid)\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# empty cuda cache\n","torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from transformers import LongformerTokenizer, LongformerForMultipleChoice\n","\n","if RUN_ON_KAGGLE:\n","    tokenizer = LongformerTokenizer.from_pretrained(\"/kaggle/input/longformer-race-model/longformer_qa_model\")\n","    model = LongformerForMultipleChoice.from_pretrained(\"/kaggle/input/longformer-race-model/longformer_qa_model\").cuda()\n","else:\n","    tokenizer = LongformerTokenizer.from_pretrained(\"/home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/longformer-race-model/longformer_qa_model\")\n","    model = LongformerForMultipleChoice.from_pretrained(\"/home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/longformer-race-model/longformer_qa_model\").cuda()\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["predictions = []\n","\n","longformer_contexts = []\n","\n","\n","for index in tqdm(range(df_valid.shape[0])):\n","    row = df_valid.iloc[index]\n","    # question is 'prompt'\n","    question = row['prompt']\n","    options = [row['A'], row['B'], row['C'], row['D'], row['E']]\n","    context1 = f\"{retrieved_articles[index][-4][2]}\\n{retrieved_articles[index][-3][2]}\\n{retrieved_articles[index][-2][2]}\\n{retrieved_articles[index][-1][2]}\"\n","    \n","    longformer_contexts.append(context1)\n","    "]},{"cell_type":"markdown","metadata":{},"source":["# viktor 8bit with longformer context"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# model_dir = \"/kaggle/input/how-to-train-open-book-model-part-1/model_v2\"\n","if RUN_ON_KAGGLE:\n","    pass # not uploaded yet\n","else:\n","    model_dir = \"/home/viktor/Documents/kaggle/kaggle_llm/work_dirs/160k-viktor-and-deotte-dataset-deotte-preproc-deberta/deberta-v3-large-2023-09-17-10-00-20/checkpoint-14400\"\n","    \n","tokenizer = AutoTokenizer.from_pretrained(model_dir)\n","model = AutoModelForMultipleChoice.from_pretrained(model_dir, load_in_8bit=True)\n","model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_valid['context'] = longformer_contexts\n","df_valid['context_len'] = df_valid['context'].apply(lambda x: len(x))\n","import matplotlib.pyplot as plt\n","\n","plt.hist(df_valid['context_len'], bins=100);"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_valid.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tokenized_test_dataset = Dataset.from_pandas(df_valid[['id', 'prompt', 'A', 'B', 'C', 'D', 'E', 'answer', 'context']].drop(columns=['id'])).map(preprocess_for_8bit, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer', 'context'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if 'answer' in df_valid.columns:\n","    tokenized_test_dataset = Dataset.from_pandas(df_valid[['id', 'prompt', 'A', 'B', 'C', 'D', 'E', 'answer', 'context']].drop(columns=['id'])).map(preprocess_for_8bit, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer', 'context'])\n","    # tokenized_test_dataset = tokenized_test_dataset.remove_columns([\"__index_level_0__\"])\n","    data_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\n","    test_dataloader = DataLoader(tokenized_test_dataset, batch_size=1, shuffle=False, collate_fn=data_collator)\n","else:\n","    tokenized_test_dataset = Dataset.from_pandas(df_valid[['id', 'prompt', 'A', 'B', 'C', 'D', 'E', 'context']].drop(columns=['id'])).map(preprocess_for_8bit, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'context'])\n","    # tokenized_test_dataset = tokenized_test_dataset.remove_columns([\"__index_level_0__\"])\n","    data_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\n","    test_dataloader = DataLoader(tokenized_test_dataset, batch_size=1, shuffle=False, collate_fn=data_collator)\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_predictions_viktor_160k_8bit_lf_context = []\n","\n","for batch in tqdm(test_dataloader):\n","    for k in batch.keys():\n","        batch[k] = batch[k].cuda()\n","    with torch.no_grad():\n","        outputs = model(**batch)\n","    test_predictions_viktor_160k_8bit_lf_context.append(outputs.logits.cpu().detach().squeeze().numpy())\n","    \n","\n","# test_predictions_viktor_160k_8bit_lf_context = torch.cat(test_predictions_viktor_160k_8bit_lf_context)\n","test_predictions_viktor_160k_8bit_lf_context = np.array(test_predictions_viktor_160k_8bit_lf_context)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_predictions_viktor_160k_8bit_lf_context.shape"]},{"cell_type":"markdown","metadata":{},"source":["# combine predictions (and find best combination on local CV )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_predictions_viktor_160k_8bit_lf_context"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["preds_dict = {\n","    'test_predictions_viktor_160k_8bit_lf_context':test_predictions_viktor_160k_8bit_lf_context,\n","}\n","# save preds_dict as pickle\n","with open('preds_dict_deberta.pickle', 'wb') as handle:\n","    pickle.dump(preds_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[1.]\n","0.48383333333333295\n"]}],"source":["\n","import pickle\n","\n","# load preds_dict_deberta.pickle as preds_dict\n","with open('preds_dict_deberta.pickle', 'rb') as handle:\n","    preds_dict = pickle.load(handle)\n","    \n","preds_dict\n","\n","weights = np.ones(len(preds_dict))\n","\n","predictions_overall = np.sum([preds_val*weight for preds_val, weight in zip(preds_dict.values(), weights)], axis=0)\n","predictions_overall = np.argsort(-predictions_overall)[:,:3]\n","predictions_as_answer_letters = np.array(list('ABCDE'))[predictions_overall]\n","df_valid['prediction'] = [' '.join(pred) for pred in predictions_as_answer_letters]\n","\n","def compute_map3(preds, actuals):\n","    correct_answers = [0, 0, 0]\n","    for pred, actual in zip(preds, actuals):\n","        if pred[0] == actual:\n","            correct_answers[0] += 1\n","        if pred[1] == actual:\n","            correct_answers[1] += 1./2\n","        if pred[2] == actual:\n","            correct_answers[2] += 1./3\n","    \n","    n_total = len(actuals)\n","    map3 = np.sum(correct_answers) / n_total\n","    \n","    return map3\n","\n","if not RUN_ON_KAGGLE and 'answer' in df_valid.columns:\n","    val_df = pd.read_csv(file_for_local_cv,index_col=0).sample(n=VAL_SIZE, random_state=42).reset_index(drop=True)\n","    \n","    val_df['A'] = val_df['A'].map(str)\n","    val_df['B'] = val_df['B'].map(str)\n","    val_df['C'] = val_df['C'].map(str)\n","    val_df['D'] = val_df['D'].map(str)\n","    val_df['E'] = val_df['E'].map(str)\n","    val_df['answer'] = val_df['answer'].map(str)\n","\n","    val_df = val_df.reset_index(drop=True)\n","    \n","    \n","    weights = np.ones(len(preds_dict))\n","    predictions_overall = np.sum([preds_val*weight for preds_val, weight in zip(preds_dict.values(), weights)], axis=0)\n","    predictions_overall = np.argsort(-predictions_overall)[:,:3]\n","    predictions_as_answer_letters = np.array(list('ABCDE'))[predictions_overall]\n","\n","\n","\n","    map3_score = compute_map3(predictions_as_answer_letters, val_df['answer'].values)\n","\n","\n","    best_map3_score = map3_score\n","    print(weights)\n","    print(map3_score)\n","\n","    \n","    # best_map3_score = 0\n","    # for i in tqdm(range(10000)):\n","    #     weights = np.random.uniform(0, 1, len(preds_dict))\n","    #     predictions_overall = np.sum([preds_val*weight for preds_val in zip(preds_dict.values(), weights)], axis=0)\n","    #     predictions_overall = np.argsort(-predictions_overall)[:,:3]\n","    #     predictions_as_answer_letters = np.array(list('ABCDE'))[predictions_overall]\n","\n","\n","    \n","    #     map3_score = compute_map3(predictions_as_answer_letters, val_df['answer'].values)\n","    \n","    #     if map3_score > best_map3_score:\n","    #         best_map3_score = map3_score\n","    #         print(weights)\n","    #         print(map3_score)\n","    \n","    # print(map3_score)\n","    \n","    "]},{"cell_type":"markdown","metadata":{},"source":["Experiments:\n","\n","- all models: 0.8"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["submission = df_valid[['id', 'prediction']]\n","submission.to_csv('submission.csv', index=False)\n","\n","pd.read_csv('submission.csv').head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":4}
