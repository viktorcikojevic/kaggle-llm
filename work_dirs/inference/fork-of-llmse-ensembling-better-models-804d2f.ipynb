{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-09-25T21:32:16.823181Z","iopub.status.busy":"2023-09-25T21:32:16.822903Z","iopub.status.idle":"2023-09-25T21:32:16.836270Z","shell.execute_reply":"2023-09-25T21:32:16.835164Z","shell.execute_reply.started":"2023-09-25T21:32:16.823155Z"},"trusted":true},"outputs":[],"source":["RUN_ON_KAGGLE = False\n","DEBUG = True"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-09-25T21:32:16.838813Z","iopub.status.busy":"2023-09-25T21:32:16.838485Z"},"trusted":true},"outputs":[],"source":["%%capture\n","if RUN_ON_KAGGLE:\n","    # installing offline dependencies\n","    !pip install -U /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n","    !cp -rf /kaggle/input/sentence-transformers-222/sentence-transformers /kaggle/working/sentence-transformers\n","    !pip install -U /kaggle/working/sentence-transformers\n","    !pip install -U /kaggle/input/blingfire-018/blingfire-0.1.8-py3-none-any.whl\n","\n","    !pip install --no-index --no-deps /kaggle/input/llm-whls/transformers-4.31.0-py3-none-any.whl\n","    !pip install --no-index --no-deps /kaggle/input/llm-whls/peft-0.4.0-py3-none-any.whl\n","    !pip install --no-index --no-deps /kaggle/input/llm-whls/datasets-2.14.3-py3-none-any.whl\n","    !pip install --no-index --no-deps /kaggle/input/llm-whls/trl-0.5.0-py3-none-any.whl"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","===================================BUG REPORT===================================\n","Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n","================================================================================\n","CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n","CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n","CUDA SETUP: Highest compute capability among GPUs detected: 8.9\n","CUDA SETUP: Detected CUDA version 121\n","CUDA SETUP: Loading binary /home/viktor/miniconda3/envs/torch-env/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda121.so...\n"]},{"name":"stderr","output_type":"stream","text":["/home/viktor/miniconda3/envs/torch-env/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /home/viktor/miniconda3/envs/torch-env did not contain libcudart.so as expected! Searching further paths...\n","  warn(msg)\n","/home/viktor/miniconda3/envs/torch-env/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n","  warn(msg)\n","/home/viktor/miniconda3/envs/torch-env/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n","  warn(msg)\n"]}],"source":["import os, time\n","import gc\n","import pandas as pd\n","import numpy as np\n","import re\n","from tqdm.auto import tqdm\n","import blingfire as bf\n","from __future__ import annotations\n","\n","from collections.abc import Iterable\n","\n","import faiss\n","from faiss import write_index, read_index\n","\n","from sentence_transformers import SentenceTransformer\n","\n","import torch\n","import ctypes\n","libc = ctypes.CDLL(\"libc.so.6\")\n","\n","from dataclasses import dataclass\n","from typing import Optional, Union\n","\n","import torch\n","import numpy as np\n","import pandas as pd\n","from datasets import Dataset\n","from transformers import AutoTokenizer\n","from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n","from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n","from torch.utils.data import DataLoader\n","\n","from scipy.special import softmax"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[],"source":["if RUN_ON_KAGGLE:\n","    SIM_MODEL = '/kaggle/input/sentencetransformers-allminilml6v2/sentence-transformers_all-MiniLM-L6-v2'\n","else:\n","    SIM_MODEL = 'sentence-transformers/all-MiniLM-L6-v2'\n","DEVICE = 0\n","MAX_LENGTH = 384\n","BATCH_SIZE = 32\n","\n","\n","FILTER_LEN = 1 if DEBUG else 9\n","IND_SEARCH = 1 if DEBUG else 7\n","NUM_SENTENCES_INCLUDE = 1 if DEBUG else 25\n","CONTEXT_LEN = 1000 if DEBUG else 2305\n","VAL_SIZE = 5 if DEBUG else 3000\n","\n","if RUN_ON_KAGGLE:\n","    WIKI_PATH = \"/kaggle/input/wikipedia-20230701\"\n","else:\n","    WIKI_PATH = \"/home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/wikipedia-2023-07-faiss-index\"\n","    \n","file_for_local_cv = \"/home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-llm-science-exam/test.csv\"    \n","# file_for_local_cv = \"/home/viktor/Documents/kaggle/kaggle_llm/data/data_dumps/more_questions/more_questions_raw_questions_wiki_sci_3_shuffled.csv\"    \n","\n","wiki_files = os.listdir(WIKI_PATH)"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[],"source":["def process_documents(documents: Iterable[str],\n","                      document_ids: Iterable,\n","                      split_sentences: bool = True,\n","                      filter_len: int = FILTER_LEN,\n","                      disable_progress_bar: bool = False) -> pd.DataFrame:\n","    \n","    df = sectionize_documents(documents, document_ids, disable_progress_bar)\n","\n","    if split_sentences:\n","        df = sentencize(df.text.values, \n","                        df.document_id.values,\n","                        df.offset.values, \n","                        filter_len, \n","                        disable_progress_bar)\n","    return df\n","\n","\n","def sectionize_documents(documents: Iterable[str],\n","                         document_ids: Iterable,\n","                         disable_progress_bar: bool = False) -> pd.DataFrame:\n","\n","    processed_documents = []\n","    for document_id, document in tqdm(zip(document_ids, documents), total=len(documents), disable=disable_progress_bar):\n","        row = {}\n","        text, start, end = (document, 0, len(document))\n","        row['document_id'] = document_id\n","        row['text'] = text\n","        row['offset'] = (start, end)\n","\n","        processed_documents.append(row)\n","\n","    _df = pd.DataFrame(processed_documents)\n","    if _df.shape[0] > 0:\n","        return _df.sort_values(['document_id', 'offset']).reset_index(drop=True)\n","    else:\n","        return _df\n","\n","\n","def sentencize(documents: Iterable[str],\n","               document_ids: Iterable,\n","               offsets: Iterable[tuple[int, int]],\n","               filter_len: int = FILTER_LEN,\n","               disable_progress_bar: bool = False) -> pd.DataFrame:\n","\n","    document_sentences = []\n","    for document, document_id, offset in tqdm(zip(documents, document_ids, offsets), total=len(documents), disable=disable_progress_bar):\n","        try:\n","            _, sentence_offsets = bf.text_to_sentences_and_offsets(document)\n","            for o in sentence_offsets:\n","                if o[1]-o[0] > filter_len:\n","                    sentence = document[o[0]:o[1]]\n","                    abs_offsets = (o[0]+offset[0], o[1]+offset[0])\n","                    row = {}\n","                    row['document_id'] = document_id\n","                    row['text'] = sentence\n","                    row['offset'] = abs_offsets\n","                    document_sentences.append(row)\n","        except:\n","            continue\n","    return pd.DataFrame(document_sentences)"]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>prompt</th>\n","      <th>A</th>\n","      <th>B</th>\n","      <th>C</th>\n","      <th>D</th>\n","      <th>E</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>What is the main sequence in astronomy?</td>\n","      <td>The main sequence is a type of galaxy that con...</td>\n","      <td>The main sequence is a type of black hole that...</td>\n","      <td>The main sequence is a continuous and distinct...</td>\n","      <td>The main sequence is a group of planets that o...</td>\n","      <td>The main sequence is a type of nebula that is ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>What is the \"ultraviolet catastrophe\"?</td>\n","      <td>It is a phenomenon that occurs only in multi-m...</td>\n","      <td>It is the misbehavior of a formula for higher ...</td>\n","      <td>It is the standing wave of a string in harmoni...</td>\n","      <td>It is a flaw in classical physics that results...</td>\n","      <td>It is a disproven theory about the distributio...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>What is the Carnot engine?</td>\n","      <td>The Carnot engine is a theoretical engine that...</td>\n","      <td>The Carnot engine is an ideal heat engine that...</td>\n","      <td>The Carnot engine is a real heat engine that o...</td>\n","      <td>The Carnot engine is a theoretical engine that...</td>\n","      <td>The Carnot engine is a real engine that operat...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>What is accelerator-based light-ion fusion?</td>\n","      <td>Accelerator-based light-ion fusion is a techni...</td>\n","      <td>Accelerator-based light-ion fusion is a techni...</td>\n","      <td>Accelerator-based light-ion fusion is a techni...</td>\n","      <td>Accelerator-based light-ion fusion is a techni...</td>\n","      <td>Accelerator-based light-ion fusion is a techni...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>What is a \"coffee ring\" in physics?</td>\n","      <td>A type of coffee that is made by boiling coffe...</td>\n","      <td>A pattern left by a particle-laden liquid afte...</td>\n","      <td>A type of coffee that is made by mixing instan...</td>\n","      <td>A type of coffee that is made by pouring hot w...</td>\n","      <td>A pattern left by a particle-laden liquid afte...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                        prompt   \n","0      What is the main sequence in astronomy?  \\\n","1       What is the \"ultraviolet catastrophe\"?   \n","2                   What is the Carnot engine?   \n","3  What is accelerator-based light-ion fusion?   \n","4          What is a \"coffee ring\" in physics?   \n","\n","                                                   A   \n","0  The main sequence is a type of galaxy that con...  \\\n","1  It is a phenomenon that occurs only in multi-m...   \n","2  The Carnot engine is a theoretical engine that...   \n","3  Accelerator-based light-ion fusion is a techni...   \n","4  A type of coffee that is made by boiling coffe...   \n","\n","                                                   B   \n","0  The main sequence is a type of black hole that...  \\\n","1  It is the misbehavior of a formula for higher ...   \n","2  The Carnot engine is an ideal heat engine that...   \n","3  Accelerator-based light-ion fusion is a techni...   \n","4  A pattern left by a particle-laden liquid afte...   \n","\n","                                                   C   \n","0  The main sequence is a continuous and distinct...  \\\n","1  It is the standing wave of a string in harmoni...   \n","2  The Carnot engine is a real heat engine that o...   \n","3  Accelerator-based light-ion fusion is a techni...   \n","4  A type of coffee that is made by mixing instan...   \n","\n","                                                   D   \n","0  The main sequence is a group of planets that o...  \\\n","1  It is a flaw in classical physics that results...   \n","2  The Carnot engine is a theoretical engine that...   \n","3  Accelerator-based light-ion fusion is a techni...   \n","4  A type of coffee that is made by pouring hot w...   \n","\n","                                                   E  \n","0  The main sequence is a type of nebula that is ...  \n","1  It is a disproven theory about the distributio...  \n","2  The Carnot engine is a real engine that operat...  \n","3  Accelerator-based light-ion fusion is a techni...  \n","4  A pattern left by a particle-laden liquid afte...  "]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["if RUN_ON_KAGGLE:\n","    trn = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\").drop(\"id\", axis=1)\n","else:\n","    trn = pd.read_csv(file_for_local_cv).drop(\"id\", axis=1).sample(n=VAL_SIZE, random_state=42).reset_index(drop=True)\n","trn.head()"]},{"cell_type":"code","execution_count":7,"metadata":{"trusted":true},"outputs":[],"source":["model = SentenceTransformer(SIM_MODEL, device='cuda')\n","model.max_seq_length = MAX_LENGTH\n","model = model.half()"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[],"source":["if RUN_ON_KAGGLE:\n","    sentence_index = read_index(\"/kaggle/input/wikipedia-2023-07-faiss-index/wikipedia_202307.index\")\n","else:\n","    sentence_index = read_index(\"/home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/wikipedia-2023-07-faiss-index/wikipedia_202307.index\")"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ce94bb9ba98c4e2abd1e72197c7dcb76","version_major":2,"version_minor":0},"text/plain":["Batches:   0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["prompt_embeddings = model.encode(trn.prompt.values, batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\n","prompt_embeddings = prompt_embeddings.detach().cpu().numpy()"]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[],"source":["## Get the top IND_SEARCH pages that are likely to contain the topic of interest\n","search_score, search_index = sentence_index.search(prompt_embeddings, IND_SEARCH)"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["1"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["## Save memory - delete sentence_index since it is no longer necessary\n","del sentence_index\n","del prompt_embeddings\n","_ = gc.collect()\n","libc.malloc_trim(0)"]},{"cell_type":"markdown","metadata":{},"source":["# Getting Sentences from the Relevant Titles"]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[],"source":["if RUN_ON_KAGGLE:\n","    df = pd.read_parquet(\"/kaggle/input/wikipedia-20230701/wiki_2023_index.parquet\", columns=['id', 'file'])\n","else:\n","    df = pd.read_parquet(\"/home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/wikipedia-2023-07-faiss-index/wiki_2023_index.parquet\", columns=['id', 'file'])"]},{"cell_type":"code","execution_count":13,"metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1f0e5d228307486c827488b84796c6ff","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["1"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["## Get the article and associated file location using the index\n","wikipedia_file_data = []\n","\n","for i, (scr, idx) in tqdm(enumerate(zip(search_score, search_index)), total=len(search_score)):\n","    scr_idx = idx\n","    _df = df.loc[scr_idx].copy()\n","    _df['prompt_id'] = i\n","    wikipedia_file_data.append(_df)\n","wikipedia_file_data = pd.concat(wikipedia_file_data).reset_index(drop=True)\n","wikipedia_file_data = wikipedia_file_data[['id', 'prompt_id', 'file']].drop_duplicates().sort_values(['file', 'id']).reset_index(drop=True)\n","\n","\n","del df\n","_ = gc.collect()\n","libc.malloc_trim(0)"]},{"cell_type":"code","execution_count":14,"metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"94302fc79dad4d12bc177fc2140fb66d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["## Get the full text data\n","wiki_text_data = []\n","\n","for file in tqdm(wikipedia_file_data.file.unique(), total=len(wikipedia_file_data.file.unique())):\n","    _id = [str(i) for i in wikipedia_file_data[wikipedia_file_data['file']==file]['id'].tolist()]\n","    _df = pd.read_parquet(f\"{WIKI_PATH}/{file}\", columns=['id', 'text'])\n","\n","    _df_temp = _df[_df['id'].isin(_id)].copy()\n","    del _df\n","    _ = gc.collect()\n","    libc.malloc_trim(0)\n","    wiki_text_data.append(_df_temp)\n","wiki_text_data = pd.concat(wiki_text_data).drop_duplicates().reset_index(drop=True)\n","\n","_ = gc.collect()"]},{"cell_type":"code","execution_count":15,"metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8de7ffa691bc4830bb907f7a3dba4af8","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dc8dd9e0d3fd421d8fe37bafb2823644","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["processed_wiki_text_data = process_documents(wiki_text_data.text.values, wiki_text_data.id.values)"]},{"cell_type":"code","execution_count":16,"metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4373b67e70914b74ad2036e4364cd9e0","version_major":2,"version_minor":0},"text/plain":["Batches:   0%|          | 0/14 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["\n","wiki_data_embeddings = model.encode(processed_wiki_text_data.text,\n","                                    batch_size=BATCH_SIZE,\n","                                    device=DEVICE,\n","                                    show_progress_bar=True,\n","                                    convert_to_tensor=True,\n","                                    normalize_embeddings=True)#.half()\n","wiki_data_embeddings = wiki_data_embeddings.detach().cpu().numpy()\n"]},{"cell_type":"code","execution_count":17,"metadata":{"trusted":true},"outputs":[],"source":["_ = gc.collect()"]},{"cell_type":"code","execution_count":18,"metadata":{"trusted":true},"outputs":[],"source":["trn['answer_all'] = trn.apply(lambda x: \" \".join([x['A'], x['B'], x['C'], x['D'], x['E']]), axis=1)\n","trn['prompt_answer_stem'] = trn['prompt'] + \" \" + trn['answer_all']"]},{"cell_type":"code","execution_count":19,"metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a9fd4390be304ce8806a4dbe6b3b3b76","version_major":2,"version_minor":0},"text/plain":["Batches:   0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["question_embeddings = model.encode(trn.prompt_answer_stem.values, batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\n","question_embeddings = question_embeddings.detach().cpu().numpy()"]},{"cell_type":"markdown","metadata":{},"source":["# Extracting Matching Prompt-Sentence Pairs"]},{"cell_type":"code","execution_count":20,"metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ad55ec6e54404deab9a801c72607275b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["contexts = []\n","\n","for r in tqdm(trn.itertuples(), total=len(trn)):\n","\n","    prompt_id = r.Index\n","\n","    prompt_indices = processed_wiki_text_data[processed_wiki_text_data['document_id'].isin(wikipedia_file_data[wikipedia_file_data['prompt_id']==prompt_id]['id'].values)].index.values\n","\n","    if prompt_indices.shape[0] > 0:\n","        prompt_index = faiss.index_factory(wiki_data_embeddings.shape[1], \"Flat\")\n","        prompt_index.add(wiki_data_embeddings[prompt_indices])\n","\n","        context = \"\"\n","        \n","        ## Get the top matches\n","        ss, ii = prompt_index.search(question_embeddings, NUM_SENTENCES_INCLUDE)\n","        for _s, _i in zip(ss[prompt_id], ii[prompt_id]):\n","            context += processed_wiki_text_data.loc[prompt_indices]['text'].iloc[_i] + \" \"\n","        \n","    contexts.append(context)\n","    \n"]},{"cell_type":"code","execution_count":21,"metadata":{"trusted":true},"outputs":[],"source":["trn['context'] = contexts"]},{"cell_type":"code","execution_count":22,"metadata":{"trusted":true},"outputs":[],"source":["if RUN_ON_KAGGLE or 'answer' not in trn.columns:\n","    trn[[\"prompt\", \"context\", \"A\", \"B\", \"C\", \"D\", \"E\"]].to_csv(\"./test_context.csv\", index=False)\n","else:\n","    trn[[\"prompt\", \"context\", \"A\", \"B\", \"C\", \"D\", \"E\", \"answer\"]].to_csv(\"./test_context.csv\", index=False)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Inference"]},{"cell_type":"code","execution_count":23,"metadata":{"trusted":true},"outputs":[],"source":["test_df = pd.read_csv(\"test_context.csv\")\n","test_df.index = list(range(len(test_df)))\n","test_df['id'] = list(range(len(test_df)))\n","# test_df[\"prompt\"] = test_df[\"context\"].apply(lambda x: x[:CONTEXT_LEN]) + \" #### \" +  test_df[\"prompt\"]"]},{"cell_type":"code","execution_count":24,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["DebertaV2ForMultipleChoice(\n","  (deberta): DebertaV2Model(\n","    (embeddings): DebertaV2Embeddings(\n","      (word_embeddings): Embedding(128100, 1024, padding_idx=0)\n","      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n","      (dropout): StableDropout()\n","    )\n","    (encoder): DebertaV2Encoder(\n","      (layer): ModuleList(\n","        (0-23): 24 x DebertaV2Layer(\n","          (attention): DebertaV2Attention(\n","            (self): DisentangledSelfAttention(\n","              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","              (pos_dropout): StableDropout()\n","              (dropout): StableDropout()\n","            )\n","            (output): DebertaV2SelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n","              (dropout): StableDropout()\n","            )\n","          )\n","          (intermediate): DebertaV2Intermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): DebertaV2Output(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n","            (dropout): StableDropout()\n","          )\n","        )\n","      )\n","      (rel_embeddings): Embedding(512, 1024)\n","      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n","    )\n","  )\n","  (pooler): ContextPooler(\n","    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","    (dropout): StableDropout()\n","  )\n","  (classifier): Linear(in_features=1024, out_features=1, bias=True)\n","  (dropout): StableDropout()\n",")"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["if RUN_ON_KAGGLE:\n","    model_dir = \"/kaggle/input/llm-science-run-context-2\"\n","else:\n","    model_dir = \"/home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/llm-science-run-context-2\"\n","    \n","tokenizer = AutoTokenizer.from_pretrained(model_dir)\n","model = AutoModelForMultipleChoice.from_pretrained(model_dir).cuda()\n","model.eval()"]},{"cell_type":"code","execution_count":25,"metadata":{"trusted":true},"outputs":[],"source":["\n","options = 'ABCDE'\n","indices = list(range(5))\n","\n","option_to_index = {option: index for option, index in zip(options, indices)}\n","index_to_option = {index: option for option, index in zip(options, indices)}\n","\n","def preprocess(example):\n","  \n","    \n","    first_sentence = [ \"[CLS] \" + example['context'] ] * 5\n","    second_sentences = [\" #### \" + example['prompt'] + \" [SEP] \" + example[option] + \" [SEP]\" for option in 'ABCDE']\n","    tokenized_example = tokenizer(first_sentence, \n","                                    second_sentences, \n","                                    truncation='only_first', \n","                                    max_length=CONTEXT_LEN, \n","                                    add_special_tokens=False)        \n","    if \"answer\" in example:\n","        tokenized_example[\"label\"] = option_to_index[example[\"answer\"]]\n","    \n","    return tokenized_example\n","            \n","    # first_sentence = [example['prompt']] * 5\n","    # second_sentence = []\n","    # for option in options:\n","    #     second_sentence.append(example[option])\n","    \n","    # tokenized_example = tokenizer(first_sentence, second_sentence, truncation='only_first')\n","    # tokenized_example['label'] = option_to_index[example['answer']]\n","    # return tokenized_example"]},{"cell_type":"code","execution_count":26,"metadata":{"trusted":true},"outputs":[],"source":["@dataclass\n","class DataCollatorForMultipleChoice:\n","    tokenizer: PreTrainedTokenizerBase\n","    padding: Union[bool, str, PaddingStrategy] = True\n","    max_length: Optional[int] = None\n","    pad_to_multiple_of: Optional[int] = None\n","    \n","    def __call__(self, features):\n","        # label_name = \"label\" if 'label' in features[0].keys() else 'labels'\n","        # labels = [feature.pop(label_name) for feature in features]\n","        # batch_size = len(features)\n","        # num_choices = len(features[0]['input_ids'])\n","        # flattened_features = [\n","        #     [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n","        # ]\n","        # flattened_features = sum(flattened_features, [])\n","        \n","        # batch = self.tokenizer.pad(\n","        #     flattened_features,\n","        #     padding=self.padding,\n","        #     max_length=self.max_length,\n","        #     pad_to_multiple_of=self.pad_to_multiple_of,\n","        #     return_tensors='pt',\n","        # )\n","        # batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n","        # batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n","        # return batch\n","        \n","        batch_size = len(features)\n","        num_choices = len(features[0][\"input_ids\"])\n","        flattened_features = [\n","            [\n","                {k: v[i] for k, v in feature.items() if k not in (\"context\", \"__index_level_0__\", \"label\", \"labels\")}\n","                for i in range(num_choices)\n","            ] for feature in features\n","        ]\n","        flattened_features = sum(flattened_features, [])\n","\n","        # _max_length = max([len(x[\"input_ids\"]) for x in flattened_features])\n","        # print(f\"{_max_length = }\")\n","\n","        batch = self.tokenizer.pad(\n","            flattened_features,\n","            padding=self.padding,\n","            max_length=self.max_length,\n","            pad_to_multiple_of=self.pad_to_multiple_of,\n","            return_tensors=\"pt\",\n","        )\n","        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n","        if \"label\" in features[0].keys() or \"labels\" in features[0].keys():\n","            label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n","            labels = [feature.pop(label_name) for feature in features]\n","            batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n","        return batch\n","        \n","        "]},{"cell_type":"code","execution_count":27,"metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8e3a8dfcb5ba4b6baf59e8604869126d","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/5 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["if 'answer' in test_df.columns:\n","    tokenized_test_dataset = Dataset.from_pandas(test_df[['id', 'prompt', 'A', 'B', 'C', 'D', 'E', 'answer', 'context']].drop(columns=['id'])).map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer', 'context'])\n","    tokenized_test_dataset = tokenized_test_dataset.remove_columns([\"__index_level_0__\"])\n","    data_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\n","    test_dataloader = DataLoader(tokenized_test_dataset, batch_size=1, shuffle=False, collate_fn=data_collator)\n","else:\n","    tokenized_test_dataset = Dataset.from_pandas(test_df[['id', 'prompt', 'A', 'B', 'C', 'D', 'E', 'context']].drop(columns=['id'])).map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'context'])\n","    tokenized_test_dataset = tokenized_test_dataset.remove_columns([\"__index_level_0__\"])\n","    data_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\n","    test_dataloader = DataLoader(tokenized_test_dataset, batch_size=1, shuffle=False, collate_fn=data_collator)\n","    "]},{"cell_type":"code","execution_count":28,"metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"087f982ad0bc4817a277a77893bcf0da","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]}],"source":["test_predictions = []\n","\n","for batch in tqdm(test_dataloader):\n","    for k in batch.keys():\n","        batch[k] = batch[k].cuda()\n","    with torch.no_grad():\n","        outputs = model(**batch)\n","    test_predictions.append(outputs.logits.cpu().detach())\n","    \n","test_predictions = torch.cat(test_predictions)"]},{"cell_type":"code","execution_count":29,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["array([[8.9814768e-07, 2.6916354e-07, 9.9999833e-01, 2.2694654e-07,\n","        2.1227021e-07],\n","       [4.1294078e-04, 9.3066752e-01, 1.1487305e-03, 6.3546047e-02,\n","        4.2247931e-03],\n","       [5.9488982e-02, 7.7599323e-01, 7.4040322e-03, 9.2546932e-02,\n","        6.4566784e-02],\n","       [6.2685543e-01, 1.1395359e-02, 2.8701448e-01, 2.4484508e-02,\n","        5.0250202e-02],\n","       [9.1069910e-07, 5.2015477e-01, 1.4847066e-06, 4.1562457e-06,\n","        4.7983864e-01]], dtype=float32)"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["test_predictions = softmax(test_predictions, axis=1)\n","test_predictions[:10]"]},{"cell_type":"code","execution_count":30,"metadata":{"trusted":true},"outputs":[],"source":["ob_preds = test_predictions\n","del test_predictions"]},{"cell_type":"markdown","metadata":{},"source":["# viktor deberta 60k"]},{"cell_type":"code","execution_count":31,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["DebertaV2ForMultipleChoice(\n","  (deberta): DebertaV2Model(\n","    (embeddings): DebertaV2Embeddings(\n","      (word_embeddings): Embedding(128100, 1024, padding_idx=0)\n","      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n","      (dropout): StableDropout()\n","    )\n","    (encoder): DebertaV2Encoder(\n","      (layer): ModuleList(\n","        (0-23): 24 x DebertaV2Layer(\n","          (attention): DebertaV2Attention(\n","            (self): DisentangledSelfAttention(\n","              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","              (pos_dropout): StableDropout()\n","              (dropout): StableDropout()\n","            )\n","            (output): DebertaV2SelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n","              (dropout): StableDropout()\n","            )\n","          )\n","          (intermediate): DebertaV2Intermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): DebertaV2Output(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n","            (dropout): StableDropout()\n","          )\n","        )\n","      )\n","      (rel_embeddings): Embedding(512, 1024)\n","      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n","    )\n","  )\n","  (pooler): ContextPooler(\n","    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","    (dropout): StableDropout()\n","  )\n","  (classifier): Linear(in_features=1024, out_features=1, bias=True)\n","  (dropout): StableDropout()\n",")"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["# model_dir = \"/kaggle/input/how-to-train-open-book-model-part-1/model_v2\"\n","if RUN_ON_KAGGLE:\n","    model_dir = \"/kaggle/input/llm-submissions-viktor/work_dirs/deberta-v3-data-wiki_sci-with-wiki-sentence-context-eval-kaggle-all-folds-grad-accum-128-60k/deberta-v3-large-2023-09-05-07-35-55/checkpoint-3281\"\n","else:\n","    model_dir = \"/home/viktor/Documents/kaggle/kaggle_llm/work_dirs/deberta-v3-data-wiki_sci-with-wiki-sentence-context-eval-kaggle-all-folds-grad-accum-128-60k/deberta-v3-large-2023-09-05-07-35-55/checkpoint-3281\"\n","    \n","tokenizer = AutoTokenizer.from_pretrained(model_dir)\n","model = AutoModelForMultipleChoice.from_pretrained(model_dir).cuda()\n","model.eval()"]},{"cell_type":"code","execution_count":32,"metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"63f4646e24544121acb55ac88e232bc1","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["test_predictions_viktor = []\n","\n","for batch in tqdm(test_dataloader):\n","    for k in batch.keys():\n","        batch[k] = batch[k].cuda()\n","    with torch.no_grad():\n","        outputs = model(**batch)\n","    test_predictions_viktor.append(outputs.logits.cpu().detach())\n","    \n","\n","test_predictions_viktor = torch.cat(test_predictions_viktor)\n"]},{"cell_type":"code","execution_count":33,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["array([[0.00806738, 0.00567649, 0.987608  , 0.00541574, 0.00512276],\n","       [0.0046474 , 0.01359271, 0.00564097, 0.08540385, 0.25813383],\n","       [0.9213304 , 0.9609606 , 0.7537892 , 0.92847383, 0.9015556 ],\n","       [0.85846037, 0.77339894, 0.91821474, 0.76533145, 0.7335206 ],\n","       [0.01201259, 0.96727717, 0.01140674, 0.01270025, 0.9791879 ]],\n","      dtype=float32)"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","    \n","test_predictions_viktor_60k = sigmoid(test_predictions_viktor).numpy()\n","test_predictions_viktor_60k[:10]"]},{"cell_type":"code","execution_count":34,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["71"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["# viktor deberta 160k"]},{"cell_type":"code","execution_count":35,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["DebertaV2ForMultipleChoice(\n","  (deberta): DebertaV2Model(\n","    (embeddings): DebertaV2Embeddings(\n","      (word_embeddings): Embedding(128100, 1024, padding_idx=0)\n","      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n","      (dropout): StableDropout()\n","    )\n","    (encoder): DebertaV2Encoder(\n","      (layer): ModuleList(\n","        (0-23): 24 x DebertaV2Layer(\n","          (attention): DebertaV2Attention(\n","            (self): DisentangledSelfAttention(\n","              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","              (pos_dropout): StableDropout()\n","              (dropout): StableDropout()\n","            )\n","            (output): DebertaV2SelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n","              (dropout): StableDropout()\n","            )\n","          )\n","          (intermediate): DebertaV2Intermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): DebertaV2Output(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n","            (dropout): StableDropout()\n","          )\n","        )\n","      )\n","      (rel_embeddings): Embedding(512, 1024)\n","      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n","    )\n","  )\n","  (pooler): ContextPooler(\n","    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","    (dropout): StableDropout()\n","  )\n","  (classifier): Linear(in_features=1024, out_features=1, bias=True)\n","  (dropout): StableDropout()\n",")"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["# model_dir = \"/kaggle/input/how-to-train-open-book-model-part-1/model_v2\"\n","if RUN_ON_KAGGLE:\n","    model_dir = \"/kaggle/input/llm-submissions-viktor/work_dirs/deberta-v3-data-wiki_sci-with-wiki-sentence-context-eval-kaggle-all-folds-grad-accum-128-60k/deberta-v3-large-2023-09-05-07-35-55/checkpoint-3281\"\n","else:\n","    model_dir = \"/home/viktor/Documents/kaggle/kaggle_llm/work_dirs/160k-viktor-and-deotte-dataset-deotte-preproc-deberta/deberta-v3-large-2023-09-17-10-00-20/checkpoint-14400\"\n","    \n","tokenizer = AutoTokenizer.from_pretrained(model_dir)\n","model = AutoModelForMultipleChoice.from_pretrained(model_dir).cuda()\n","model.eval()"]},{"cell_type":"code","execution_count":36,"metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9cba3fb2a7f04a1ba41613d0e1b728d5","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["test_predictions_viktor_160k = []\n","\n","for batch in tqdm(test_dataloader):\n","    for k in batch.keys():\n","        batch[k] = batch[k].cuda()\n","    with torch.no_grad():\n","        outputs = model(**batch)\n","    test_predictions_viktor_160k.append(outputs.logits.cpu().detach())\n","    \n","\n","test_predictions_viktor_160k = torch.cat(test_predictions_viktor_160k)\n"]},{"cell_type":"code","execution_count":37,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["array([[0.05857772, 0.1019251 , 0.98344284, 0.1918341 , 0.01984959],\n","       [0.00974156, 0.31793797, 0.01701488, 0.32973242, 0.32266703],\n","       [0.9677205 , 0.9862401 , 0.8824824 , 0.9648506 , 0.94473433],\n","       [0.99009913, 0.9200463 , 0.9876875 , 0.9406066 , 0.9802437 ],\n","       [0.48350713, 0.9900304 , 0.42608353, 0.45247197, 0.992195  ]],\n","      dtype=float32)"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","    \n","test_predictions_viktor_160k = sigmoid(test_predictions_viktor_160k).numpy()\n","test_predictions_viktor_160k[:10]"]},{"cell_type":"markdown","metadata":{},"source":["# longformer (kaggle)"]},{"cell_type":"code","execution_count":38,"metadata":{"trusted":true},"outputs":[],"source":["if RUN_ON_KAGGLE:\n","    !cp /kaggle/input/datasets-wheel/datasets-2.14.4-py3-none-any.whl /kaggle/working\n","    !pip install  /kaggle/working/datasets-2.14.4-py3-none-any.whl\n","    !cp /kaggle/input/backup-806/util_openbook.py ."]},{"cell_type":"code","execution_count":39,"metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5ef3926929f14b339a190c82610238a5","version_major":2,"version_minor":0},"text/plain":["Batches:   0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8f059ad296d940569d616909dd416946","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4b945a84f8294df6a699b310257f0710","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/20 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e830c9ad201f4b21ac962d1729512c57","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/100 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cc8badc4ab7c40ee8b321473cfd3156d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/100 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"229cff95155a4938a813835ce6855795","version_major":2,"version_minor":0},"text/plain":["Batches:   0%|          | 0/380 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6119113514e6484393c480c533c42f79","version_major":2,"version_minor":0},"text/plain":["Batches:   0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7ddf1414a0ec4060bfda4fdc58131860","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1835046336594953aa17ed0f49d58211","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/5 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"data":{"text/plain":["20"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["from util_openbook import get_contexts, generate_openbook_output\n","import pickle\n","\n","get_contexts(RUN_ON_KAGGLE, VAL_SIZE, file_for_local_cv)\n","generate_openbook_output(RUN_ON_KAGGLE)\n","\n","import gc\n","gc.collect()"]},{"cell_type":"code","execution_count":40,"metadata":{"trusted":true},"outputs":[],"source":["import pandas as pd\n","backup_model_predictions = pd.read_csv(\"submission_backup.csv\")"]},{"cell_type":"code","execution_count":41,"metadata":{"trusted":true},"outputs":[],"source":["if RUN_ON_KAGGLE:\n","    !cp -r /kaggle/input/stem-wiki-cohere-no-emb /kaggle/working\n","    !cp -r /kaggle/input/all-paraphs-parsed-expanded /kaggle/working/"]},{"cell_type":"code","execution_count":42,"metadata":{"trusted":true},"outputs":[],"source":["from datasets import load_dataset, load_from_disk"]},{"cell_type":"code","execution_count":43,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","import unicodedata\n","\n","\n","def SplitList(mylist, chunk_size):\n","    return [mylist[offs:offs+chunk_size] for offs in range(0, len(mylist), chunk_size)]\n","\n","def get_relevant_documents_parsed(df_valid):\n","    df_chunk_size=600\n","    if RUN_ON_KAGGLE:\n","        paraphs_parsed_dataset = load_from_disk(\"/kaggle/working/all-paraphs-parsed-expanded\")\n","    else:\n","        paraphs_parsed_dataset = load_from_disk(\"/home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/all-paraphs-parsed-expanded\")\n","    \n","    modified_texts = paraphs_parsed_dataset.map(lambda example:\n","                                             {'temp_text':\n","                                              f\"{example['title']} {example['section']} {example['text']}\".replace('\\n',\" \").replace(\"'\",\"\")},\n","                                             num_proc=2)[\"temp_text\"]\n","    \n","    all_articles_indices = []\n","    all_articles_values = []\n","    for idx in tqdm(range(0, df_valid.shape[0], df_chunk_size)):\n","        df_valid_ = df_valid.iloc[idx: idx+df_chunk_size]\n","    \n","        articles_indices, merged_top_scores = retrieval(df_valid_, modified_texts)\n","        all_articles_indices.append(articles_indices)\n","        all_articles_values.append(merged_top_scores)\n","        \n","    article_indices_array =  np.concatenate(all_articles_indices, axis=0)\n","    articles_values_array = np.concatenate(all_articles_values, axis=0).reshape(-1)\n","    \n","    top_per_query = article_indices_array.shape[1]\n","    articles_flatten = [(\n","                         articles_values_array[index],\n","                         paraphs_parsed_dataset[idx.item()][\"title\"],\n","                         paraphs_parsed_dataset[idx.item()][\"text\"],\n","                        )\n","                        for index,idx in enumerate(article_indices_array.reshape(-1))]\n","    retrieved_articles = SplitList(articles_flatten, top_per_query)\n","    return retrieved_articles\n","\n","\n","\n","def get_relevant_documents(df_valid):\n","    df_chunk_size=800\n","    if RUN_ON_KAGGLE:\n","        cohere_dataset_filtered = load_from_disk(\"/kaggle/working/stem-wiki-cohere-no-emb\")\n","    else:\n","        cohere_dataset_filtered = load_from_disk(\"/home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/wiki-stem-cohere\")\n","    modified_texts = cohere_dataset_filtered.map(lambda example:\n","                                             {'temp_text':\n","                                              unicodedata.normalize(\"NFKD\", f\"{example['title']} {example['text']}\").replace('\"',\"\")},\n","                                             num_proc=2)[\"temp_text\"]\n","    \n","    all_articles_indices = []\n","    all_articles_values = []\n","    for idx in tqdm(range(0, df_valid.shape[0], df_chunk_size)):\n","        df_valid_ = df_valid.iloc[idx: idx+df_chunk_size]\n","    \n","        articles_indices, merged_top_scores = retrieval(df_valid_, modified_texts)\n","        all_articles_indices.append(articles_indices)\n","        all_articles_values.append(merged_top_scores)\n","        \n","    article_indices_array =  np.concatenate(all_articles_indices, axis=0)\n","    articles_values_array = np.concatenate(all_articles_values, axis=0).reshape(-1)\n","    \n","    top_per_query = article_indices_array.shape[1]\n","    articles_flatten = [(\n","                         articles_values_array[index],\n","                         cohere_dataset_filtered[idx.item()][\"title\"],\n","                         unicodedata.normalize(\"NFKD\", cohere_dataset_filtered[idx.item()][\"text\"]),\n","                        )\n","                        for index,idx in enumerate(article_indices_array.reshape(-1))]\n","    retrieved_articles = SplitList(articles_flatten, top_per_query)\n","    return retrieved_articles\n","\n","\n","\n","def retrieval(df_valid, modified_texts):\n","    \n","    corpus_df_valid = df_valid.apply(lambda row:\n","                                     f'{row[\"prompt\"]}\\n{row[\"prompt\"]}\\n{row[\"prompt\"]}\\n{row[\"A\"]}\\n{row[\"B\"]}\\n{row[\"C\"]}\\n{row[\"D\"]}\\n{row[\"E\"]}',\n","                                     axis=1).values\n","    vectorizer1 = TfidfVectorizer(ngram_range=(1,2),\n","                                 token_pattern=r\"(?u)\\b[\\w/.-]+\\b|!|/|\\?|\\\"|\\'\",\n","                                 stop_words=stop_words)\n","    vectorizer1.fit(corpus_df_valid)\n","    vocab_df_valid = vectorizer1.get_feature_names_out()\n","    vectorizer = TfidfVectorizer(ngram_range=(1,2),\n","                                 token_pattern=r\"(?u)\\b[\\w/.-]+\\b|!|/|\\?|\\\"|\\'\",\n","                                 stop_words=stop_words,\n","                                 vocabulary=vocab_df_valid)\n","    vectorizer.fit(modified_texts[:500000])\n","    corpus_tf_idf = vectorizer.transform(corpus_df_valid)\n","    \n","    print(f\"length of vectorizer vocab is {len(vectorizer.get_feature_names_out())}\")\n","\n","    chunk_size = 100000\n","    top_per_chunk = 10\n","    top_per_query = 10\n","\n","    all_chunk_top_indices = []\n","    all_chunk_top_values = []\n","\n","    for idx in tqdm(range(0, len(modified_texts), chunk_size)):\n","        wiki_vectors = vectorizer.transform(modified_texts[idx: idx+chunk_size])\n","        temp_scores = (corpus_tf_idf * wiki_vectors.T).toarray()\n","        chunk_top_indices = temp_scores.argpartition(-top_per_chunk, axis=1)[:, -top_per_chunk:]\n","        chunk_top_values = temp_scores[np.arange(temp_scores.shape[0])[:, np.newaxis], chunk_top_indices]\n","\n","        all_chunk_top_indices.append(chunk_top_indices + idx)\n","        all_chunk_top_values.append(chunk_top_values)\n","\n","    top_indices_array = np.concatenate(all_chunk_top_indices, axis=1)\n","    top_values_array = np.concatenate(all_chunk_top_values, axis=1)\n","    \n","    merged_top_scores = np.sort(top_values_array, axis=1)[:,-top_per_query:]\n","    merged_top_indices = top_values_array.argsort(axis=1)[:,-top_per_query:]\n","    articles_indices = top_indices_array[np.arange(top_indices_array.shape[0])[:, np.newaxis], merged_top_indices]\n","    \n","    return articles_indices, merged_top_scores\n","\n","\n","def prepare_answering_input(\n","        tokenizer, \n","        question,  \n","        options,   \n","        context,   \n","        max_seq_length=4096,\n","    ):\n","    c_plus_q   = context + ' ' + tokenizer.bos_token + ' ' + question\n","    c_plus_q_4 = [c_plus_q] * len(options)\n","    tokenized_examples = tokenizer(\n","        c_plus_q_4, options,\n","        max_length=max_seq_length,\n","        padding=\"longest\",\n","        truncation=False,\n","        return_tensors=\"pt\",\n","    )\n","    input_ids = tokenized_examples['input_ids'].unsqueeze(0)\n","    attention_mask = tokenized_examples['attention_mask'].unsqueeze(0)\n","    example_encoded = {\n","        \"input_ids\": input_ids.to(model.device.index),\n","        \"attention_mask\": attention_mask.to(model.device.index),\n","    }\n","    return example_encoded"]},{"cell_type":"code","execution_count":44,"metadata":{"trusted":true},"outputs":[],"source":["stop_words = ['each', 'you', 'the', 'use', 'used',\n","                  'where', 'themselves', 'nor', \"it's\", 'how', \"don't\", 'just', 'your',\n","                  'about', 'himself', 'with', \"weren't\", 'hers', \"wouldn't\", 'more', 'its', 'were',\n","                  'his', 'their', 'then', 'been', 'myself', 're', 'not',\n","                  'ours', 'will', 'needn', 'which', 'here', 'hadn', 'it', 'our', 'there', 'than',\n","                  'most', \"couldn't\", 'both', 'some', 'for', 'up', 'couldn', \"that'll\",\n","                  \"she's\", 'over', 'this', 'now', 'until', 'these', 'few', 'haven',\n","                  'of', 'wouldn', 'into', 'too', 'to', 'very', 'shan', 'before', 'the', 'they',\n","                  'between', \"doesn't\", 'are', 'was', 'out', 'we', 'me',\n","                  'after', 'has', \"isn't\", 'have', 'such', 'should', 'yourselves', 'or', 'during', 'herself',\n","                  'doing', 'in', \"shouldn't\", \"won't\", 'when', 'do', 'through', 'she',\n","                  'having', 'him', \"haven't\", 'against', 'itself', 'that',\n","                  'did', 'theirs', 'can', 'those',\n","                  'own', 'so', 'and', 'who', \"you've\", 'yourself', 'her', 'he', 'only',\n","                  'what', 'ourselves', 'again', 'had', \"you'd\", 'is', 'other',\n","                  'why', 'while', 'from', 'them', 'if', 'above', 'does', 'whom',\n","                  'yours', 'but', 'being', \"wasn't\", 'be']"]},{"cell_type":"code","execution_count":45,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Loading cached processed dataset at /home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/all-paraphs-parsed-expanded/cache-6598ee86506899ad_*_of_00002.arrow\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cefdc6b06c474fe780ba672c65518836","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/viktor/miniconda3/envs/torch-env/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'd', 'doesn', 'don', 'isn', 'll', 's', 'shouldn', 't', 've', 'wasn', 'weren', 'won'] not in stop_words.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["length of vectorizer vocab is 403\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"70c6873aa4474e95a8728d4203c7fb4d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/22 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["38"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["if RUN_ON_KAGGLE:\n","    df_valid = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\")\n","else:\n","    df_valid = pd.read_csv(file_for_local_cv).sample(n=VAL_SIZE, random_state=42).reset_index(drop=True)\n","retrieved_articles_parsed = get_relevant_documents_parsed(df_valid)\n","gc.collect()"]},{"cell_type":"code","execution_count":46,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>prompt</th>\n","      <th>A</th>\n","      <th>B</th>\n","      <th>C</th>\n","      <th>D</th>\n","      <th>E</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>95</td>\n","      <td>What is the main sequence in astronomy?</td>\n","      <td>The main sequence is a type of galaxy that con...</td>\n","      <td>The main sequence is a type of black hole that...</td>\n","      <td>The main sequence is a continuous and distinct...</td>\n","      <td>The main sequence is a group of planets that o...</td>\n","      <td>The main sequence is a type of nebula that is ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>15</td>\n","      <td>What is the \"ultraviolet catastrophe\"?</td>\n","      <td>It is a phenomenon that occurs only in multi-m...</td>\n","      <td>It is the misbehavior of a formula for higher ...</td>\n","      <td>It is the standing wave of a string in harmoni...</td>\n","      <td>It is a flaw in classical physics that results...</td>\n","      <td>It is a disproven theory about the distributio...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>30</td>\n","      <td>What is the Carnot engine?</td>\n","      <td>The Carnot engine is a theoretical engine that...</td>\n","      <td>The Carnot engine is an ideal heat engine that...</td>\n","      <td>The Carnot engine is a real heat engine that o...</td>\n","      <td>The Carnot engine is a theoretical engine that...</td>\n","      <td>The Carnot engine is a real engine that operat...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>158</td>\n","      <td>What is accelerator-based light-ion fusion?</td>\n","      <td>Accelerator-based light-ion fusion is a techni...</td>\n","      <td>Accelerator-based light-ion fusion is a techni...</td>\n","      <td>Accelerator-based light-ion fusion is a techni...</td>\n","      <td>Accelerator-based light-ion fusion is a techni...</td>\n","      <td>Accelerator-based light-ion fusion is a techni...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>128</td>\n","      <td>What is a \"coffee ring\" in physics?</td>\n","      <td>A type of coffee that is made by boiling coffe...</td>\n","      <td>A pattern left by a particle-laden liquid afte...</td>\n","      <td>A type of coffee that is made by mixing instan...</td>\n","      <td>A type of coffee that is made by pouring hot w...</td>\n","      <td>A pattern left by a particle-laden liquid afte...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    id                                       prompt   \n","0   95      What is the main sequence in astronomy?  \\\n","1   15       What is the \"ultraviolet catastrophe\"?   \n","2   30                   What is the Carnot engine?   \n","3  158  What is accelerator-based light-ion fusion?   \n","4  128          What is a \"coffee ring\" in physics?   \n","\n","                                                   A   \n","0  The main sequence is a type of galaxy that con...  \\\n","1  It is a phenomenon that occurs only in multi-m...   \n","2  The Carnot engine is a theoretical engine that...   \n","3  Accelerator-based light-ion fusion is a techni...   \n","4  A type of coffee that is made by boiling coffe...   \n","\n","                                                   B   \n","0  The main sequence is a type of black hole that...  \\\n","1  It is the misbehavior of a formula for higher ...   \n","2  The Carnot engine is an ideal heat engine that...   \n","3  Accelerator-based light-ion fusion is a techni...   \n","4  A pattern left by a particle-laden liquid afte...   \n","\n","                                                   C   \n","0  The main sequence is a continuous and distinct...  \\\n","1  It is the standing wave of a string in harmoni...   \n","2  The Carnot engine is a real heat engine that o...   \n","3  Accelerator-based light-ion fusion is a techni...   \n","4  A type of coffee that is made by mixing instan...   \n","\n","                                                   D   \n","0  The main sequence is a group of planets that o...  \\\n","1  It is a flaw in classical physics that results...   \n","2  The Carnot engine is a theoretical engine that...   \n","3  Accelerator-based light-ion fusion is a techni...   \n","4  A type of coffee that is made by pouring hot w...   \n","\n","                                                   E  \n","0  The main sequence is a type of nebula that is ...  \n","1  It is a disproven theory about the distributio...  \n","2  The Carnot engine is a real engine that operat...  \n","3  Accelerator-based light-ion fusion is a techni...  \n","4  A pattern left by a particle-laden liquid afte...  "]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["df_valid"]},{"cell_type":"code","execution_count":47,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Loading cached processed dataset at /home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/wiki-stem-cohere/cache-8cfa8c0564ee26f0_*_of_00002.arrow\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0a3227c975864cad8b1040a6be6623be","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/viktor/miniconda3/envs/torch-env/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'd', 'doesn', 'don', 'isn', 'll', 's', 'shouldn', 't', 've', 'wasn', 'weren', 'won'] not in stop_words.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["length of vectorizer vocab is 403\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"59b36c818b1b4a1587094534c48cbf67","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/28 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["38"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["retrieved_articles = get_relevant_documents(df_valid)\n","gc.collect()"]},{"cell_type":"code","execution_count":48,"metadata":{"trusted":true},"outputs":[],"source":["from transformers import LongformerTokenizer, LongformerForMultipleChoice\n","\n","if RUN_ON_KAGGLE:\n","    tokenizer = LongformerTokenizer.from_pretrained(\"/kaggle/input/longformer-race-model/longformer_qa_model\")\n","    model = LongformerForMultipleChoice.from_pretrained(\"/kaggle/input/longformer-race-model/longformer_qa_model\").cuda()\n","else:\n","    tokenizer = LongformerTokenizer.from_pretrained(\"/home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/longformer-race-model/longformer_qa_model\")\n","    model = LongformerForMultipleChoice.from_pretrained(\"/home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/longformer-race-model/longformer_qa_model\").cuda()\n","    "]},{"cell_type":"code","execution_count":49,"metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a6bce37f296b466dafcf9bbb581776df","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["predictions = []\n","\n","test_predictions_longformer_1 = []\n","test_predictions_longformer_2 = []\n","\n","for index in tqdm(range(df_valid.shape[0])):\n","    row = df_valid.iloc[index]\n","    # question is 'prompt'\n","    question = row['prompt']\n","    options = [row['A'], row['B'], row['C'], row['D'], row['E']]\n","    context1 = f\"{retrieved_articles[index][-4][2]}\\n{retrieved_articles[index][-3][2]}\\n{retrieved_articles[index][-2][2]}\\n{retrieved_articles[index][-1][2]}\"\n","    context2 = f\"{retrieved_articles_parsed[index][-3][2]}\\n{retrieved_articles_parsed[index][-2][2]}\\n{retrieved_articles_parsed[index][-1][2]}\"\n","    inputs1 = prepare_answering_input(\n","        tokenizer=tokenizer, question=question,\n","        options=options, context=context1,\n","        )\n","    inputs2 = prepare_answering_input(\n","        tokenizer=tokenizer, question=question,\n","        options=options, context=context2,\n","        )\n","    \n","    with torch.no_grad():\n","        outputs1 = model(**inputs1)    \n","        losses1 = -outputs1.logits[0].detach().cpu().numpy()\n","        probability1 = torch.softmax(torch.tensor(-losses1), dim=-1)\n","        \n","    with torch.no_grad():\n","        outputs2 = model(**inputs2)\n","        losses2 = -outputs2.logits[0].detach().cpu().numpy()\n","        probability2 = torch.softmax(torch.tensor(-losses2), dim=-1)\n","        \n","    \n","    test_predictions_longformer_1.append(probability1.numpy().astype(np.float16))\n","    test_predictions_longformer_2.append(probability2.numpy().astype(np.float16))\n","    \n"]},{"cell_type":"code","execution_count":50,"metadata":{"trusted":true},"outputs":[],"source":["test_predictions_longformer_1 = np.stack(test_predictions_longformer_1)\n","test_predictions_longformer_2 = np.stack(test_predictions_longformer_2)"]},{"cell_type":"code","execution_count":51,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["((5, 5),\n"," array([[3.772e-04, 7.933e-05, 9.995e-01, 1.222e-05, 2.249e-04],\n","        [3.593e-04, 8.735e-01, 1.248e-03, 1.080e-01, 1.711e-02],\n","        [1.836e-02, 8.931e-01, 6.824e-03, 2.588e-02, 5.579e-02],\n","        [9.199e-01, 1.831e-02, 3.096e-02, 1.475e-02, 1.627e-02],\n","        [4.649e-06, 3.633e-01, 1.025e-05, 5.841e-06, 6.367e-01]],\n","       dtype=float16))"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["test_predictions_longformer_1.shape, test_predictions_longformer_1[:10]"]},{"cell_type":"markdown","metadata":{},"source":["# checkpoint-5975-09025"]},{"cell_type":"code","execution_count":52,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["DebertaV2ForMultipleChoice(\n","  (deberta): DebertaV2Model(\n","    (embeddings): DebertaV2Embeddings(\n","      (word_embeddings): Embedding(128100, 1024, padding_idx=0)\n","      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n","      (dropout): StableDropout()\n","    )\n","    (encoder): DebertaV2Encoder(\n","      (layer): ModuleList(\n","        (0-23): 24 x DebertaV2Layer(\n","          (attention): DebertaV2Attention(\n","            (self): DisentangledSelfAttention(\n","              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","              (pos_dropout): StableDropout()\n","              (dropout): StableDropout()\n","            )\n","            (output): DebertaV2SelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n","              (dropout): StableDropout()\n","            )\n","          )\n","          (intermediate): DebertaV2Intermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): DebertaV2Output(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n","            (dropout): StableDropout()\n","          )\n","        )\n","      )\n","      (rel_embeddings): Embedding(512, 1024)\n","      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n","    )\n","  )\n","  (pooler): ContextPooler(\n","    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","    (dropout): StableDropout()\n","  )\n","  (classifier): Linear(in_features=1024, out_features=1, bias=True)\n","  (dropout): StableDropout()\n",")"]},"execution_count":52,"metadata":{},"output_type":"execute_result"}],"source":["# model_dir = \"/kaggle/input/how-to-train-open-book-model-part-1/model_v2\"\n","if RUN_ON_KAGGLE:\n","    model_dir = \"/kaggle/input/checkpoint-5975-09025\"\n","else:\n","    model_dir = \"/home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/checkpoint-5975-09025\"\n","tokenizer = AutoTokenizer.from_pretrained(model_dir)\n","model = AutoModelForMultipleChoice.from_pretrained(model_dir).cuda()\n","model.eval()"]},{"cell_type":"code","execution_count":53,"metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5c701629a5194810bdf56d754a46aea9","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["test_predictionsc = []\n","\n","for batch in tqdm(test_dataloader):\n","    for k in batch.keys():\n","        batch[k] = batch[k].cuda()\n","    with torch.no_grad():\n","        outputs = model(**batch)\n","    test_predictionsc.append(outputs.logits.cpu().detach())\n","\n","\n","test_predictionsc = torch.cat(test_predictionsc)"]},{"cell_type":"code","execution_count":54,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["array([[0.00704007, 0.00550796, 0.98039484, 0.00399233, 0.00306486],\n","       [0.06614213, 0.22226083, 0.10455986, 0.37266037, 0.23437679],\n","       [0.17931306, 0.38645336, 0.0974683 , 0.18907335, 0.14769192],\n","       [0.34639886, 0.07131083, 0.36189464, 0.09030268, 0.13009304],\n","       [0.01327508, 0.4556936 , 0.01422865, 0.02171131, 0.49509135]],\n","      dtype=float32)"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["test_predictionsc = softmax(test_predictionsc, axis=1)\n","test_predictionsc[:10]"]},{"cell_type":"code","execution_count":55,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["75"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["gc.collect()"]},{"cell_type":"code","execution_count":56,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["DebertaV2ForMultipleChoice(\n","  (deberta): DebertaV2Model(\n","    (embeddings): DebertaV2Embeddings(\n","      (word_embeddings): Embedding(128100, 1024, padding_idx=0)\n","      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n","      (dropout): StableDropout()\n","    )\n","    (encoder): DebertaV2Encoder(\n","      (layer): ModuleList(\n","        (0-23): 24 x DebertaV2Layer(\n","          (attention): DebertaV2Attention(\n","            (self): DisentangledSelfAttention(\n","              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","              (pos_dropout): StableDropout()\n","              (dropout): StableDropout()\n","            )\n","            (output): DebertaV2SelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n","              (dropout): StableDropout()\n","            )\n","          )\n","          (intermediate): DebertaV2Intermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): DebertaV2Output(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n","            (dropout): StableDropout()\n","          )\n","        )\n","      )\n","      (rel_embeddings): Embedding(512, 1024)\n","      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n","    )\n","  )\n","  (pooler): ContextPooler(\n","    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","    (dropout): StableDropout()\n","  )\n","  (classifier): Linear(in_features=1024, out_features=1, bias=True)\n","  (dropout): StableDropout()\n",")"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["if RUN_ON_KAGGLE:\n","    model_dir = \"/kaggle/input/using-deepspeed-with-hf-trainer/checkpoints_1\"\n","else:\n","    model_dir = \"/home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/using-deepspeed-with-hf-trainer/checkpoints_1\"\n","tokenizer = AutoTokenizer.from_pretrained(model_dir)\n","model = AutoModelForMultipleChoice.from_pretrained(model_dir).cuda()\n","model.eval()"]},{"cell_type":"code","execution_count":57,"metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ff956e8f434e4f5cb1ae05629150a819","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["test_predictionsi = []\n","\n","for batch in tqdm(test_dataloader):\n","    for k in batch.keys():\n","        batch[k] = batch[k].cuda()\n","    with torch.no_grad():\n","        outputs = model(**batch)\n","    test_predictionsi.append(outputs.logits.cpu().detach())\n","    \n","test_predictionsi = torch.cat(test_predictionsi)"]},{"cell_type":"code","execution_count":58,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["array([[2.0666278e-04, 6.0566967e-05, 9.9910635e-01, 5.8210699e-04,\n","        4.4261560e-05],\n","       [3.8008273e-02, 1.6111714e-01, 1.1277935e-02, 6.5992033e-01,\n","        1.2967631e-01],\n","       [9.8168515e-02, 4.7188583e-01, 5.4660004e-02, 2.6706249e-01,\n","        1.0822317e-01],\n","       [1.9976807e-01, 1.1816469e-01, 1.2887728e-01, 1.6459544e-01,\n","        3.8859457e-01],\n","       [3.8349428e-04, 4.8323649e-01, 3.0860497e-04, 1.3359212e-03,\n","        5.1473546e-01]], dtype=float32)"]},"execution_count":58,"metadata":{},"output_type":"execute_result"}],"source":["test_predictionsi = softmax(test_predictionsi, axis=1)\n","test_predictionsi[:10]"]},{"cell_type":"code","execution_count":59,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["51"]},"execution_count":59,"metadata":{},"output_type":"execute_result"}],"source":["gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["# combine predictions (and find best combination on local CV )"]},{"cell_type":"code","execution_count":63,"metadata":{"trusted":true},"outputs":[],"source":["preds_dict = {\n","    'chris': test_predictionsc,\n","    'openbook': ob_preds, \n","    'itk_ob': test_predictionsi,   \n","    'viktor_60k': test_predictions_viktor_60k,\n","    'test_predictions_viktor_160k': test_predictions_viktor_160k,\n","    'test_predictions_longformer_1': test_predictions_longformer_1,\n","    'test_predictions_longformer_2': test_predictions_longformer_2\n","}\n","# save preds_dict as pickle\n","with open('preds_dict.pickle', 'wb') as handle:\n","    pickle.dump(preds_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","\n","weights = np.ones(len(preds_dict))\n","\n","predictions_overall = np.sum([preds_val*weight for preds_val, weight in zip(preds_dict.values(), weights)], axis=0)\n","predictions_overall = np.argsort(-predictions_overall)[:,:3]\n","predictions_as_answer_letters = np.array(list('ABCDE'))[predictions_overall]\n","test_df['prediction'] = [' '.join(pred) for pred in predictions_as_answer_letters]\n","\n","def compute_map3(preds, actuals):\n","    correct_answers = [0, 0, 0]\n","    for pred, actual in zip(preds, actuals):\n","        if pred[0] == actual:\n","            correct_answers[0] += 1\n","        if pred[1] == actual:\n","            correct_answers[1] += 1./2\n","        if pred[2] == actual:\n","            correct_answers[2] += 1./3\n","    \n","    n_total = len(actuals)\n","    map3 = np.sum(correct_answers) / n_total\n","    \n","    return map3\n","\n","if not RUN_ON_KAGGLE and 'answer' in test_df.columns:\n","    val_df = pd.read_csv(file_for_local_cv,index_col=0).sample(n=VAL_SIZE, random_state=42).reset_index(drop=True)\n","    \n","    val_df['A'] = val_df['A'].map(str)\n","    val_df['B'] = val_df['B'].map(str)\n","    val_df['C'] = val_df['C'].map(str)\n","    val_df['D'] = val_df['D'].map(str)\n","    val_df['E'] = val_df['E'].map(str)\n","    val_df['answer'] = val_df['answer'].map(str)\n","\n","    val_df = val_df.reset_index(drop=True)\n","    \n","    # best_map3_score = 0\n","    # for i in tqdm(range(10000)):\n","    #     weights = np.random.uniform(0, 1, len(preds_dict))\n","    #     predictions_overall = np.sum([preds_val*weight for preds_val in zip(preds_dict.values(), weights)], axis=0)\n","    #     predictions_overall = np.argsort(-predictions_overall)[:,:3]\n","    #     predictions_as_answer_letters = np.array(list('ABCDE'))[predictions_overall]\n","\n","\n","    \n","    #     map3_score = compute_map3(predictions_as_answer_letters, val_df['answer'].values)\n","    \n","    #     if map3_score > best_map3_score:\n","    #         best_map3_score = map3_score\n","    #         print(weights)\n","    #         print(map3_score)\n","    \n","    # print(map3_score)\n","    \n","    "]},{"cell_type":"markdown","metadata":{},"source":["Experiments:\n","\n","- all models: 0.8"]},{"cell_type":"code","execution_count":64,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>prediction</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>C D B</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>B D E</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>B E D</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>A C E</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>E B A</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id prediction\n","0   0      C D B\n","1   1      B D E\n","2   2      B E D\n","3   3      A C E\n","4   4      E B A"]},"execution_count":64,"metadata":{},"output_type":"execute_result"}],"source":["submission = test_df[['id', 'prediction']]\n","submission.to_csv('submission.csv', index=False)\n","\n","pd.read_csv('submission.csv').head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":4}
