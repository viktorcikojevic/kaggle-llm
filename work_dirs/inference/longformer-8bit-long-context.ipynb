{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-09-25T21:32:16.823181Z","iopub.status.busy":"2023-09-25T21:32:16.822903Z","iopub.status.idle":"2023-09-25T21:32:16.836270Z","shell.execute_reply":"2023-09-25T21:32:16.835164Z","shell.execute_reply.started":"2023-09-25T21:32:16.823155Z"},"trusted":true},"outputs":[],"source":["RUN_ON_KAGGLE = False\n","DEBUG = False"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-09-25T21:32:16.838813Z","iopub.status.busy":"2023-09-25T21:32:16.838485Z"},"trusted":true},"outputs":[],"source":["%%capture\n","if RUN_ON_KAGGLE:\n","    # installing offline dependencies\n","    !pip install -U /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n","    !cp -rf /kaggle/input/sentence-transformers-222/sentence-transformers /kaggle/working/sentence-transformers\n","    !pip install -U /kaggle/working/sentence-transformers\n","    !pip install -U /kaggle/input/blingfire-018/blingfire-0.1.8-py3-none-any.whl\n","\n","    !pip install --no-index --no-deps /kaggle/input/llm-whls/transformers-4.31.0-py3-none-any.whl\n","    !pip install --no-index --no-deps /kaggle/input/llm-whls/peft-0.4.0-py3-none-any.whl\n","    !pip install --no-index --no-deps /kaggle/input/llm-whls/datasets-2.14.3-py3-none-any.whl\n","    !pip install --no-index --no-deps /kaggle/input/llm-whls/trl-0.5.0-py3-none-any.whl\n","    \n","    !pip install \"/kaggle/input/blingfire-018/blingfire-0.1.8-py3-none-any.whl\" --no-index\n","    !pip install \"/kaggle/input/peft-native/peft-0.4.0-py3-none-any.whl\" --no-index --find-links=\"/kaggle/input/peft-native/\"\n","    !pip install \"/kaggle/input/bitsandbytes-0410/bitsandbytes-0.41.0-py3-none-any.whl\" --no-index\n","    !pip install \"/kaggle/input/accelerate-native/accelerate-0.21.0-py3-none-any.whl\" --no-index --find-links=\"/kaggle/input/accelerate-native/\"\n","    !pip install \"/kaggle/input/transformers-native/transformers-4.31.0-py3-none-any.whl\" --no-index --find-links=\"/kaggle/input/transformers-native/\""]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","===================================BUG REPORT===================================\n","Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n","================================================================================\n","CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n","CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n","CUDA SETUP: Highest compute capability among GPUs detected: 8.9\n","CUDA SETUP: Detected CUDA version 121\n","CUDA SETUP: Loading binary /home/viktor/miniconda3/envs/torch-env/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda121.so...\n"]},{"name":"stderr","output_type":"stream","text":["/home/viktor/miniconda3/envs/torch-env/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /home/viktor/miniconda3/envs/torch-env did not contain libcudart.so as expected! Searching further paths...\n","  warn(msg)\n","/home/viktor/miniconda3/envs/torch-env/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n","  warn(msg)\n","/home/viktor/miniconda3/envs/torch-env/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n","  warn(msg)\n"]}],"source":["import os, time\n","import gc\n","import pandas as pd\n","import numpy as np\n","import re\n","from tqdm.auto import tqdm\n","import blingfire as bf\n","# from __future__ import annotations\n","\n","from collections.abc import Iterable\n","\n","import faiss\n","from faiss import write_index, read_index\n","\n","from sentence_transformers import SentenceTransformer\n","\n","import torch\n","import ctypes\n","libc = ctypes.CDLL(\"libc.so.6\")\n","\n","from dataclasses import dataclass\n","from typing import Optional, Union\n","\n","import torch\n","import numpy as np\n","import pandas as pd\n","from datasets import Dataset\n","from transformers import AutoTokenizer\n","from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n","from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n","from torch.utils.data import DataLoader\n","\n","from scipy.special import softmax"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[],"source":["if RUN_ON_KAGGLE:\n","    SIM_MODEL = '/kaggle/input/sentencetransformers-allminilml6v2/sentence-transformers_all-MiniLM-L6-v2'\n","else:\n","    SIM_MODEL = 'sentence-transformers/all-MiniLM-L6-v2'\n","DEVICE = 0\n","MAX_LENGTH = 384\n","BATCH_SIZE = 1\n","\n","\n","FILTER_LEN = 1 if DEBUG else 9\n","IND_SEARCH = 1 if DEBUG else 7\n","NUM_SENTENCES_INCLUDE = 1 if DEBUG else 25\n","CONTEXT_LEN = 1000 if DEBUG else 2305\n","CONTEXT_LEN_8BIT = 1000 if DEBUG else 4096\n","VAL_SIZE = 20 if DEBUG else 2000\n","\n","if RUN_ON_KAGGLE:\n","    WIKI_PATH = \"/kaggle/input/wikipedia-20230701\"\n","else:\n","    WIKI_PATH = \"/home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/wikipedia-2023-07-faiss-index\"\n","    \n","# file_for_local_cv = \"/home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-llm-science-exam/test.csv\"    \n","file_for_local_cv = \"/home/viktor/Documents/kaggle/kaggle_llm/data/data_dumps/more_questions/more_questions_raw_questions_wiki_sci_3_shuffled.csv\"    \n","\n","wiki_files = os.listdir(WIKI_PATH)"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[],"source":["def process_documents(documents: Iterable[str],\n","                      document_ids: Iterable,\n","                      split_sentences: bool = True,\n","                      filter_len: int = FILTER_LEN,\n","                      disable_progress_bar: bool = False) -> pd.DataFrame:\n","    \n","    df = sectionize_documents(documents, document_ids, disable_progress_bar)\n","\n","    if split_sentences:\n","        df = sentencize(df.text.values, \n","                        df.document_id.values,\n","                        df.offset.values, \n","                        filter_len, \n","                        disable_progress_bar)\n","    return df\n","\n","\n","def sectionize_documents(documents: Iterable[str],\n","                         document_ids: Iterable,\n","                         disable_progress_bar: bool = False) -> pd.DataFrame:\n","\n","    processed_documents = []\n","    for document_id, document in tqdm(zip(document_ids, documents), total=len(documents), disable=disable_progress_bar):\n","        row = {}\n","        text, start, end = (document, 0, len(document))\n","        row['document_id'] = document_id\n","        row['text'] = text\n","        row['offset'] = (start, end)\n","\n","        processed_documents.append(row)\n","\n","    _df = pd.DataFrame(processed_documents)\n","    if _df.shape[0] > 0:\n","        return _df.sort_values(['document_id', 'offset']).reset_index(drop=True)\n","    else:\n","        return _df\n","\n","\n","def sentencize(documents: Iterable[str],\n","               document_ids: Iterable,\n","               offsets: Iterable[tuple[int, int]],\n","               filter_len: int = FILTER_LEN,\n","               disable_progress_bar: bool = False) -> pd.DataFrame:\n","\n","    document_sentences = []\n","    for document, document_id, offset in tqdm(zip(documents, document_ids, offsets), total=len(documents), disable=disable_progress_bar):\n","        try:\n","            _, sentence_offsets = bf.text_to_sentences_and_offsets(document)\n","            for o in sentence_offsets:\n","                if o[1]-o[0] > filter_len:\n","                    sentence = document[o[0]:o[1]]\n","                    abs_offsets = (o[0]+offset[0], o[1]+offset[0])\n","                    row = {}\n","                    row['document_id'] = document_id\n","                    row['text'] = sentence\n","                    row['offset'] = abs_offsets\n","                    document_sentences.append(row)\n","        except:\n","            continue\n","    return pd.DataFrame(document_sentences)"]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>prompt</th>\n","      <th>A</th>\n","      <th>B</th>\n","      <th>C</th>\n","      <th>D</th>\n","      <th>E</th>\n","      <th>answer</th>\n","      <th>cluster number</th>\n","      <th>round</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>13884</td>\n","      <td>What is the relationship between Adiantum and ...</td>\n","      <td>Adiantum is a variant of AES</td>\n","      <td>Adiantum is a cryptographic message authentica...</td>\n","      <td>Adiantum uses AES for disk encryption</td>\n","      <td>Adiantum is a low-powered mobile device runnin...</td>\n","      <td>Adiantum uses a different construction for the...</td>\n","      <td>C</td>\n","      <td>590</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>20379</td>\n","      <td>What is the cause of the painful bump in Osgoo...</td>\n","      <td>Excessive bone growth at the tibial tuberosity</td>\n","      <td>Fluid accumulation in the patellar ligament</td>\n","      <td>Inflammation of the quadriceps muscle</td>\n","      <td>Degeneration of the knee cartilage</td>\n","      <td>Dislocation of the tibiofemoral joint</td>\n","      <td>A</td>\n","      <td>827</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>24313</td>\n","      <td>What are the two most common types of tarsal c...</td>\n","      <td>Calcaneo-cuboid and calcaneo-navicular</td>\n","      <td>Talo-cuneiform and talo-tarsal</td>\n","      <td>Navicular-cuboid and calcaneo-tarsal</td>\n","      <td>Talo-calcaneal and talo-navicular</td>\n","      <td>Calcaneo-navicular and talo-calcaneal</td>\n","      <td>E</td>\n","      <td>988</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>22258</td>\n","      <td>What is the significance of dietary restrictio...</td>\n","      <td>Dietary restrictions can worsen the symptoms o...</td>\n","      <td>Dietary restrictions can completely cure ocula...</td>\n","      <td>Dietary restrictions have no impact on the sym...</td>\n","      <td>Dietary restrictions may reduce or eliminate t...</td>\n","      <td>Dietary restrictions only impact the symptoms ...</td>\n","      <td>D</td>\n","      <td>902</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>16046</td>\n","      <td>What is the purpose of isolating Parvibaculum ...</td>\n","      <td>To remove it from wastewater treatment plants ...</td>\n","      <td>To improve the aesthetics of activated sludge</td>\n","      <td>To study its metabolic abilities and potential...</td>\n","      <td>To increase the production of linear alkylbenz...</td>\n","      <td>To enhance the growth of other bacteria in the...</td>\n","      <td>C</td>\n","      <td>668</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0                                             prompt   \n","0       13884  What is the relationship between Adiantum and ...  \\\n","1       20379  What is the cause of the painful bump in Osgoo...   \n","2       24313  What are the two most common types of tarsal c...   \n","3       22258  What is the significance of dietary restrictio...   \n","4       16046  What is the purpose of isolating Parvibaculum ...   \n","\n","                                                   A   \n","0                       Adiantum is a variant of AES  \\\n","1     Excessive bone growth at the tibial tuberosity   \n","2             Calcaneo-cuboid and calcaneo-navicular   \n","3  Dietary restrictions can worsen the symptoms o...   \n","4  To remove it from wastewater treatment plants ...   \n","\n","                                                   B   \n","0  Adiantum is a cryptographic message authentica...  \\\n","1        Fluid accumulation in the patellar ligament   \n","2                     Talo-cuneiform and talo-tarsal   \n","3  Dietary restrictions can completely cure ocula...   \n","4      To improve the aesthetics of activated sludge   \n","\n","                                                   C   \n","0              Adiantum uses AES for disk encryption  \\\n","1              Inflammation of the quadriceps muscle   \n","2               Navicular-cuboid and calcaneo-tarsal   \n","3  Dietary restrictions have no impact on the sym...   \n","4  To study its metabolic abilities and potential...   \n","\n","                                                   D   \n","0  Adiantum is a low-powered mobile device runnin...  \\\n","1                 Degeneration of the knee cartilage   \n","2                  Talo-calcaneal and talo-navicular   \n","3  Dietary restrictions may reduce or eliminate t...   \n","4  To increase the production of linear alkylbenz...   \n","\n","                                                   E answer  cluster number   \n","0  Adiantum uses a different construction for the...      C             590  \\\n","1              Dislocation of the tibiofemoral joint      A             827   \n","2              Calcaneo-navicular and talo-calcaneal      E             988   \n","3  Dietary restrictions only impact the symptoms ...      D             902   \n","4  To enhance the growth of other bacteria in the...      C             668   \n","\n","   round  \n","0      0  \n","1      1  \n","2      0  \n","3      0  \n","4      0  "]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["if RUN_ON_KAGGLE:\n","    trn = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\").drop(\"id\", axis=1)\n","else:\n","    trn = pd.read_csv(file_for_local_cv).drop(\"id\", axis=1).sample(n=VAL_SIZE, random_state=42).reset_index(drop=True)\n","trn.head()"]},{"cell_type":"markdown","metadata":{},"source":["# Inference"]},{"cell_type":"code","execution_count":7,"metadata":{"trusted":true},"outputs":[],"source":["test_df = pd.read_csv(\"test_context.csv\")\n","test_df.index = list(range(len(test_df)))\n","test_df['id'] = list(range(len(test_df)))\n","# test_df[\"prompt\"] = test_df[\"context\"].apply(lambda x: x[:CONTEXT_LEN]) + \" #### \" +  test_df[\"prompt\"]"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[],"source":["\n","options = 'ABCDE'\n","indices = list(range(5))\n","\n","option_to_index = {option: index for option, index in zip(options, indices)}\n","index_to_option = {index: option for option, index in zip(options, indices)}\n","\n","def preprocess(example):\n","  \n","    \n","    first_sentence = [ \"[CLS] \" + example['context'] ] * 5\n","    second_sentences = [\" #### \" + example['prompt'] + \" [SEP] \" + example[option] + \" [SEP]\" for option in 'ABCDE']\n","    tokenized_example = tokenizer(first_sentence, \n","                                    second_sentences, \n","                                    truncation='only_first', \n","                                    max_length=CONTEXT_LEN, \n","                                    add_special_tokens=False)        \n","    if \"answer\" in example:\n","        tokenized_example[\"label\"] = option_to_index[example[\"answer\"]]\n","    \n","    return tokenized_example\n","            \n","    # first_sentence = [example['prompt']] * 5\n","    # second_sentence = []\n","    # for option in options:\n","    #     second_sentence.append(example[option])\n","    \n","    # tokenized_example = tokenizer(first_sentence, second_sentence, truncation='only_first')\n","    # tokenized_example['label'] = option_to_index[example['answer']]\n","    # return tokenized_example\n","    \n","def preprocess_for_8bit(example):\n","  \n","    \n","    first_sentence = [ \"[CLS] \" + example['context'] ] * 5\n","    second_sentences = [\" #### \" + example['prompt'] + \" [SEP] \" + example[option] + \" [SEP]\" for option in 'ABCDE']\n","    tokenized_example = tokenizer(first_sentence, \n","                                    second_sentences, \n","                                    truncation='only_first', \n","                                    max_length=CONTEXT_LEN_8BIT, \n","                                    add_special_tokens=False)        \n","    if \"answer\" in example:\n","        tokenized_example[\"label\"] = option_to_index[example[\"answer\"]]\n","    \n","    return tokenized_example\n","            \n","    # first_sentence = [example['prompt']] * 5\n","    # second_sentence = []\n","    # for option in options:\n","    #     second_sentence.append(example[option])\n","    \n","    # tokenized_example = tokenizer(first_sentence, second_sentence, truncation='only_first')\n","    # tokenized_example['label'] = option_to_index[example['answer']]\n","    # return tokenized_example"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[],"source":["@dataclass\n","class DataCollatorForMultipleChoice:\n","    tokenizer: PreTrainedTokenizerBase\n","    padding: Union[bool, str, PaddingStrategy] = True\n","    max_length: Optional[int] = None\n","    pad_to_multiple_of: Optional[int] = None\n","    \n","    def __call__(self, features):\n","        # label_name = \"label\" if 'label' in features[0].keys() else 'labels'\n","        # labels = [feature.pop(label_name) for feature in features]\n","        # batch_size = len(features)\n","        # num_choices = len(features[0]['input_ids'])\n","        # flattened_features = [\n","        #     [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n","        # ]\n","        # flattened_features = sum(flattened_features, [])\n","        \n","        # batch = self.tokenizer.pad(\n","        #     flattened_features,\n","        #     padding=self.padding,\n","        #     max_length=self.max_length,\n","        #     pad_to_multiple_of=self.pad_to_multiple_of,\n","        #     return_tensors='pt',\n","        # )\n","        # batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n","        # batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n","        # return batch\n","        \n","        batch_size = len(features)\n","        num_choices = len(features[0][\"input_ids\"])\n","        flattened_features = [\n","            [\n","                {k: v[i] for k, v in feature.items() if k not in (\"context\", \"__index_level_0__\", \"label\", \"labels\")}\n","                for i in range(num_choices)\n","            ] for feature in features\n","        ]\n","        flattened_features = sum(flattened_features, [])\n","\n","        # _max_length = max([len(x[\"input_ids\"]) for x in flattened_features])\n","        # print(f\"{_max_length = }\")\n","\n","        batch = self.tokenizer.pad(\n","            flattened_features,\n","            padding=self.padding,\n","            max_length=self.max_length,\n","            pad_to_multiple_of=self.pad_to_multiple_of,\n","            return_tensors=\"pt\",\n","        )\n","        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n","        if \"label\" in features[0].keys() or \"labels\" in features[0].keys():\n","            label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n","            labels = [feature.pop(label_name) for feature in features]\n","            batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n","        return batch\n","        \n","        "]},{"cell_type":"markdown","metadata":{},"source":["# longformer (kaggle)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["from transformers import LongformerTokenizer, LongformerForMultipleChoice\n","\n","if RUN_ON_KAGGLE:\n","    tokenizer = LongformerTokenizer.from_pretrained(\"/kaggle/input/longformer-race-model/longformer_qa_model\")\n","    model = LongformerForMultipleChoice.from_pretrained(\"/kaggle/input/longformer-race-model/longformer_qa_model\").cuda()\n","else:\n","    tokenizer = LongformerTokenizer.from_pretrained(\"/home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/longformer-race-model/longformer_qa_model\")\n","    model = LongformerForMultipleChoice.from_pretrained(\"/home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/longformer-race-model/longformer_qa_model\").cuda()\n","    "]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[],"source":["if RUN_ON_KAGGLE:\n","    !cp /kaggle/input/datasets-wheel/datasets-2.14.4-py3-none-any.whl /kaggle/working\n","    !pip install  /kaggle/working/datasets-2.14.4-py3-none-any.whl\n","    !cp /kaggle/input/backup-806-viktor/util_openbook.py ."]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["import os\n","import gc\n","import pandas as pd\n","import numpy as np\n","import re\n","from tqdm.auto import tqdm\n","import blingfire as bf\n","\n","from collections.abc import Iterable\n","\n","import faiss\n","from faiss import write_index, read_index\n","\n","from sentence_transformers import SentenceTransformer\n","\n","import torch\n","import ctypes\n","libc = ctypes.CDLL(\"libc.so.6\")\n","\n","from dataclasses import dataclass\n","from typing import Optional, Union\n","\n","import torch\n","import numpy as np\n","import pandas as pd\n","from datasets import Dataset\n","from transformers import AutoTokenizer\n","from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n","from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n","from torch.utils.data import DataLoader\n","\n","\n","def process_documents(documents: Iterable[str],\n","                      document_ids: Iterable,\n","                      split_sentences: bool = True,\n","                      filter_len: int = 3,\n","                      disable_progress_bar: bool = False) -> pd.DataFrame:\n","    \"\"\"\n","    Main helper function to process documents from the EMR.\n","\n","    :param documents: Iterable containing documents which are strings\n","    :param document_ids: Iterable containing document unique identifiers\n","    :param document_type: String denoting the document type to be processed\n","    :param document_sections: List of sections for a given document type to process\n","    :param split_sentences: Flag to determine whether to further split sections into sentences\n","    :param filter_len: Minimum character length of a sentence (otherwise filter out)\n","    :param disable_progress_bar: Flag to disable tqdm progress bar\n","    :return: Pandas DataFrame containing the columns `document_id`, `text`, `section`, `offset`\n","    \"\"\"\n","\n","    df = sectionize_documents(documents, document_ids, disable_progress_bar)\n","\n","    if split_sentences:\n","        df = sentencize(df.text.values,\n","                        df.document_id.values,\n","                        df.offset.values,\n","                        filter_len,\n","                        disable_progress_bar)\n","    return df\n","\n","\n","def sectionize_documents(documents: Iterable[str],\n","                         document_ids: Iterable,\n","                         disable_progress_bar: bool = False) -> pd.DataFrame:\n","    \"\"\"\n","    Obtains the sections of the imaging reports and returns only the\n","    selected sections (defaults to FINDINGS, IMPRESSION, and ADDENDUM).\n","\n","    :param documents: Iterable containing documents which are strings\n","    :param document_ids: Iterable containing document unique identifiers\n","    :param disable_progress_bar: Flag to disable tqdm progress bar\n","    :return: Pandas DataFrame containing the columns `document_id`, `text`, `offset`\n","    \"\"\"\n","    processed_documents = []\n","    for document_id, document in tqdm(zip(document_ids, documents), total=len(documents), disable=disable_progress_bar):\n","        row = {}\n","        text, start, end = (document, 0, len(document))\n","        row['document_id'] = document_id\n","        row['text'] = text\n","        row['offset'] = (start, end)\n","\n","        processed_documents.append(row)\n","\n","    _df = pd.DataFrame(processed_documents)\n","    if _df.shape[0] > 0:\n","        return _df.sort_values(['document_id', 'offset']).reset_index(drop=True)\n","    else:\n","        return _df\n","\n","\n","def sentencize(documents: Iterable[str],\n","               document_ids: Iterable,\n","               offsets: Iterable[tuple[int, int]],\n","               filter_len: int = 3,\n","               disable_progress_bar: bool = False) -> pd.DataFrame:\n","    \"\"\"\n","    Split a document into sentences. Can be used with `sectionize_documents`\n","    to further split documents into more manageable pieces. Takes in offsets\n","    to ensure that after splitting, the sentences can be matched to the\n","    location in the original documents.\n","\n","    :param documents: Iterable containing documents which are strings\n","    :param document_ids: Iterable containing document unique identifiers\n","    :param offsets: Iterable tuple of the start and end indices\n","    :param filter_len: Minimum character length of a sentence (otherwise filter out)\n","    :return: Pandas DataFrame containing the columns `document_id`, `text`, `section`, `offset`\n","    \"\"\"\n","\n","    document_sentences = []\n","    for document, document_id, offset in tqdm(zip(documents, document_ids, offsets), total=len(documents),\n","                                              disable=disable_progress_bar):\n","        try:\n","            _, sentence_offsets = bf.text_to_sentences_and_offsets(document)\n","            for o in sentence_offsets:\n","                if o[1] - o[0] > filter_len:\n","                    sentence = document[o[0]:o[1]]\n","                    abs_offsets = (o[0] + offset[0], o[1] + offset[0])\n","                    row = {}\n","                    row['document_id'] = document_id\n","                    row['text'] = sentence\n","                    row['offset'] = abs_offsets\n","                    document_sentences.append(row)\n","        except:\n","            continue\n","    return pd.DataFrame(document_sentences)\n","\n","\n","def get_contexts(RUN_ON_KAGGLE, VAL_SIZE, file_for_local_cv):\n","    if RUN_ON_KAGGLE:\n","        SIM_MODEL = '/kaggle/input/sentencetransformers-allminilml6v2/sentence-transformers_all-MiniLM-L6-v2'\n","    else:\n","        SIM_MODEL = 'sentence-transformers/all-MiniLM-L6-v2'\n","        \n","    DEVICE = 0\n","    MAX_LENGTH = 384\n","    BATCH_SIZE = 16\n","\n","    if RUN_ON_KAGGLE:\n","        WIKI_PATH = \"/kaggle/input/wikipedia-20230701\"\n","    else:\n","        WIKI_PATH = \"/home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/wikipedia-2023-07-faiss-index\"\n","    wiki_files = os.listdir(WIKI_PATH)\n","\n","\n","    if RUN_ON_KAGGLE:\n","        trn = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\").drop(\"id\", axis=1)\n","    else:\n","        trn = pd.read_csv(file_for_local_cv,index_col=0).sample(n=VAL_SIZE, random_state=42).reset_index(drop=True)\n","\n","    model = SentenceTransformer(SIM_MODEL, device='cuda')\n","    model.max_seq_length = MAX_LENGTH\n","    model = model.half()\n","\n","    if RUN_ON_KAGGLE:\n","        sentence_index = read_index(\"/kaggle/input/wikipedia-2023-07-faiss-index/wikipedia_202307.index\")\n","    else:\n","        sentence_index = read_index(\"/home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/wikipedia-2023-07-faiss-index/wikipedia_202307.index\")\n","\n","    # prompt_embeddings = model.encode(trn.prompt.values, batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\n","    prompt_embeddings = model.encode(\n","        trn.apply(lambda row: f\"{row['prompt']}\\n{row['A']}\\n{row['B']}\\n{row['C']}\\n{row['D']}\\n{row['E']}\",\n","                  axis=1).values,\n","        batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\n","\n","    prompt_embeddings = prompt_embeddings.detach().cpu().numpy()\n","    _ = gc.collect()\n","\n","    # Get the top 20 pages that are likely to contain the topic of interest\n","    search_score, search_index = sentence_index.search(prompt_embeddings, 20)\n","\n","    # Save memory - delete sentence_index since it is no longer necessary\n","    del sentence_index\n","    del prompt_embeddings\n","    _ = gc.collect()\n","    libc.malloc_trim(0)\n","\n","    if RUN_ON_KAGGLE:\n","        df = pd.read_parquet(\"/kaggle/input/wikipedia-20230701/wiki_2023_index.parquet\", columns=['id', 'file'])\n","    else:\n","        df = pd.read_parquet(\"/home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/wikipedia-2023-07-faiss-index/wiki_2023_index.parquet\", columns=['id', 'file'])\n","\n","    # Get the article and associated file location using the index\n","    wikipedia_file_data = []\n","\n","    for i, (scr, idx) in tqdm(enumerate(zip(search_score, search_index)), total=len(search_score)):\n","        scr_idx = idx\n","        _df = df.loc[scr_idx].copy()\n","        _df['prompt_id'] = i\n","        wikipedia_file_data.append(_df)\n","    wikipedia_file_data = pd.concat(wikipedia_file_data).reset_index(drop=True)\n","    wikipedia_file_data = wikipedia_file_data[['id', 'prompt_id', 'file']].drop_duplicates().sort_values(\n","        ['file', 'id']).reset_index(drop=True)\n","\n","    # Save memory - delete df since it is no longer necessary\n","    del df\n","    _ = gc.collect()\n","    libc.malloc_trim(0)\n","\n","    # Get the full text data\n","    wiki_text_data = []\n","\n","    for file in tqdm(wikipedia_file_data.file.unique(), total=len(wikipedia_file_data.file.unique())):\n","        _id = [str(i) for i in wikipedia_file_data[wikipedia_file_data['file'] == file]['id'].tolist()]\n","        _df = pd.read_parquet(f\"{WIKI_PATH}/{file}\", columns=['id', 'text', 'title'])\n","\n","        _df_temp = _df[_df['id'].isin(_id)].copy()\n","        del _df\n","        _ = gc.collect()\n","        libc.malloc_trim(0)\n","        wiki_text_data.append(_df_temp)\n","    wiki_text_data = pd.concat(wiki_text_data).drop_duplicates().reset_index(drop=True)\n","    _ = gc.collect()\n","\n","    # Parse documents into sentences\n","    processed_wiki_text_data = process_documents(wiki_text_data.text.values, wiki_text_data.id.values)\n","\n","    # Get embeddings of the wiki text data\n","    wiki_data_embeddings = model.encode(processed_wiki_text_data.text,\n","                                        batch_size=BATCH_SIZE,\n","                                        device=DEVICE,\n","                                        show_progress_bar=True,\n","                                        convert_to_tensor=True,\n","                                        normalize_embeddings=True)  # .half()\n","    wiki_data_embeddings = wiki_data_embeddings.detach().cpu().numpy()\n","\n","    _ = gc.collect()\n","\n","    # Combine all answers\n","    trn['answer_all'] = trn.apply(lambda x: \" \".join([x['A'], x['B'], x['C'], x['D'], x['E']]), axis=1)\n","\n","    # Search using the prompt and answers to guide the search\n","    trn['prompt_answer_stem'] = trn['prompt'] + \" \" + trn['answer_all']\n","\n","    question_embeddings = model.encode(trn.prompt_answer_stem.values, batch_size=BATCH_SIZE, device=DEVICE,\n","                                       show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\n","    question_embeddings = question_embeddings.detach().cpu().numpy()\n","\n","    # Parameter to determine how many relevant sentences to include\n","    NUM_SENTENCES_INCLUDE = 6\n","\n","    # List containing just Context\n","    contexts = []\n","\n","    for r in tqdm(trn.itertuples(), total=len(trn)):\n","\n","        prompt_id = r.Index\n","\n","        prompt_indices = processed_wiki_text_data[processed_wiki_text_data['document_id'].isin(\n","            wikipedia_file_data[wikipedia_file_data['prompt_id'] == prompt_id]['id'].values)].index.values\n","\n","        if prompt_indices.shape[0] > 0:\n","            prompt_index = faiss.index_factory(wiki_data_embeddings.shape[1], \"Flat\")\n","            prompt_index.add(wiki_data_embeddings[prompt_indices])\n","\n","            context = \"\"\n","\n","            # Get the top matches\n","            ss, ii = prompt_index.search(question_embeddings, NUM_SENTENCES_INCLUDE)\n","            for _s, _i in zip(ss[prompt_id], ii[prompt_id]):\n","                context += processed_wiki_text_data.loc[prompt_indices]['text'].iloc[_i] + \" \"\n","        contexts.append(context)\n","\n","    trn['context'] = contexts\n","\n","    trn[[\"prompt\", \"context\", \"A\", \"B\", \"C\", \"D\", \"E\"]].to_csv(\"./test_context.csv\", index=False)\n","\n","\n","# def generate_openbook_output(RUN_ON_KAGGLE):\n","#     test_df = pd.read_csv(\"test_context.csv\")\n","#     test_df.index = list(range(len(test_df)))\n","#     test_df['id'] = list(range(len(test_df)))\n","#     test_df[\"prompt\"] = test_df[\"context\"].apply(lambda x: x[:1750]) + \" #### \" + test_df[\"prompt\"]\n","#     test_df['answer'] = 'A'\n","#     if RUN_ON_KAGGLE:\n","#         model_dir = \"/kaggle/input/llm-science-run-context-2\"\n","#     else:\n","#         model_dir = \"/home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/llm-science-run-context-2\"\n","#     tokenizer = AutoTokenizer.from_pretrained(model_dir)\n","#     model = AutoModelForMultipleChoice.from_pretrained(model_dir).cuda()\n","#     model.eval()\n","\n","#     # We'll create a dictionary to convert option names (A, B, C, D, E) into indices and back again\n","#     options = 'ABCDE'\n","#     indices = list(range(5))\n","\n","#     option_to_index = {option: index for option, index in zip(options, indices)}\n","#     index_to_option = {index: option for option, index in zip(options, indices)}\n","\n","#     def preprocess(example):\n","#         # The AutoModelForMultipleChoice class expects a set of question/answer pairs\n","#         # so we'll copy our question 5 times before tokenizing\n","#         first_sentence = [example['prompt']] * 5\n","#         second_sentence = []\n","#         for option in options:\n","#             second_sentence.append(example[option])\n","#         # Our tokenizer will turn our text into token IDs BERT can understand\n","#         tokenized_example = tokenizer(first_sentence, second_sentence, truncation=True)\n","#         tokenized_example['label'] = option_to_index[example['answer']]\n","#         return tokenized_example\n","\n","#     tokenized_test_dataset = Dataset.from_pandas(test_df[['id', 'prompt', 'A', 'B', 'C', 'D', 'E', 'answer']].drop(columns=['id'])).map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])\n","#     tokenized_test_dataset = tokenized_test_dataset.remove_columns([\"__index_level_0__\"])\n","#     data_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\n","#     test_dataloader = DataLoader(tokenized_test_dataset, batch_size=1, shuffle=False, collate_fn=data_collator)\n","\n","#     test_predictions = []\n","#     for batch in test_dataloader:\n","#         for k in batch.keys():\n","#             batch[k] = batch[k].cuda()\n","#         with torch.no_grad():\n","#             outputs = model(**batch)\n","#         test_predictions.append(outputs.logits.cpu().detach())\n","\n","#     test_predictions = torch.cat(test_predictions)\n","\n","#     predictions_as_ids = np.argsort(-test_predictions, 1)\n","\n","#     predictions_as_answer_letters = np.array(list('ABCDE'))[predictions_as_ids]\n","#     # predictions_as_answer_letters[:3]\n","\n","#     predictions_as_string = test_df['prediction'] = [\n","#         ' '.join(row) for row in predictions_as_answer_letters[:, :3]\n","#     ]\n","\n","#     submission = test_df[['id', 'prediction']]\n","#     submission.to_csv('submission_backup.csv', index=False)\n"]},{"cell_type":"code","execution_count":13,"metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d605e7e0278b4d628cca8e8249eda10f","version_major":2,"version_minor":0},"text/plain":["Batches:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5ac5ac7d21864d4f80c39144c90711ff","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/20 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4159b46c95cd48a5895627a44452beb0","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/26 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dc113c7256d0483e9857924620e099cb","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/400 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"066e01c7dc6d43f4bca8ea0e66d4704d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/400 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a4a89992eeb84535b6e0d1bf6909c943","version_major":2,"version_minor":0},"text/plain":["Batches:   0%|          | 0/726 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"be279576e06b4ab49978b9af8cf9363c","version_major":2,"version_minor":0},"text/plain":["Batches:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7302c0fb85794db29a1bb637a58375aa","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/20 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["95"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["import pickle\n","\n","get_contexts(RUN_ON_KAGGLE, VAL_SIZE, file_for_local_cv)\n","# generate_openbook_output(RUN_ON_KAGGLE)\n","\n","import gc\n","gc.collect()"]},{"cell_type":"code","execution_count":14,"metadata":{"trusted":true},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":15,"metadata":{"trusted":true},"outputs":[],"source":["if RUN_ON_KAGGLE:\n","    !cp -r /kaggle/input/stem-wiki-cohere-no-emb /kaggle/working\n","    !cp -r /kaggle/input/all-paraphs-parsed-expanded /kaggle/working/"]},{"cell_type":"code","execution_count":16,"metadata":{"trusted":true},"outputs":[],"source":["from datasets import load_dataset, load_from_disk"]},{"cell_type":"code","execution_count":17,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","import unicodedata\n","\n","\n","def SplitList(mylist, chunk_size):\n","    return [mylist[offs:offs+chunk_size] for offs in range(0, len(mylist), chunk_size)]\n","\n","def get_relevant_documents_parsed(df_test):\n","    df_chunk_size=600\n","    if RUN_ON_KAGGLE:\n","        paraphs_parsed_dataset = load_from_disk(\"/kaggle/working/all-paraphs-parsed-expanded\")\n","    else:\n","        paraphs_parsed_dataset = load_from_disk(\"/home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/all-paraphs-parsed-expanded\")\n","    \n","    modified_texts = paraphs_parsed_dataset.map(lambda example:\n","                                             {'temp_text':\n","                                              f\"{example['title']} {example['section']} {example['text']}\".replace('\\n',\" \").replace(\"'\",\"\")},\n","                                             num_proc=2)[\"temp_text\"]\n","    \n","    all_articles_indices = []\n","    all_articles_values = []\n","    for idx in tqdm(range(0, df_test.shape[0], df_chunk_size)):\n","        df_test_ = df_test.iloc[idx: idx+df_chunk_size]\n","    \n","        articles_indices, merged_top_scores = retrieval(df_test_, modified_texts)\n","        all_articles_indices.append(articles_indices)\n","        all_articles_values.append(merged_top_scores)\n","        \n","    article_indices_array =  np.concatenate(all_articles_indices, axis=0)\n","    articles_values_array = np.concatenate(all_articles_values, axis=0).reshape(-1)\n","    \n","    top_per_query = article_indices_array.shape[1]\n","    articles_flatten = [(\n","                         articles_values_array[index],\n","                         paraphs_parsed_dataset[idx.item()][\"title\"],\n","                         paraphs_parsed_dataset[idx.item()][\"text\"],\n","                        )\n","                        for index,idx in enumerate(article_indices_array.reshape(-1))]\n","    retrieved_articles = SplitList(articles_flatten, top_per_query)\n","    return retrieved_articles\n","\n","\n","\n","def get_relevant_documents(df_test):\n","    df_chunk_size=800\n","    if RUN_ON_KAGGLE:\n","        cohere_dataset_filtered = load_from_disk(\"/kaggle/working/stem-wiki-cohere-no-emb\")\n","    else:\n","        cohere_dataset_filtered = load_from_disk(\"/home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/wiki-stem-cohere\")\n","    modified_texts = cohere_dataset_filtered.map(lambda example:\n","                                             {'temp_text':\n","                                              unicodedata.normalize(\"NFKD\", f\"{example['title']} {example['text']}\").replace('\"',\"\")},\n","                                             num_proc=2)[\"temp_text\"]\n","    \n","    all_articles_indices = []\n","    all_articles_values = []\n","    for idx in tqdm(range(0, df_test.shape[0], df_chunk_size)):\n","        df_test_ = df_test.iloc[idx: idx+df_chunk_size]\n","    \n","        articles_indices, merged_top_scores = retrieval(df_test_, modified_texts)\n","        all_articles_indices.append(articles_indices)\n","        all_articles_values.append(merged_top_scores)\n","        \n","    article_indices_array =  np.concatenate(all_articles_indices, axis=0)\n","    articles_values_array = np.concatenate(all_articles_values, axis=0).reshape(-1)\n","    \n","    top_per_query = article_indices_array.shape[1]\n","    articles_flatten = [(\n","                         articles_values_array[index],\n","                         cohere_dataset_filtered[idx.item()][\"title\"],\n","                         unicodedata.normalize(\"NFKD\", cohere_dataset_filtered[idx.item()][\"text\"]),\n","                        )\n","                        for index,idx in enumerate(article_indices_array.reshape(-1))]\n","    retrieved_articles = SplitList(articles_flatten, top_per_query)\n","    return retrieved_articles\n","\n","\n","\n","def retrieval(df_test, modified_texts):\n","    \n","    corpus_df_test = df_test.apply(lambda row:\n","                                     f'{row[\"prompt\"]}\\n{row[\"prompt\"]}\\n{row[\"prompt\"]}\\n{row[\"A\"]}\\n{row[\"B\"]}\\n{row[\"C\"]}\\n{row[\"D\"]}\\n{row[\"E\"]}',\n","                                     axis=1).values\n","    vectorizer1 = TfidfVectorizer(ngram_range=(1,2),\n","                                 token_pattern=r\"(?u)\\b[\\w/.-]+\\b|!|/|\\?|\\\"|\\'\",\n","                                 stop_words=stop_words)\n","    vectorizer1.fit(corpus_df_test)\n","    vocab_df_test = vectorizer1.get_feature_names_out()\n","    vectorizer = TfidfVectorizer(ngram_range=(1,2),\n","                                 token_pattern=r\"(?u)\\b[\\w/.-]+\\b|!|/|\\?|\\\"|\\'\",\n","                                 stop_words=stop_words,\n","                                 vocabulary=vocab_df_test)\n","    vectorizer.fit(modified_texts[:500000])\n","    corpus_tf_idf = vectorizer.transform(corpus_df_test)\n","    \n","    print(f\"length of vectorizer vocab is {len(vectorizer.get_feature_names_out())}\")\n","\n","    chunk_size = 100000\n","    top_per_chunk = 10\n","    top_per_query = 10\n","\n","    all_chunk_top_indices = []\n","    all_chunk_top_values = []\n","\n","    for idx in tqdm(range(0, len(modified_texts), chunk_size)):\n","        wiki_vectors = vectorizer.transform(modified_texts[idx: idx+chunk_size])\n","        temp_scores = (corpus_tf_idf * wiki_vectors.T).toarray()\n","        chunk_top_indices = temp_scores.argpartition(-top_per_chunk, axis=1)[:, -top_per_chunk:]\n","        chunk_top_values = temp_scores[np.arange(temp_scores.shape[0])[:, np.newaxis], chunk_top_indices]\n","\n","        all_chunk_top_indices.append(chunk_top_indices + idx)\n","        all_chunk_top_values.append(chunk_top_values)\n","\n","    top_indices_array = np.concatenate(all_chunk_top_indices, axis=1)\n","    top_values_array = np.concatenate(all_chunk_top_values, axis=1)\n","    \n","    merged_top_scores = np.sort(top_values_array, axis=1)[:,-top_per_query:]\n","    merged_top_indices = top_values_array.argsort(axis=1)[:,-top_per_query:]\n","    articles_indices = top_indices_array[np.arange(top_indices_array.shape[0])[:, np.newaxis], merged_top_indices]\n","    \n","    return articles_indices, merged_top_scores\n","\n","\n","def prepare_answering_input(\n","        tokenizer, \n","        question,  \n","        options,   \n","        context,   \n","        max_seq_length=4096,\n","    ):\n","    c_plus_q   = context + ' ' + tokenizer.bos_token + ' ' + question\n","    c_plus_q_4 = [c_plus_q] * len(options)\n","    tokenized_examples = tokenizer(\n","        c_plus_q_4, options,\n","        max_length=max_seq_length,\n","        padding=\"longest\",\n","        truncation=False,\n","        return_tensors=\"pt\",\n","    )\n","    input_ids = tokenized_examples['input_ids'].unsqueeze(0)\n","    attention_mask = tokenized_examples['attention_mask'].unsqueeze(0)\n","    example_encoded = {\n","        \"input_ids\": input_ids.to(model.device.index),\n","        \"attention_mask\": attention_mask.to(model.device.index),\n","    }\n","    return example_encoded"]},{"cell_type":"code","execution_count":18,"metadata":{"trusted":true},"outputs":[],"source":["stop_words = ['each', 'you', 'the', 'use', 'used',\n","                  'where', 'themselves', 'nor', \"it's\", 'how', \"don't\", 'just', 'your',\n","                  'about', 'himself', 'with', \"weren't\", 'hers', \"wouldn't\", 'more', 'its', 'were',\n","                  'his', 'their', 'then', 'been', 'myself', 're', 'not',\n","                  'ours', 'will', 'needn', 'which', 'here', 'hadn', 'it', 'our', 'there', 'than',\n","                  'most', \"couldn't\", 'both', 'some', 'for', 'up', 'couldn', \"that'll\",\n","                  \"she's\", 'over', 'this', 'now', 'until', 'these', 'few', 'haven',\n","                  'of', 'wouldn', 'into', 'too', 'to', 'very', 'shan', 'before', 'the', 'they',\n","                  'between', \"doesn't\", 'are', 'was', 'out', 'we', 'me',\n","                  'after', 'has', \"isn't\", 'have', 'such', 'should', 'yourselves', 'or', 'during', 'herself',\n","                  'doing', 'in', \"shouldn't\", \"won't\", 'when', 'do', 'through', 'she',\n","                  'having', 'him', \"haven't\", 'against', 'itself', 'that',\n","                  'did', 'theirs', 'can', 'those',\n","                  'own', 'so', 'and', 'who', \"you've\", 'yourself', 'her', 'he', 'only',\n","                  'what', 'ourselves', 'again', 'had', \"you'd\", 'is', 'other',\n","                  'why', 'while', 'from', 'them', 'if', 'above', 'does', 'whom',\n","                  'yours', 'but', 'being', \"wasn't\", 'be']"]},{"cell_type":"code","execution_count":19,"metadata":{"trusted":true},"outputs":[],"source":["if RUN_ON_KAGGLE:\n","    df_valid = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\")\n","else:\n","    df_valid = pd.read_csv(file_for_local_cv).sample(n=VAL_SIZE, random_state=42).reset_index(drop=True)\n"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Loading cached processed dataset at /home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/all-paraphs-parsed-expanded/cache-d6a95ad6d2289009_*_of_00002.arrow\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a8ee419952074949bfef025c823aee1f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/viktor/miniconda3/envs/torch-env/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'd', 'doesn', 'don', 'isn', 'll', 's', 'shouldn', 't', 've', 'wasn', 'weren', 'won'] not in stop_words.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["length of vectorizer vocab is 911\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d9639f9445f44afc84546ba9a94dc0a9","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/22 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["38"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["retrieved_articles_parsed = get_relevant_documents_parsed(df_valid)\n","gc.collect()"]},{"cell_type":"code","execution_count":21,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>id</th>\n","      <th>prompt</th>\n","      <th>A</th>\n","      <th>B</th>\n","      <th>C</th>\n","      <th>D</th>\n","      <th>E</th>\n","      <th>answer</th>\n","      <th>cluster number</th>\n","      <th>round</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>13884</td>\n","      <td>13988</td>\n","      <td>What is the relationship between Adiantum and ...</td>\n","      <td>Adiantum is a variant of AES</td>\n","      <td>Adiantum is a cryptographic message authentica...</td>\n","      <td>Adiantum uses AES for disk encryption</td>\n","      <td>Adiantum is a low-powered mobile device runnin...</td>\n","      <td>Adiantum uses a different construction for the...</td>\n","      <td>C</td>\n","      <td>590</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>20379</td>\n","      <td>20507</td>\n","      <td>What is the cause of the painful bump in Osgoo...</td>\n","      <td>Excessive bone growth at the tibial tuberosity</td>\n","      <td>Fluid accumulation in the patellar ligament</td>\n","      <td>Inflammation of the quadriceps muscle</td>\n","      <td>Degeneration of the knee cartilage</td>\n","      <td>Dislocation of the tibiofemoral joint</td>\n","      <td>A</td>\n","      <td>827</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>24313</td>\n","      <td>24459</td>\n","      <td>What are the two most common types of tarsal c...</td>\n","      <td>Calcaneo-cuboid and calcaneo-navicular</td>\n","      <td>Talo-cuneiform and talo-tarsal</td>\n","      <td>Navicular-cuboid and calcaneo-tarsal</td>\n","      <td>Talo-calcaneal and talo-navicular</td>\n","      <td>Calcaneo-navicular and talo-calcaneal</td>\n","      <td>E</td>\n","      <td>988</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>22258</td>\n","      <td>22394</td>\n","      <td>What is the significance of dietary restrictio...</td>\n","      <td>Dietary restrictions can worsen the symptoms o...</td>\n","      <td>Dietary restrictions can completely cure ocula...</td>\n","      <td>Dietary restrictions have no impact on the sym...</td>\n","      <td>Dietary restrictions may reduce or eliminate t...</td>\n","      <td>Dietary restrictions only impact the symptoms ...</td>\n","      <td>D</td>\n","      <td>902</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>16046</td>\n","      <td>16155</td>\n","      <td>What is the purpose of isolating Parvibaculum ...</td>\n","      <td>To remove it from wastewater treatment plants ...</td>\n","      <td>To improve the aesthetics of activated sludge</td>\n","      <td>To study its metabolic abilities and potential...</td>\n","      <td>To increase the production of linear alkylbenz...</td>\n","      <td>To enhance the growth of other bacteria in the...</td>\n","      <td>C</td>\n","      <td>668</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>17785</td>\n","      <td>17902</td>\n","      <td>In what field do Latin rectangles have applica...</td>\n","      <td>Mathematics and number theory</td>\n","      <td>Computer science and data visualization</td>\n","      <td>Statistics and the design of experiments</td>\n","      <td>Chemistry and molecular biology</td>\n","      <td>Physics and quantum mechanics</td>\n","      <td>C</td>\n","      <td>730</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>5117</td>\n","      <td>5151</td>\n","      <td>What is the role of acetolactate decarboxylase?</td>\n","      <td>To cleave carbon-carbon bonds in the acetoacet...</td>\n","      <td>To convert (R)-2-acetoin to (S)-2-hydroxy-2-me...</td>\n","      <td>To catalyze the synthesis of (S)-2-hydroxy-2-m...</td>\n","      <td>To participate in butanoate metabolism and c5-...</td>\n","      <td>To catalyze the conversion of (S)-2-hydroxy-2-...</td>\n","      <td>E</td>\n","      <td>279</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>11895</td>\n","      <td>11975</td>\n","      <td>What are the main fields of computational engi...</td>\n","      <td>Numerical analysis and scientific simulation</td>\n","      <td>Structural mechanics and acoustics</td>\n","      <td>Computational biology and chemistry</td>\n","      <td>Electromagnetics and thermodynamics</td>\n","      <td>Elasticity and fluid mechanics</td>\n","      <td>E</td>\n","      <td>524</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>11941</td>\n","      <td>12021</td>\n","      <td>What is the significance of the name 'mycofact...</td>\n","      <td>It is closely related to the biosynthesis of s...</td>\n","      <td>It is derived from 'Mycobacterium', predicting...</td>\n","      <td>It predicts the co-occurrence of enzymes as a ...</td>\n","      <td>It is derived from the peptide sequence of myc...</td>\n","      <td>It indicates its involvement in redox pathways</td>\n","      <td>C</td>\n","      <td>525</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>10163</td>\n","      <td>10243</td>\n","      <td>What is the term used to describe AIPL1 mutati...</td>\n","      <td>Impaired vision or blindness</td>\n","      <td>The earlier onset of LCA</td>\n","      <td>Increased severity of LCA</td>\n","      <td>Recessive LCA</td>\n","      <td>Impaired electroretinogram</td>\n","      <td>C</td>\n","      <td>459</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>24576</td>\n","      <td>24732</td>\n","      <td>What are the main genera included in the Acant...</td>\n","      <td>Kennediella, Protacanthoceras, Pseudovascocera...</td>\n","      <td>Acompsoceras, Cunningtoniceras, Kastanoceras, ...</td>\n","      <td>Acanthoceras, Benueites, Calycoceras, Conlinoc...</td>\n","      <td>Nebraskites, Nigericeras, Paraconlinoceras, Pl...</td>\n","      <td>Hypacanthohoplites, Sumitomoceras, Tarrantocer...</td>\n","      <td>C</td>\n","      <td>997</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>9194</td>\n","      <td>9262</td>\n","      <td>What was the escape character used in the 6/12...</td>\n","      <td>Octal 12</td>\n","      <td>Octal 60</td>\n","      <td>Octal 63</td>\n","      <td>Octal 76</td>\n","      <td>Octal 100</td>\n","      <td>D</td>\n","      <td>431</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>23004</td>\n","      <td>23148</td>\n","      <td>What is the function of cementicles in the den...</td>\n","      <td>To strengthen the enamel layer of a tooth.</td>\n","      <td>To provide a protective barrier against dental...</td>\n","      <td>To facilitate the attachment of periodontal li...</td>\n","      <td>To aid in the formation of dental caries.</td>\n","      <td>To contribute to tooth eruption.</td>\n","      <td>C</td>\n","      <td>928</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>19159</td>\n","      <td>19287</td>\n","      <td>What is the purpose of a pulsed inductive thru...</td>\n","      <td>To generate a radial magnetic field</td>\n","      <td>To accelerate a propellant using perpendicular...</td>\n","      <td>To release a puff of gas into space</td>\n","      <td>To measure the exhaust velocity of a propellant</td>\n","      <td>To ionize charged particles in a gas</td>\n","      <td>B</td>\n","      <td>779</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>3820</td>\n","      <td>3851</td>\n","      <td>What is the distribution of the neutral hydrog...</td>\n","      <td>The neutral hydrogen in Dwingeloo 1 is absent.</td>\n","      <td>The neutral hydrogen in Dwingeloo 1 is concent...</td>\n","      <td>The neutral hydrogen in Dwingeloo 1 forms a ri...</td>\n","      <td>The neutral hydrogen in Dwingeloo 1 is detecte...</td>\n","      <td>The neutral hydrogen in Dwingeloo 1 is distrib...</td>\n","      <td>D</td>\n","      <td>23</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>2136</td>\n","      <td>2146</td>\n","      <td>What are the two main improvements of the P5 m...</td>\n","      <td>The P5 microarchitecture introduced a 64-bit e...</td>\n","      <td>The P5 microarchitecture had a wider data bus ...</td>\n","      <td>The P5 microarchitecture had a fully hardware-...</td>\n","      <td>The P5 microarchitecture introduced a supersca...</td>\n","      <td>The P5 microarchitecture introduced enhanced d...</td>\n","      <td>D</td>\n","      <td>169</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>12536</td>\n","      <td>12616</td>\n","      <td>What is the term used to describe the removal ...</td>\n","      <td>Lobectomy</td>\n","      <td>Extrapleural pneumonectomy</td>\n","      <td>Pneumonectomy</td>\n","      <td>Segmentectomy</td>\n","      <td>Wedge resection</td>\n","      <td>B</td>\n","      <td>548</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>13888</td>\n","      <td>13992</td>\n","      <td>What is the difference between a substitution ...</td>\n","      <td>In a substitution cipher, the units of the pla...</td>\n","      <td>In a substitution cipher, the units of the pla...</td>\n","      <td>In a substitution cipher, the units of the pla...</td>\n","      <td>In a substitution cipher, the units of the pla...</td>\n","      <td>In a substitution cipher, the units of the pla...</td>\n","      <td>B</td>\n","      <td>590</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>23956</td>\n","      <td>24102</td>\n","      <td>What is the role of the caloric polynomial in ...</td>\n","      <td>It is used to solve parabolic equations</td>\n","      <td>It is used to solve polynomial equations</td>\n","      <td>It is used to solve differential equations</td>\n","      <td>It is used to solve linear equations</td>\n","      <td>It is used to solve the heat equation</td>\n","      <td>E</td>\n","      <td>967</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>14381</td>\n","      <td>14485</td>\n","      <td>What is the estimated percentage of viral sequ...</td>\n","      <td>None of the above</td>\n","      <td>Around 20%</td>\n","      <td>Less than 10%</td>\n","      <td>More than 90%</td>\n","      <td>Between 40% and 90%</td>\n","      <td>E</td>\n","      <td>605</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    Unnamed: 0     id                                             prompt   \n","0        13884  13988  What is the relationship between Adiantum and ...  \\\n","1        20379  20507  What is the cause of the painful bump in Osgoo...   \n","2        24313  24459  What are the two most common types of tarsal c...   \n","3        22258  22394  What is the significance of dietary restrictio...   \n","4        16046  16155  What is the purpose of isolating Parvibaculum ...   \n","5        17785  17902  In what field do Latin rectangles have applica...   \n","6         5117   5151    What is the role of acetolactate decarboxylase?   \n","7        11895  11975  What are the main fields of computational engi...   \n","8        11941  12021  What is the significance of the name 'mycofact...   \n","9        10163  10243  What is the term used to describe AIPL1 mutati...   \n","10       24576  24732  What are the main genera included in the Acant...   \n","11        9194   9262  What was the escape character used in the 6/12...   \n","12       23004  23148  What is the function of cementicles in the den...   \n","13       19159  19287  What is the purpose of a pulsed inductive thru...   \n","14        3820   3851  What is the distribution of the neutral hydrog...   \n","15        2136   2146  What are the two main improvements of the P5 m...   \n","16       12536  12616  What is the term used to describe the removal ...   \n","17       13888  13992  What is the difference between a substitution ...   \n","18       23956  24102  What is the role of the caloric polynomial in ...   \n","19       14381  14485  What is the estimated percentage of viral sequ...   \n","\n","                                                    A   \n","0                        Adiantum is a variant of AES  \\\n","1      Excessive bone growth at the tibial tuberosity   \n","2              Calcaneo-cuboid and calcaneo-navicular   \n","3   Dietary restrictions can worsen the symptoms o...   \n","4   To remove it from wastewater treatment plants ...   \n","5                       Mathematics and number theory   \n","6   To cleave carbon-carbon bonds in the acetoacet...   \n","7        Numerical analysis and scientific simulation   \n","8   It is closely related to the biosynthesis of s...   \n","9                        Impaired vision or blindness   \n","10  Kennediella, Protacanthoceras, Pseudovascocera...   \n","11                                           Octal 12   \n","12         To strengthen the enamel layer of a tooth.   \n","13                To generate a radial magnetic field   \n","14     The neutral hydrogen in Dwingeloo 1 is absent.   \n","15  The P5 microarchitecture introduced a 64-bit e...   \n","16                                          Lobectomy   \n","17  In a substitution cipher, the units of the pla...   \n","18            It is used to solve parabolic equations   \n","19                                  None of the above   \n","\n","                                                    B   \n","0   Adiantum is a cryptographic message authentica...  \\\n","1         Fluid accumulation in the patellar ligament   \n","2                      Talo-cuneiform and talo-tarsal   \n","3   Dietary restrictions can completely cure ocula...   \n","4       To improve the aesthetics of activated sludge   \n","5             Computer science and data visualization   \n","6   To convert (R)-2-acetoin to (S)-2-hydroxy-2-me...   \n","7                  Structural mechanics and acoustics   \n","8   It is derived from 'Mycobacterium', predicting...   \n","9                            The earlier onset of LCA   \n","10  Acompsoceras, Cunningtoniceras, Kastanoceras, ...   \n","11                                           Octal 60   \n","12  To provide a protective barrier against dental...   \n","13  To accelerate a propellant using perpendicular...   \n","14  The neutral hydrogen in Dwingeloo 1 is concent...   \n","15  The P5 microarchitecture had a wider data bus ...   \n","16                         Extrapleural pneumonectomy   \n","17  In a substitution cipher, the units of the pla...   \n","18           It is used to solve polynomial equations   \n","19                                         Around 20%   \n","\n","                                                    C   \n","0               Adiantum uses AES for disk encryption  \\\n","1               Inflammation of the quadriceps muscle   \n","2                Navicular-cuboid and calcaneo-tarsal   \n","3   Dietary restrictions have no impact on the sym...   \n","4   To study its metabolic abilities and potential...   \n","5            Statistics and the design of experiments   \n","6   To catalyze the synthesis of (S)-2-hydroxy-2-m...   \n","7                 Computational biology and chemistry   \n","8   It predicts the co-occurrence of enzymes as a ...   \n","9                           Increased severity of LCA   \n","10  Acanthoceras, Benueites, Calycoceras, Conlinoc...   \n","11                                           Octal 63   \n","12  To facilitate the attachment of periodontal li...   \n","13                To release a puff of gas into space   \n","14  The neutral hydrogen in Dwingeloo 1 forms a ri...   \n","15  The P5 microarchitecture had a fully hardware-...   \n","16                                      Pneumonectomy   \n","17  In a substitution cipher, the units of the pla...   \n","18         It is used to solve differential equations   \n","19                                      Less than 10%   \n","\n","                                                    D   \n","0   Adiantum is a low-powered mobile device runnin...  \\\n","1                  Degeneration of the knee cartilage   \n","2                   Talo-calcaneal and talo-navicular   \n","3   Dietary restrictions may reduce or eliminate t...   \n","4   To increase the production of linear alkylbenz...   \n","5                     Chemistry and molecular biology   \n","6   To participate in butanoate metabolism and c5-...   \n","7                 Electromagnetics and thermodynamics   \n","8   It is derived from the peptide sequence of myc...   \n","9                                       Recessive LCA   \n","10  Nebraskites, Nigericeras, Paraconlinoceras, Pl...   \n","11                                           Octal 76   \n","12          To aid in the formation of dental caries.   \n","13    To measure the exhaust velocity of a propellant   \n","14  The neutral hydrogen in Dwingeloo 1 is detecte...   \n","15  The P5 microarchitecture introduced a supersca...   \n","16                                      Segmentectomy   \n","17  In a substitution cipher, the units of the pla...   \n","18               It is used to solve linear equations   \n","19                                      More than 90%   \n","\n","                                                    E answer  cluster number   \n","0   Adiantum uses a different construction for the...      C             590  \\\n","1               Dislocation of the tibiofemoral joint      A             827   \n","2               Calcaneo-navicular and talo-calcaneal      E             988   \n","3   Dietary restrictions only impact the symptoms ...      D             902   \n","4   To enhance the growth of other bacteria in the...      C             668   \n","5                       Physics and quantum mechanics      C             730   \n","6   To catalyze the conversion of (S)-2-hydroxy-2-...      E             279   \n","7                      Elasticity and fluid mechanics      E             524   \n","8      It indicates its involvement in redox pathways      C             525   \n","9                          Impaired electroretinogram      C             459   \n","10  Hypacanthohoplites, Sumitomoceras, Tarrantocer...      C             997   \n","11                                          Octal 100      D             431   \n","12                   To contribute to tooth eruption.      C             928   \n","13               To ionize charged particles in a gas      B             779   \n","14  The neutral hydrogen in Dwingeloo 1 is distrib...      D              23   \n","15  The P5 microarchitecture introduced enhanced d...      D             169   \n","16                                    Wedge resection      B             548   \n","17  In a substitution cipher, the units of the pla...      B             590   \n","18              It is used to solve the heat equation      E             967   \n","19                                Between 40% and 90%      E             605   \n","\n","    round  \n","0       0  \n","1       1  \n","2       0  \n","3       0  \n","4       0  \n","5       1  \n","6       0  \n","7       1  \n","8       1  \n","9       1  \n","10      0  \n","11      1  \n","12      1  \n","13      1  \n","14      1  \n","15      1  \n","16      0  \n","17      1  \n","18      1  \n","19      0  "]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["df_valid"]},{"cell_type":"code","execution_count":22,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Loading cached processed dataset at /home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/wiki-stem-cohere/cache-e94d488c6798573e_*_of_00002.arrow\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4a3bdd23945c42f08ef7af492f2563df","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/viktor/miniconda3/envs/torch-env/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'd', 'doesn', 'don', 'isn', 'll', 's', 'shouldn', 't', 've', 'wasn', 'weren', 'won'] not in stop_words.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["length of vectorizer vocab is 911\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0e3acd29ab284573b31bac72b886888b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/28 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["38"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["retrieved_articles = get_relevant_documents(df_valid)\n","gc.collect()"]},{"cell_type":"code","execution_count":23,"metadata":{"trusted":true},"outputs":[],"source":["from transformers import LongformerTokenizer, LongformerForMultipleChoice\n","\n","if RUN_ON_KAGGLE:\n","    tokenizer = LongformerTokenizer.from_pretrained(\"/kaggle/input/longformer-race-model/longformer_qa_model\")\n","    model = LongformerForMultipleChoice.from_pretrained(\"/kaggle/input/longformer-race-model/longformer_qa_model\").cuda()\n","else:\n","    tokenizer = LongformerTokenizer.from_pretrained(\"/home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/longformer-race-model/longformer_qa_model\")\n","    model = LongformerForMultipleChoice.from_pretrained(\"/home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/longformer-race-model/longformer_qa_model\").cuda()\n","    "]},{"cell_type":"code","execution_count":24,"metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a667290a4ae746ad9ccf887073188452","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/20 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["predictions = []\n","\n","test_predictions_longformer_1 = []\n","test_predictions_longformer_2 = []\n","longformer_contexts = []\n","\n","\n","for index in tqdm(range(df_valid.shape[0])):\n","    row = df_valid.iloc[index]\n","    # question is 'prompt'\n","    question = row['prompt']\n","    options = [row['A'], row['B'], row['C'], row['D'], row['E']]\n","    context1 = f\"{retrieved_articles[index][-4][2]}\\n{retrieved_articles[index][-3][2]}\\n{retrieved_articles[index][-2][2]}\\n{retrieved_articles[index][-1][2]}\"\n","    context2 = f\"{retrieved_articles_parsed[index][-3][2]}\\n{retrieved_articles_parsed[index][-2][2]}\\n{retrieved_articles_parsed[index][-1][2]}\"\n","    \n","    longformer_contexts.append(context1)\n","    \n","    inputs1 = prepare_answering_input(\n","        tokenizer=tokenizer, question=question,\n","        options=options, context=context1,\n","        )\n","    inputs2 = prepare_answering_input(\n","        tokenizer=tokenizer, question=question,\n","        options=options, context=context2,\n","        )\n","    \n","    with torch.no_grad():\n","        outputs1 = model(**inputs1)    \n","        losses1 = -outputs1.logits[0].detach().cpu().numpy()\n","        probability1 = torch.softmax(torch.tensor(-losses1), dim=-1)\n","        \n","    with torch.no_grad():\n","        outputs2 = model(**inputs2)\n","        losses2 = -outputs2.logits[0].detach().cpu().numpy()\n","        probability2 = torch.softmax(torch.tensor(-losses2), dim=-1)\n","        \n","    \n","    test_predictions_longformer_1.append(probability1.numpy().astype(np.float16))\n","    test_predictions_longformer_2.append(probability2.numpy().astype(np.float16))\n","    \n"]},{"cell_type":"markdown","metadata":{},"source":["# combine predictions (and find best combination on local CV )"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["preds_dict = {\n","    'test_predictions_longformer_1':np.stack(test_predictions_longformer_1),\n","}\n","# save preds_dict as pickle\n","with open('preds_dict_longformer.pickle', 'wb') as handle:\n","    pickle.dump(preds_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":33,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[1.]\n","0.675\n"]}],"source":["\n","import pickle\n","\n","# load preds_dict_longformer.pickle as preds_dict\n","with open('preds_dict_longformer.pickle', 'rb') as handle:\n","    preds_dict = pickle.load(handle)\n","    \n","preds_dict\n","\n","weights = np.ones(len(preds_dict))\n","\n","predictions_overall = np.sum([preds_val*weight for preds_val, weight in zip(preds_dict.values(), weights)], axis=0)\n","predictions_overall = np.argsort(-predictions_overall)[:,:3]\n","predictions_as_answer_letters = np.array(list('ABCDE'))[predictions_overall]\n","df_valid['prediction'] = [' '.join(pred) for pred in predictions_as_answer_letters]\n","\n","def compute_map3(preds, actuals):\n","    correct_answers = [0, 0, 0]\n","    for pred, actual in zip(preds, actuals):\n","        if pred[0] == actual:\n","            correct_answers[0] += 1\n","        if pred[1] == actual:\n","            correct_answers[1] += 1./2\n","        if pred[2] == actual:\n","            correct_answers[2] += 1./3\n","    \n","    n_total = len(actuals)\n","    map3 = np.sum(correct_answers) / n_total\n","    \n","    return map3\n","\n","if not RUN_ON_KAGGLE and 'answer' in df_valid.columns:\n","    val_df = pd.read_csv(file_for_local_cv,index_col=0).sample(n=VAL_SIZE, random_state=42).reset_index(drop=True)\n","    \n","    val_df['A'] = val_df['A'].map(str)\n","    val_df['B'] = val_df['B'].map(str)\n","    val_df['C'] = val_df['C'].map(str)\n","    val_df['D'] = val_df['D'].map(str)\n","    val_df['E'] = val_df['E'].map(str)\n","    val_df['answer'] = val_df['answer'].map(str)\n","\n","    val_df = val_df.reset_index(drop=True)\n","    \n","    \n","    weights = np.ones(len(preds_dict))\n","    predictions_overall = np.sum([preds_val*weight for preds_val, weight in zip(preds_dict.values(), weights)], axis=0)\n","    predictions_overall = np.argsort(-predictions_overall)[:,:3]\n","    predictions_as_answer_letters = np.array(list('ABCDE'))[predictions_overall]\n","\n","\n","\n","    map3_score = compute_map3(predictions_as_answer_letters, val_df['answer'].values)\n","\n","\n","    best_map3_score = map3_score\n","    print(weights)\n","    print(map3_score)\n","\n","    \n","    # best_map3_score = 0\n","    # for i in tqdm(range(10000)):\n","    #     weights = np.random.uniform(0, 1, len(preds_dict))\n","    #     predictions_overall = np.sum([preds_val*weight for preds_val in zip(preds_dict.values(), weights)], axis=0)\n","    #     predictions_overall = np.argsort(-predictions_overall)[:,:3]\n","    #     predictions_as_answer_letters = np.array(list('ABCDE'))[predictions_overall]\n","\n","\n","    \n","    #     map3_score = compute_map3(predictions_as_answer_letters, val_df['answer'].values)\n","    \n","    #     if map3_score > best_map3_score:\n","    #         best_map3_score = map3_score\n","    #         print(weights)\n","    #         print(map3_score)\n","    \n","    # print(map3_score)\n","    \n","    "]},{"cell_type":"markdown","metadata":{},"source":["Experiments:\n","\n","- all models: 0.8"]},{"cell_type":"code","execution_count":27,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 submission = df_valid[[<span style=\"color: #808000; text-decoration-color: #808000\">'id'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">'prediction'</span>]]                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>submission.to_csv(<span style=\"color: #808000; text-decoration-color: #808000\">'submission.csv'</span>, index=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>)                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 </span>pd.read_csv(<span style=\"color: #808000; text-decoration-color: #808000\">'submission.csv'</span>).head(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">10</span>)                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/viktor/miniconda3/envs/torch-env/lib/python3.9/site-packages/pandas/core/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">frame.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3767</span> in  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__getitem__</span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3764 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3765 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> is_iterator(key):                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3766 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>key = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">list</span>(key)                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 3767 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>indexer = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.columns._get_indexer_strict(key, <span style=\"color: #808000; text-decoration-color: #808000\">\"columns\"</span>)[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>]                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3768 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3769 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># take() does not accept boolean indexers</span>                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3770 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">getattr</span>(indexer, <span style=\"color: #808000; text-decoration-color: #808000\">\"dtype\"</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>) == <span style=\"color: #00ffff; text-decoration-color: #00ffff\">bool</span>:                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/viktor/miniconda3/envs/torch-env/lib/python3.9/site-packages/pandas/core/indexes/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">5</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">876</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_get_indexer_strict</span>                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5873 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5874 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>keyarr, indexer, new_indexer = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._reindex_non_unique(keyarr)               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5875 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>5876 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._raise_if_missing(keyarr, indexer, axis_name)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5877 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5878 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>keyarr = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.take(indexer)                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5879 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(key, Index):                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/viktor/miniconda3/envs/torch-env/lib/python3.9/site-packages/pandas/core/indexes/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">5</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">938</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_raise_if_missing</span>                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5935 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">KeyError</span>(<span style=\"color: #808000; text-decoration-color: #808000\">f\"None of [{</span>key<span style=\"color: #808000; text-decoration-color: #808000\">}] are in the [{</span>axis_name<span style=\"color: #808000; text-decoration-color: #808000\">}]\"</span>)               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5936 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5937 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>not_found = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">list</span>(ensure_index(key)[missing_mask.nonzero()[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>]].unique())       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>5938 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">KeyError</span>(<span style=\"color: #808000; text-decoration-color: #808000\">f\"{</span>not_found<span style=\"color: #808000; text-decoration-color: #808000\">} not in index\"</span>)                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5939 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5940 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">@overload</span>                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5941 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_get_indexer_non_comparable</span>(                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n","<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyError: </span><span style=\"color: #008000; text-decoration-color: #008000\">\"['prediction'] not in index\"</span>\n","</pre>\n"],"text/plain":["\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n","\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 submission = df_valid[[\u001b[33m'\u001b[0m\u001b[33mid\u001b[0m\u001b[33m'\u001b[0m, \u001b[33m'\u001b[0m\u001b[33mprediction\u001b[0m\u001b[33m'\u001b[0m]]                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0msubmission.to_csv(\u001b[33m'\u001b[0m\u001b[33msubmission.csv\u001b[0m\u001b[33m'\u001b[0m, index=\u001b[94mFalse\u001b[0m)                                             \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m3 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m4 \u001b[0mpd.read_csv(\u001b[33m'\u001b[0m\u001b[33msubmission.csv\u001b[0m\u001b[33m'\u001b[0m).head(\u001b[94m10\u001b[0m)                                                       \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/home/viktor/miniconda3/envs/torch-env/lib/python3.9/site-packages/pandas/core/\u001b[0m\u001b[1;33mframe.py\u001b[0m:\u001b[94m3767\u001b[0m in  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[92m__getitem__\u001b[0m                                                                                      \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 3764 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                            \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 3765 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m is_iterator(key):                                                         \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 3766 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mkey = \u001b[96mlist\u001b[0m(key)                                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 3767 \u001b[2m│   │   │   \u001b[0mindexer = \u001b[96mself\u001b[0m.columns._get_indexer_strict(key, \u001b[33m\"\u001b[0m\u001b[33mcolumns\u001b[0m\u001b[33m\"\u001b[0m)[\u001b[94m1\u001b[0m]                \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 3768 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                 \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 3769 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# take() does not accept boolean indexers\u001b[0m                                        \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 3770 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mgetattr\u001b[0m(indexer, \u001b[33m\"\u001b[0m\u001b[33mdtype\u001b[0m\u001b[33m\"\u001b[0m, \u001b[94mNone\u001b[0m) == \u001b[96mbool\u001b[0m:                                      \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/home/viktor/miniconda3/envs/torch-env/lib/python3.9/site-packages/pandas/core/indexes/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m5\u001b[0m \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[94m876\u001b[0m in \u001b[92m_get_indexer_strict\u001b[0m                                                                       \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m5873 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m5874 \u001b[0m\u001b[2m│   │   │   \u001b[0mkeyarr, indexer, new_indexer = \u001b[96mself\u001b[0m._reindex_non_unique(keyarr)               \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m5875 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m5876 \u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m._raise_if_missing(keyarr, indexer, axis_name)                                \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m5877 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m5878 \u001b[0m\u001b[2m│   │   \u001b[0mkeyarr = \u001b[96mself\u001b[0m.take(indexer)                                                       \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m5879 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(key, Index):                                                        \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/home/viktor/miniconda3/envs/torch-env/lib/python3.9/site-packages/pandas/core/indexes/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m5\u001b[0m \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[94m938\u001b[0m in \u001b[92m_raise_if_missing\u001b[0m                                                                         \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m5935 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mKeyError\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mNone of [\u001b[0m\u001b[33m{\u001b[0mkey\u001b[33m}\u001b[0m\u001b[33m] are in the [\u001b[0m\u001b[33m{\u001b[0maxis_name\u001b[33m}\u001b[0m\u001b[33m]\u001b[0m\u001b[33m\"\u001b[0m)               \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m5936 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m5937 \u001b[0m\u001b[2m│   │   │   \u001b[0mnot_found = \u001b[96mlist\u001b[0m(ensure_index(key)[missing_mask.nonzero()[\u001b[94m0\u001b[0m]].unique())       \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m5938 \u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mKeyError\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0mnot_found\u001b[33m}\u001b[0m\u001b[33m not in index\u001b[0m\u001b[33m\"\u001b[0m)                                   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m5939 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m5940 \u001b[0m\u001b[2m│   \u001b[0m\u001b[1;95m@overload\u001b[0m                                                                             \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m5941 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_get_indexer_non_comparable\u001b[0m(                                                      \u001b[31m│\u001b[0m\n","\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n","\u001b[1;91mKeyError: \u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32m'prediction'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m not in index\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"}],"source":["submission = df_valid[['id', 'prediction']]\n","submission.to_csv('submission.csv', index=False)\n","\n","pd.read_csv('submission.csv').head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":4}
