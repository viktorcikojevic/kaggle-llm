{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T20:15:53.742954Z","iopub.status.busy":"2023-10-09T20:15:53.742591Z"},"trusted":true},"outputs":[],"source":["# %%capture\n","# # installing offline dependencies\n","# !pip install -U /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n","# !cp -rf /kaggle/input/sentence-transformers-222/sentence-transformers /kaggle/working/sentence-transformers\n","# !pip install -U /kaggle/working/sentence-transformers\n","# !pip install -U /kaggle/input/blingfire-018/blingfire-0.1.8-py3-none-any.whl\n","\n","# !pip install --no-index --no-deps /kaggle/input/llm-whls/transformers-4.31.0-py3-none-any.whl\n","# !pip install --no-index --no-deps /kaggle/input/llm-whls/peft-0.4.0-py3-none-any.whl\n","# !pip install --no-index --no-deps /kaggle/input/llm-whls/datasets-2.14.3-py3-none-any.whl\n","# !pip install --no-index --no-deps /kaggle/input/llm-whls/trl-0.5.0-py3-none-any.whl\n","\n","# !pip install -U --no-deps /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n","# !pip install -U --no-deps /kaggle/input/datasets-214/datasets-2.14.5-py3-none-any.whl"]},{"cell_type":"markdown","metadata":{},"source":["# platybus"]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[],"source":["# Installing offline dependencies\n","# !pip install -U --no-deps /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n","# !pip install -U --no-deps /kaggle/input/datasets-214/datasets-2.14.5-py3-none-any.whl"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>prompt</th>\n","      <th>A</th>\n","      <th>B</th>\n","      <th>C</th>\n","      <th>D</th>\n","      <th>E</th>\n","      <th>answer</th>\n","      <th>id</th>\n","      <th>context</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>What is the method of transcription in the lif...</td>\n","      <td>RNA-templated transcription is the method of t...</td>\n","      <td>Transcription occurs through a unique mechanis...</td>\n","      <td>Reverse transcription is the method of transcr...</td>\n","      <td>DNA-templated transcription is the method of t...</td>\n","      <td>Transcription does not occur in the life cycle...</td>\n","      <td>D</td>\n","      <td>0</td>\n","      <td>The given phonetic transcription is how the pr...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>What is the role of the viral fiber glycoprote...</td>\n","      <td>The viral fiber glycoproteins are involved in ...</td>\n","      <td>The viral fiber glycoproteins code for 40 prot...</td>\n","      <td>The viral fiber glycoproteins are responsible ...</td>\n","      <td>The viral fiber glycoproteins mediate endocyto...</td>\n","      <td>The viral fiber glycoproteins are responsible ...</td>\n","      <td>D</td>\n","      <td>1</td>\n","      <td>Viral glycoprotein-receptor interactions are r...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>What is the significance of the faint Hα emiss...</td>\n","      <td>The emission lines indicate that 3 Geminorum i...</td>\n","      <td>The emission lines indicate that 3 Geminorum i...</td>\n","      <td>The emission lines indicate that 3 Geminorum i...</td>\n","      <td>The emission lines indicate that 3 Geminorum i...</td>\n","      <td>The emission lines indicate that 3 Geminorum i...</td>\n","      <td>A</td>\n","      <td>2</td>\n","      <td>OU Geminorum (OU Gem) is a visual binary or po...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>What is the significance of the pedicellariae ...</td>\n","      <td>They are used for climbing on corals.</td>\n","      <td>They resemble the traps of the Venus fly trap ...</td>\n","      <td>They are covered by short and stout spines.</td>\n","      <td>They are found on the central disc of the sea ...</td>\n","      <td>They are a characteristic feature of the Gonia...</td>\n","      <td>B</td>\n","      <td>3</td>\n","      <td>Another question arises: Neural wiring minimiz...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>What is the role of the microprocessor complex...</td>\n","      <td>The microprocessor complex is responsible for ...</td>\n","      <td>The microprocessor complex is responsible for ...</td>\n","      <td>The microprocessor complex is involved in the ...</td>\n","      <td>The microprocessor complex is involved in the ...</td>\n","      <td>The microprocessor complex is responsible for ...</td>\n","      <td>A</td>\n","      <td>4</td>\n","      <td>The involvement of miRNAs in diseases has led ...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>495</th>\n","      <td>195</td>\n","      <td>What is the relation between the three moment ...</td>\n","      <td>The three moment theorem expresses the relatio...</td>\n","      <td>The three moment theorem is used to calculate ...</td>\n","      <td>The three moment theorem describes the relatio...</td>\n","      <td>The three moment theorem is used to calculate ...</td>\n","      <td>The three moment theorem is used to derive the...</td>\n","      <td>C</td>\n","      <td>495</td>\n","      <td>The change in slope of a deflection curve betw...</td>\n","    </tr>\n","    <tr>\n","      <th>496</th>\n","      <td>196</td>\n","      <td>What is the throttling process, and why is it ...</td>\n","      <td>The throttling process is a steady flow of a f...</td>\n","      <td>The throttling process is a steady adiabatic f...</td>\n","      <td>The throttling process is a steady adiabatic f...</td>\n","      <td>The throttling process is a steady flow of a f...</td>\n","      <td>The throttling process is a steady adiabatic f...</td>\n","      <td>B</td>\n","      <td>496</td>\n","      <td>The throttling process is a good example of an...</td>\n","    </tr>\n","    <tr>\n","      <th>497</th>\n","      <td>197</td>\n","      <td>What happens to excess base metal as a solutio...</td>\n","      <td>The excess base metal will often solidify, bec...</td>\n","      <td>The excess base metal will often crystallize-o...</td>\n","      <td>The excess base metal will often dissolve, bec...</td>\n","      <td>The excess base metal will often liquefy, beco...</td>\n","      <td>The excess base metal will often evaporate, be...</td>\n","      <td>B</td>\n","      <td>497</td>\n","      <td>To overcome this reversibility, the reaction o...</td>\n","    </tr>\n","    <tr>\n","      <th>498</th>\n","      <td>198</td>\n","      <td>What is the relationship between mass, force, ...</td>\n","      <td>Mass is a property that determines the weight ...</td>\n","      <td>Mass is an inertial property that determines a...</td>\n","      <td>Mass is an inertial property that determines a...</td>\n","      <td>Mass is an inertial property that determines a...</td>\n","      <td>Mass is a property that determines the size of...</td>\n","      <td>D</td>\n","      <td>498</td>\n","      <td>The newton (symbol: N) is the unit of force in...</td>\n","    </tr>\n","    <tr>\n","      <th>499</th>\n","      <td>199</td>\n","      <td>What did Arthur Eddington discover about two o...</td>\n","      <td>Arthur Eddington showed that two of Einstein's...</td>\n","      <td>Arthur Eddington showed that two of Einstein's...</td>\n","      <td>Arthur Eddington showed that two of Einstein's...</td>\n","      <td>Arthur Eddington showed that two of Einstein's...</td>\n","      <td>Arthur Eddington showed that two of Einstein's...</td>\n","      <td>C</td>\n","      <td>499</td>\n","      <td>Also, Sir Arthur Eddington had discussed notio...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>500 rows × 10 columns</p>\n","</div>"],"text/plain":["     Unnamed: 0                                             prompt   \n","0             0  What is the method of transcription in the lif...  \\\n","1             1  What is the role of the viral fiber glycoprote...   \n","2             2  What is the significance of the faint Hα emiss...   \n","3             3  What is the significance of the pedicellariae ...   \n","4             4  What is the role of the microprocessor complex...   \n","..          ...                                                ...   \n","495         195  What is the relation between the three moment ...   \n","496         196  What is the throttling process, and why is it ...   \n","497         197  What happens to excess base metal as a solutio...   \n","498         198  What is the relationship between mass, force, ...   \n","499         199  What did Arthur Eddington discover about two o...   \n","\n","                                                     A   \n","0    RNA-templated transcription is the method of t...  \\\n","1    The viral fiber glycoproteins are involved in ...   \n","2    The emission lines indicate that 3 Geminorum i...   \n","3                They are used for climbing on corals.   \n","4    The microprocessor complex is responsible for ...   \n","..                                                 ...   \n","495  The three moment theorem expresses the relatio...   \n","496  The throttling process is a steady flow of a f...   \n","497  The excess base metal will often solidify, bec...   \n","498  Mass is a property that determines the weight ...   \n","499  Arthur Eddington showed that two of Einstein's...   \n","\n","                                                     B   \n","0    Transcription occurs through a unique mechanis...  \\\n","1    The viral fiber glycoproteins code for 40 prot...   \n","2    The emission lines indicate that 3 Geminorum i...   \n","3    They resemble the traps of the Venus fly trap ...   \n","4    The microprocessor complex is responsible for ...   \n","..                                                 ...   \n","495  The three moment theorem is used to calculate ...   \n","496  The throttling process is a steady adiabatic f...   \n","497  The excess base metal will often crystallize-o...   \n","498  Mass is an inertial property that determines a...   \n","499  Arthur Eddington showed that two of Einstein's...   \n","\n","                                                     C   \n","0    Reverse transcription is the method of transcr...  \\\n","1    The viral fiber glycoproteins are responsible ...   \n","2    The emission lines indicate that 3 Geminorum i...   \n","3          They are covered by short and stout spines.   \n","4    The microprocessor complex is involved in the ...   \n","..                                                 ...   \n","495  The three moment theorem describes the relatio...   \n","496  The throttling process is a steady adiabatic f...   \n","497  The excess base metal will often dissolve, bec...   \n","498  Mass is an inertial property that determines a...   \n","499  Arthur Eddington showed that two of Einstein's...   \n","\n","                                                     D   \n","0    DNA-templated transcription is the method of t...  \\\n","1    The viral fiber glycoproteins mediate endocyto...   \n","2    The emission lines indicate that 3 Geminorum i...   \n","3    They are found on the central disc of the sea ...   \n","4    The microprocessor complex is involved in the ...   \n","..                                                 ...   \n","495  The three moment theorem is used to calculate ...   \n","496  The throttling process is a steady flow of a f...   \n","497  The excess base metal will often liquefy, beco...   \n","498  Mass is an inertial property that determines a...   \n","499  Arthur Eddington showed that two of Einstein's...   \n","\n","                                                     E answer   id   \n","0    Transcription does not occur in the life cycle...      D    0  \\\n","1    The viral fiber glycoproteins are responsible ...      D    1   \n","2    The emission lines indicate that 3 Geminorum i...      A    2   \n","3    They are a characteristic feature of the Gonia...      B    3   \n","4    The microprocessor complex is responsible for ...      A    4   \n","..                                                 ...    ...  ...   \n","495  The three moment theorem is used to derive the...      C  495   \n","496  The throttling process is a steady adiabatic f...      B  496   \n","497  The excess base metal will often evaporate, be...      B  497   \n","498  Mass is a property that determines the size of...      D  498   \n","499  Arthur Eddington showed that two of Einstein's...      C  499   \n","\n","                                               context  \n","0    The given phonetic transcription is how the pr...  \n","1    Viral glycoprotein-receptor interactions are r...  \n","2    OU Geminorum (OU Gem) is a visual binary or po...  \n","3    Another question arises: Neural wiring minimiz...  \n","4    The involvement of miRNAs in diseases has led ...  \n","..                                                 ...  \n","495  The change in slope of a deflection curve betw...  \n","496  The throttling process is a good example of an...  \n","497  To overcome this reversibility, the reaction o...  \n","498  The newton (symbol: N) is the unit of force in...  \n","499  Also, Sir Arthur Eddington had discussed notio...  \n","\n","[500 rows x 10 columns]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","\n","df = pd.read_csv(\"/home/viktor/Documents/kaggle/kaggle_llm/work_dirs/160k-viktor-and-deotte-dataset-deotte-preproc-deberta-window-inference/test_with_context.csv\")\n","df"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting platybus.py\n"]}],"source":["%%writefile platybus.py\n","\n","#!/usr/bin/env python\n","# coding: utf-8\n","\n","# # Platypus2-70B + Wikipedia RAG\n","# \n","\n","\n","\n","\n","import gc\n","import logging\n","from time import time\n","from pathlib import Path\n","from concurrent.futures import ThreadPoolExecutor\n","import ctypes\n","from functools import partial\n","from scipy.special import kl_div\n","\n","\n","import torch\n","import numpy as np\n","import pandas as pd\n","from tqdm.auto import tqdm\n","\n","# For RAG\n","import faiss\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from datasets import load_from_disk, Dataset\n","\n","NUM_TITLES = 5\n","MAX_SEQ_LEN = 512\n","# MODEL_PATH = \"/kaggle/input/bge-small-faiss/\"\n","MODEL_PATH = \"/home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/bge-small-faiss\"\n","\n","# For LLM\n","from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, AutoModel\n","from accelerate import init_empty_weights\n","from accelerate.utils.modeling import set_module_tensor_to_device\n","from safetensors.torch import load_file\n","\n","N_BATCHES = 1 \n","MAX_CONTEXT = 2300\n","MAX_LENGTH = 4096\n","\n","\n","\n","\n","# Function to clean RAM & vRAM\n","def clean_memory():\n","    gc.collect()\n","    ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n","    torch.cuda.empty_cache()\n","\n","# Load data\n","# df = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\", index_col=\"id\")\n","# df = pd.read_csv(\"/home/viktor/Documents/kaggle/kaggle_llm/work_dirs/160k-viktor-and-deotte-dataset-deotte-preproc-deberta-window-inference/test_with_context.csv\", index_col=\"id\")[:30]\n","df = pd.read_csv(\"/home/viktor/Documents/kaggle/kaggle_llm/notebooks/generate-v5-dataset/train_with_context.csv\")[:20]\n","\n","# Variable used to avoid running the notebook for 3 hours when submitting. Credit : CPMP\n","IS_TEST_SET = len(df) != 200\n","\n","# Uncomment this to see results on the train set\n","# df = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/train.csv\", index_col=\"id\")\n","# IS_TEST_SET = True\n","# N_BATCHES = 1\n","\n","\n","# ## 1. Wikipedia Retrieval Augmented Generation (RAG)\n","# \n","# The following code is adapted from [the notebook of @MGöksu](https://www.kaggle.com/code/mgoksu/0-807-sharing-my-trained-with-context-model) and [the notebook of @MB](https://www.kaggle.com/code/mbanaei/86-2-with-only-270k-articles/notebook). We use the [bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) to embed the Wikipedia dataset.\n","\n","\n","\n","# New SentenceTransformer class similar to the one used in @Mgöksu notebook but relying on the transformers library only\n","\n","class SentenceTransformer:\n","    def __init__(self, checkpoint, device=\"cuda:0\"):\n","        self.device = device\n","        self.checkpoint = checkpoint\n","        self.model = AutoModel.from_pretrained(checkpoint).to(self.device).half()\n","        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","\n","    def transform(self, batch):\n","        tokens = self.tokenizer(batch[\"text\"], truncation=True, padding=True, return_tensors=\"pt\", max_length=MAX_SEQ_LEN)\n","        return tokens.to(self.device)  \n","\n","    def get_dataloader(self, sentences, batch_size=32):\n","        sentences = [\"Represent this sentence for searching relevant passages: \" + x for x in sentences]\n","        dataset = Dataset.from_dict({\"text\": sentences})\n","        dataset.set_transform(self.transform)\n","        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n","        return dataloader\n","\n","    def encode(self, sentences, show_progress_bar=False, batch_size=32):\n","        dataloader = self.get_dataloader(sentences, batch_size=batch_size)\n","        pbar = tqdm(dataloader) if show_progress_bar else dataloader\n","\n","        embeddings = []\n","        for batch in pbar:\n","            with torch.no_grad():\n","                e = self.model(**batch).pooler_output\n","                e = F.normalize(e, p=2, dim=1)\n","                embeddings.append(e.detach().cpu().numpy())\n","        embeddings = np.concatenate(embeddings, axis=0)\n","        return embeddings\n","\n","\n","\n","\n","if IS_TEST_SET:\n","    # Load embedding model\n","    start = time()\n","    print(f\"Starting prompt embedding, t={time() - start :.1f}s\")\n","    model = SentenceTransformer(MODEL_PATH, device=\"cuda:0\")\n","\n","    # Get embeddings of prompts\n","    f = lambda row : \" \".join([row[\"prompt\"], row[\"A\"], row[\"B\"], row[\"C\"], row[\"D\"], row[\"E\"]])\n","    inputs = df.apply(f, axis=1).values # better results than prompt only\n","    prompt_embeddings = model.encode(inputs, show_progress_bar=False)\n","\n","    # Search closest sentences in the wikipedia index \n","    print(f\"Loading faiss index, t={time() - start :.1f}s\")\n","    faiss_index = faiss.read_index(MODEL_PATH + '/faiss.index')\n","    # faiss_index = faiss.index_cpu_to_all_gpus(faiss_index) # causes OOM, and not that long on CPU\n","\n","    print(f\"Starting text search, t={time() - start :.1f}s\")\n","    search_index = faiss_index.search(np.float32(prompt_embeddings), NUM_TITLES)[1]\n","\n","    print(f\"Starting context extraction, t={time() - start :.1f}s\")\n","    # dataset = load_from_disk(\"/kaggle/input/all-paraphs-parsed-expanded\")\n","    dataset = load_from_disk(\"/home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/all-paraphs-parsed-expanded\")\n","    for i in range(len(df)):\n","        df.loc[i, \"context\"] = \"-\" + \"\\n-\".join([dataset[int(j)][\"text\"] for j in search_index[i]])\n","\n","    # Free memory\n","    faiss_index.reset()\n","    del faiss_index, prompt_embeddings, model, dataset\n","    clean_memory()\n","    print(f\"Context added, t={time() - start :.1f}s\")\n","\n","\n","# ## 2: Run Platypus2-70B\n","# \n","# To run such a large model on a single T4 GPU, we run it layer by layer and sample by sample\n","\n","\n","\n","# Create symlinks from kaggle datasets to fake cached model\n","\n","checkpoint_path = Path(\"./cache\")\n","checkpoint_path.mkdir(exist_ok=True, parents=True)\n","\n","for part in [1]:\n","    # source_dir = Path(f\"/kaggle/input/platypus2-70b-instruct-part{part}\")\n","    source_dir = Path(f\"/home/viktor/Documents/kaggle/kaggle_llm/data/kaggle-datasets/platybus\")\n","    for path in source_dir.glob(\"*\"):\n","        try:\n","            (checkpoint_path / path.name).symlink_to(path)\n","        except:\n","            pass\n","\n","\n","\n","\n","# Class for sharded llama\n","\n","class ShardedLlama:\n","    def __init__(self, checkpoint_path, device=\"cuda:0\", dtype=torch.float16):\n","        \"\"\"\n","        Sharded version of LlamaForCausalLM : the model is splitted into layer shards to reduce GPU memory usage.\n","        During the forward pass, the inputs are processed layer by layer, and the GPU memory is freed after each layer.\n","        To avoid loading the layers multiple times, we could save all the intermediate activations in RAM, but\n","        as Kaggle accelerators have more GPU memory than CPU, we simply batch the inputs and keep them on the GPU.\n","\n","        Parameters\n","        ----------\n","        checkpoint_path : str or Path\n","            path to the checkpoint\n","        device : str, optional\n","            device, by default \"cuda:0\"\n","        dtype : torch.dtype, optional\n","            dtype, by default torch.float16\n","        \"\"\"\n","        \n","        # Save parameters\n","        self.checkpoint_path = Path(checkpoint_path)\n","        self.device = device \n","        self.dtype = dtype\n","\n","        # Create model\n","        self.config = AutoConfig.from_pretrained(self.checkpoint_path)\n","        \n","        # For flash attention when Turing architecture will be supported : https://github.com/Dao-AILab/flash-attention/issues/542\n","        # self.config.auto_map = {\"AutoModelForCausalLM\" : \"togethercomputer/LLaMA-2-7B-32K--modeling_flash_llama.LlamaForCausalLM\"} \n","        \n","        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n","        self.tokenizer.pad_token = self.tokenizer.eos_token\n","        self.tokenizer.padding_side = \"right\"\n","        self.init_model()\n","        # self.layer_names = [\"model.embed_tokens\"] + [f\"model.layers.{i}\" for i in range(len(self.model.model.layers))] + [\"model.norm\", \"lm_head\"]\n","        self.layer_names = [\"model.embed_tokens\"] + [f\"model.layers.{i}\" for i in range(len(self.model.model.layers))] + [\"model.norm\", \"lm_head\"]\n","\n","    def init_model(self):\n","    \n","        # Load meta model (no memory used)\n","        with init_empty_weights():\n","            self.model = AutoModelForCausalLM.from_config(self.config, trust_remote_code=True)\n","            self.model.tie_weights()\n","            \n","        # self.layers = [self.model.model.embed_tokens] + list(self.model.model.layers) + [self.model.model.norm, self.model.lm_head]\n","        self.layers = [self.model.model.embed_tokens] + list(self.model.model.layers) + [self.model.model.norm, self.model.lm_head]\n","            \n","        # Move buffers to device (not that much GPU memory used)\n","        for buffer_name, buffer in self.model.named_buffers():\n","            set_module_tensor_to_device(self.model, buffer_name, self.device, value=buffer, dtype=self.dtype)\n","\n","    def load_layer(self, layer_name):\n","        state_dict = load_file(self.checkpoint_path / (layer_name + \".safetensors\"), device=self.device)\n","        for param_name, param in state_dict.items():\n","            assert param.dtype != torch.int8, \"int8 not supported (need to add fp16_statistics)\"\n","            set_module_tensor_to_device(self.model, param_name, self.device, value=param, dtype=self.dtype)\n","\n","    def jensen_shannon_divergence(self, p, q):\n","        m = 0.5 * (p + q)\n","        return 0.5 * (kl_div(p, m) + kl_div(q, m)).sum()\n","    \n","    def contrast_distributions(self, mature_logits, premature_logits, alpha=0.1):\n","        # Compute the log probabilities\n","        log_q_N = mature_logits - np.log(np.sum(np.exp(mature_logits), axis=-1, keepdims=True))\n","        log_q_M = premature_logits - np.log(np.sum(np.exp(premature_logits), axis=-1, keepdims=True))\n","\n","        # Compute the set V_head based on the adaptive plausibility constraint\n","        threshold = alpha * np.max(np.exp(log_q_N), axis=-1, keepdims=True)\n","        V_head = np.where(np.exp(log_q_N) >= threshold, 1, 0)\n","\n","        # Compute the contrasted distribution\n","        contrasted_logits = np.where(V_head == 1, log_q_N - log_q_M, -np.inf)\n","\n","        return contrasted_logits\n","    \n","    def get_final_probs(self, contrasted_logits):\n","        return np.exp(contrasted_logits) / np.sum(np.exp(contrasted_logits), axis=-1, keepdims=True)\n","\n","    \n","            \n","    def __call__(self, inputs, output_token):\n","        # inputs = [(prefix, suffix), ...] with prefix.shape[0] = 1 and suffix.shape[0] = 5\n","        \n","        # Reboot the model to make sure buffers are loaded and memory is clean\n","        del self.model\n","        clean_memory()\n","        self.init_model()\n","        \n","       # Send batch to device\n","        batch = [(prefix.to(self.device), suffix.to(self.device)) for prefix, suffix in inputs]\n","        n_suffixes = len(batch[0][1])\n","        suffix_eos = [(suffix != self.tokenizer.pad_token_id).sum(1) - 1 for _, suffix in inputs]\n","\n","        # Create attention mask for the largest input, and position ids to use KV cache\n","        attention_mask = torch.finfo(self.dtype).min * torch.ones(MAX_LENGTH, MAX_LENGTH)\n","        attention_mask = attention_mask.triu(diagonal=1)[None, None, ...]\n","        attention_mask = attention_mask.to(self.device)\n","        position_ids = torch.arange(MAX_LENGTH, dtype=torch.long, device=self.device)[None, :]\n","\n","        # get device num\n","        device_num = int(self.device.split(\":\")[-1])\n","\n","        \n","        # allowed_exit_ints = [10, 20, 30, 40, 50, 60, 70, 79]\n","        allowed_exit_ints = [55, 60, 65, 70, 75, 79]\n","        # tune_layers = []\n","        \n","        with ThreadPoolExecutor() as executor, torch.inference_mode():\n","\n","            # Load first layer\n","            #future = executor.submit(self.load_layer, \"model.embed_tokens\")\n","            # self.load_layer(\"model.embed_tokens\")\n","\n","            for i, (layer_name, layer) in tqdm(enumerate(zip(self.layer_names, self.layers)), desc=self.device, total=len(self.layers)):\n","                \n","                # print(f\"Turns out {layer_name} is index {i}\")\n","\n","                # Wait for previous layer to be loaded and load next layer\n","                #future.result()\n","                # if (i + 1) < len(self.layer_names):\n","                    #future = executor.submit(self.load_layer, self.layer_names[i + 1])\n","                    # self.load_layer(self.layer_names[i + 1])\n","                    \n","                self.load_layer(self.layer_names[i])\n","\n","                # Run layer\n","                for j, (prefix, suffix) in enumerate(batch):\n","                    \n","                    \n","                    if \"model.layers.\" in layer_name:\n","                        # Process with current transformer layer\n","                        len_p, len_s = prefix.shape[1], suffix.shape[1]\n","                        new_prefix, (k_cache, v_cache) = layer(prefix, use_cache=True, attention_mask=attention_mask[:, :, -len_p:, -len_p:])\n","                        pos = position_ids[:, len_p:len_p + len_s].repeat(n_suffixes, 1)\n","                        attn = attention_mask[:, :, -len_s:, -len_p - len_s:].repeat(n_suffixes, 1, 1, 1)\n","                        kv_cache = (k_cache.repeat(n_suffixes, 1, 1, 1), v_cache.repeat(n_suffixes, 1, 1, 1))\n","                        new_suffix = layer(suffix, past_key_value=kv_cache, position_ids=pos, attention_mask=attn)[0]\n","                        batch[j] = (new_prefix, new_suffix)\n","                        \n","                        \n","                        layer_exit_int = int(layer_name.split(\".\")[-1]) # model.layers.0 -> 0\n","                        if layer_exit_int in allowed_exit_ints:\n","                             \n","\n","                            # Load and apply model.norm\n","                            indx = 0\n","                            for indx_, name in enumerate(self.layer_names):\n","                                if name == 'model.norm':\n","                                    indx = indx_\n","                            layer = self.layers[indx]\n","                            clean_memory()\n","                            self.load_layer(\"model.norm\")\n","                            # norm_suffix = layer(new_suffix)\n","                            norm_suffix_2 = layer(new_suffix[torch.arange(n_suffixes), suffix_eos[j]][:, None])\n","\n","                            # Free memory\n","                            # layer.to(\"meta\")\n","                            clean_memory()\n","\n","                            # Load and apply lm_head\n","                            # get index of lm_head\n","                            indx = 0\n","                            for indx_, name in enumerate(self.layer_names):\n","                                if name == 'lm_head':\n","                                    indx = indx_\n","                            layer = self.layers[indx]\n","                            clean_memory()\n","                            self.load_layer(\"lm_head\")\n","                            logits = layer(norm_suffix_2)[:, 0, output_token].detach().cpu().numpy()\n","                            \n","                            # These logits are huge. Let's \"normalize\" them\n","                            # logits = logits / np.sum(logits) * 30\n","                            def sigmoid(x):\n","                                return 1 / (1 + np.exp(-x))\n","                            \n","                            logits = sigmoid(logits)\n","                            \n","                            # print(f\"saving early exit for layer: {layer_name}.\")\n","                            \n","                            # Save\n","                            np.save(f\"early_exit_logits_{layer_exit_int}_batch_{j}_device_{device_num}.npy\", logits)\n","\n","                            # Free memory\n","                            # layer.to(\"meta\")\n","                            clean_memory()\n","\n","                            # Reload the transformer layer for further processing\n","                            # get index of a current layer_name\n","                            layer = self.layers[i]\n","                            clean_memory()\n","                            self.load_layer(self.layer_names[i])\n","                            layer.to(self.device)\n","\n","                            \n","                        else:\n","                            # print(f\"{layer_exit_int} not in {allowed_exit_ints}, skipping exit ints...\")\n","                            pass\n","                            \n","                        \n","                    else:\n","                        if layer_name == \"model.embed_tokens\":\n","                            batch[j] = (layer(prefix), layer(suffix))\n","                        elif layer_name == \"model.norm\":\n","                            # Only keep the last token at this point\n","                            batch[j] = (None, layer(suffix[torch.arange(n_suffixes), suffix_eos[j]][:, None]))\n","                        elif layer_name == \"lm_head\":\n","                            # batch[j] = layer(suffix)[:, 0, output_token].detach().cpu().numpy()\n","                            batch[j] = self.model.lm_head(suffix)[:, 0, output_token].detach().cpu().numpy()\n","                            \n","                            # To fine-tune\n","                            suffix_np = suffix.detach().cpu().numpy()\n","                            randint = np.random.randint(0, 1000000)\n","                            np.save(f\"suffix_np_{randint}\", suffix_np)\n","                            \n","                            \n","                        \n","                # clean current layer\n","                layer.to(\"meta\")\n","                clean_memory() # proposed by CPMP\n","                \n","        # return np.vstack(batch)\n","        \n","        # Now apply dola to the saved exit layers. Return array like this [array([12.36 , 12.016, 11.71 , 12.17 , 12.02 ], dtype=float16)]\n","        \n","        # Load the logits for the mature layer\n","        largest_exit_int = max(allowed_exit_ints)\n","        \n","        final_probs = []\n","        for j, b in enumerate(batch):\n","            mature_logits = np.load(f\"early_exit_logits_{largest_exit_int}_batch_{j}_device_{device_num}.npy\")\n","                \n","            mature_probs = np.exp(mature_logits) / np.sum(np.exp(mature_logits), axis=-1, keepdims=True)\n","\n","            max_jsd = -np.inf\n","            premature_logits = None\n","            premature_probs = None\n","            premature_layer = None\n","\n","            for exit_int in allowed_exit_ints[:-1]: \n","                current_logits = np.load(f\"early_exit_logits_{exit_int}_batch_{j}_device_{device_num}.npy\")\n","                \n","                current_probs = np.exp(current_logits) / np.sum(np.exp(current_logits), axis=-1, keepdims=True)\n","                jsd = self.jensen_shannon_divergence(mature_probs, current_probs)\n","\n","                if jsd > max_jsd:\n","                    max_jsd = jsd\n","                    premature_logits = current_logits\n","                    premature_probs = current_probs\n","                    premature_layer = exit_int\n","\n","            print(f\"mature_logits: {mature_logits}, premature_logits: {premature_logits}\")\n","\n","            contrasted_logits = self.contrast_distributions(mature_logits, premature_logits)\n","            probs = self.get_final_probs(contrasted_logits)\n","            final_probs.append(probs)\n","            \n","        final_probs = np.vstack(final_probs)\n","        print(f\"final_probs from call method: {final_probs}\")\n","        return final_probs\n","    \n","        \n","        # Get scores\n","        # return batch\n","\n","\n","# Run model on the 2 GPUs\n","\n","def get_tokens(row, tokenizer):\n","        system_prefix = \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_prefix}\"\n","        instruction = \"Your task is to analyze the question and answer below. If the answer is correct, respond yes, if it is not correct respond no. As a potential aid to your answer, background context from Wikipedia articles is at your disposal, even if they might not always be relevant.\"\n","        input_prefix = f\"Context: {row['context'][:MAX_CONTEXT]}\\nQuestion: {row['prompt']}\\nProposed answer: \"\n","        prompt_prefix = system_prefix.format(instruction=instruction, input_prefix=input_prefix)\n","        prefix = tokenizer(prompt_prefix, return_tensors=\"pt\", return_attention_mask=False, truncation=True, max_length=MAX_LENGTH)[\"input_ids\"]\n","        prompt_suffix = [f\"{row[letter]}\\n\\n### Response:\\n\" for letter in \"ABCDE\"]\n","        suffix = tokenizer(prompt_suffix, return_tensors=\"pt\", return_attention_mask=False, truncation=True, max_length=MAX_LENGTH, padding=True)[\"input_ids\"][:, 1:]\n","        return prefix, suffix\n","\n","def run_model(device, df):\n","    model = ShardedLlama(checkpoint_path, device=f\"cuda:{device}\")\n","    f = partial(get_tokens, tokenizer=model.tokenizer)\n","    inputs = df.apply(f, axis=1).values\n","    # print(inputs[:2])\n","    batches = np.array_split(inputs, N_BATCHES)\n","    outputs = []\n","    for i, batch in enumerate(batches):\n","        # Token #4874 is yes.\n","        results = model(batch, output_token=4874)\n","        # print(f\"results for batch={batch} -> {results}\")\n","        if results.shape[0] == 0:\n","            print(f\"Warning: Received empty outputs for batch {i}. batch={batch}\")\n","        else:\n","            outputs.append(results)\n","        # outputs += model(batch, output_token=4874) # model returns numpy array of size 5, like this: array([0.2001, 0.2001, 0.2001, 0.2001, 0.2001], dtype=float16)\n","    return outputs\n","\n","# Run model\n","if IS_TEST_SET: \n","    \n","    # take only first 1000 samples, boost only part of the test\n","    n_limit = 0\n","    n_zeros = len(df) - n_limit\n","    df = df[:n_zeros]\n","    \n","    \n","    with ThreadPoolExecutor() as executor:\n","        outputs = run_model(0, df)\n","        # outputs = list(executor.map(run_model, [0, 1], np.array_split(df, 2))) \n","        # outputs = sum(outputs, [])\n","    \n","    print(f\"final outputs: {outputs}\")\n","    \n","    outputs = np.vstack(outputs).astype(np.float32)\n","    \n","    # save to disk\n","    np.save(\"platybus_scores.npy\", outputs)\n","    \n","    # load from disk\n","    # outputs = np.load(\"platybus_scores.npy\")\n","    "]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting prompt embedding, t=0.0s\n","Loading faiss index, t=2.4s\n","Starting text search, t=4.4s\n","Starting context extraction, t=4.6s\n","Context added, t=5.6s\n","cuda:0:  35%|████████████▏                      | 29/83 [00:33<01:03,  1.18s/it]^C\n","cuda:0:  35%|████████████▏                      | 29/83 [00:34<01:04,  1.19s/it]\n","\u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/home/viktor/Documents/kaggle/kaggle_llm/work_dirs/platypus-fine-tune/\u001b[0m\u001b[1;33mplatyb\u001b[0m \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[1;33mus.py\u001b[0m:\u001b[94m473\u001b[0m in \u001b[92m<module>\u001b[0m                                                        \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m470 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m471 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m472 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mwith\u001b[0m ThreadPoolExecutor() \u001b[94mas\u001b[0m executor:                             \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m473 \u001b[2m│   │   \u001b[0moutputs = run_model(\u001b[94m0\u001b[0m, df)                                     \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m474 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# outputs = list(executor.map(run_model, [0, 1], np.array_spli\u001b[0m \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m475 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# outputs = sum(outputs, [])\u001b[0m                                   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m476 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/home/viktor/Documents/kaggle/kaggle_llm/work_dirs/platypus-fine-tune/\u001b[0m\u001b[1;33mplatyb\u001b[0m \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[1;33mus.py\u001b[0m:\u001b[94m454\u001b[0m in \u001b[92mrun_model\u001b[0m                                                       \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m451 \u001b[0m\u001b[2m│   \u001b[0moutputs = []                                                       \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m452 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mfor\u001b[0m i, batch \u001b[95min\u001b[0m \u001b[96menumerate\u001b[0m(batches):                                \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m453 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Token #4874 is yes.\u001b[0m                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m454 \u001b[2m│   │   \u001b[0mresults = model(batch, output_token=\u001b[94m4874\u001b[0m)                      \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m455 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# print(f\"results for batch={batch} -> {results}\")\u001b[0m             \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m456 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m results.shape[\u001b[94m0\u001b[0m] == \u001b[94m0\u001b[0m:                                      \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m457 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mWarning: Received empty outputs for batch \u001b[0m\u001b[33m{\u001b[0mi\u001b[33m}\u001b[0m\u001b[33m. bat\u001b[0m \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/home/viktor/Documents/kaggle/kaggle_llm/work_dirs/platypus-fine-tune/\u001b[0m\u001b[1;33mplatyb\u001b[0m \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[1;33mus.py\u001b[0m:\u001b[94m386\u001b[0m in \u001b[92m__call__\u001b[0m                                                        \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m383 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m                                                       \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m384 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[2m# clean current layer\u001b[0m                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m385 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mlayer.to(\u001b[33m\"\u001b[0m\u001b[33mmeta\u001b[0m\u001b[33m\"\u001b[0m)                                       \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m386 \u001b[2m│   │   │   │   \u001b[0mclean_memory() \u001b[2m# proposed by CPMP\u001b[0m                      \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m387 \u001b[0m\u001b[2m│   │   \u001b[0m                                                               \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m388 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# return np.vstack(batch)\u001b[0m                                      \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m389 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/home/viktor/Documents/kaggle/kaggle_llm/work_dirs/platypus-fine-tune/\u001b[0m\u001b[1;33mplatyb\u001b[0m \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[1;33mus.py\u001b[0m:\u001b[94m54\u001b[0m in \u001b[92mclean_memory\u001b[0m                                                     \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 51 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mclean_memory\u001b[0m():                                                    \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 52 \u001b[0m\u001b[2m│   \u001b[0mgc.collect()                                                       \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 53 \u001b[0m\u001b[2m│   \u001b[0mctypes.CDLL(\u001b[33m\"\u001b[0m\u001b[33mlibc.so.6\u001b[0m\u001b[33m\"\u001b[0m).malloc_trim(\u001b[94m0\u001b[0m)                            \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 54 \u001b[2m│   \u001b[0mtorch.cuda.empty_cache()                                           \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 55 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 56 \u001b[0m\u001b[2m# Load data\u001b[0m                                                            \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 57 \u001b[0m\u001b[2m# df = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\", i\u001b[0m \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/home/viktor/miniconda3/envs/torch-env/lib/python3.9/site-packages/torch/cud\u001b[0m \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33ma/\u001b[0m\u001b[1;33mmemory.py\u001b[0m:\u001b[94m133\u001b[0m in \u001b[92mempty_cache\u001b[0m                                               \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m130 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33mmore details about GPU memory management.\u001b[0m                      \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m131 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m132 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m is_initialized():                                               \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m133 \u001b[2m│   │   \u001b[0mtorch._C._cuda_emptyCache()                                    \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m134 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m135 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m136 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mmemory_stats\u001b[0m(device: Union[Device, \u001b[96mint\u001b[0m] = \u001b[94mNone\u001b[0m) -> Dict[\u001b[96mstr\u001b[0m, Any]: \u001b[31m│\u001b[0m\n","\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n","\u001b[1;91mKeyboardInterrupt\u001b[0m\n"]}],"source":["!python platybus.py"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":4}
