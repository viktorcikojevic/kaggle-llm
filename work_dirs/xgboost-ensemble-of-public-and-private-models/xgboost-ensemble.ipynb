{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>context</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Which of the following is the most direct caus...</td>\n",
       "      <td>Within volvocine algae three genes have been i...</td>\n",
       "      <td>RNA transcription</td>\n",
       "      <td>Supercoiling of chromatin</td>\n",
       "      <td>Chromosome replication without cell division</td>\n",
       "      <td>Chromosome recombination</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>All of the following statements about muscle c...</td>\n",
       "      <td>According to the sliding filament theory, the ...</td>\n",
       "      <td>The ends of actin filaments move closer together.</td>\n",
       "      <td>The length of myosin filaments does not change.</td>\n",
       "      <td>Calcium-troponin binding precedes actin-myosin...</td>\n",
       "      <td>Calcium-tropomyosin binding precedes actin-myo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Short-term changes in plant growth rate mediat...</td>\n",
       "      <td>In a living plant, auxins and other plant horm...</td>\n",
       "      <td>loss of turgor pressure in the affected cells</td>\n",
       "      <td>increased extensibility of the walls of affect...</td>\n",
       "      <td>suppression of metabolic activity in affected ...</td>\n",
       "      <td>cytoskeletal rearrangements in the affected cells</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RNA is thought to have played an important rol...</td>\n",
       "      <td>The role of RNA in the origin of life is best ...</td>\n",
       "      <td>I only</td>\n",
       "      <td>II only</td>\n",
       "      <td>III only</td>\n",
       "      <td>II and III</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>An organism with a lobed thallus, rhizoids, an...</td>\n",
       "      <td>Rhizophysidae is a family of siphonophores in ...</td>\n",
       "      <td>moss</td>\n",
       "      <td>liverwort</td>\n",
       "      <td>fern</td>\n",
       "      <td>mushroom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>For a long time commercial media have been us...</td>\n",
       "      <td>There are multiple techniques for achieving an...</td>\n",
       "      <td>Direct response.</td>\n",
       "      <td>Behavioural response.</td>\n",
       "      <td>Attitudinal response.</td>\n",
       "      <td>Call-to-action response .</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>The _________ model emphasizes that each phase...</td>\n",
       "      <td>This term is used merely to describe the struc...</td>\n",
       "      <td>Circular</td>\n",
       "      <td>Linear</td>\n",
       "      <td>Two-way</td>\n",
       "      <td>AIDA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>This is the unpaid peer-to-peer communication...</td>\n",
       "      <td>To create successful viral marketing messages,...</td>\n",
       "      <td>Viral marketing.</td>\n",
       "      <td>Word-of-mouth marketing.</td>\n",
       "      <td>Direct-response advertising.</td>\n",
       "      <td>Peer-to-peer marketing.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>Bollywood cinema aimed at audiences in the Ind...</td>\n",
       "      <td>Bollywood has long influenced Indian society a...</td>\n",
       "      <td>Group influence.</td>\n",
       "      <td>Ethnic marketing.</td>\n",
       "      <td>Social grade.</td>\n",
       "      <td>Lifestyle.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>Segmentation by benefits sought is a type of ...</td>\n",
       "      <td>Marketers using benefit segmentation might dev...</td>\n",
       "      <td>To determine the marketer's effectiveness in s...</td>\n",
       "      <td>Used to create a psychographic profile of the ...</td>\n",
       "      <td>As a post-consumption evaluation tool.</td>\n",
       "      <td>That focuses on the attributes that people see...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 prompt   \n",
       "0     Which of the following is the most direct caus...  \\\n",
       "1     All of the following statements about muscle c...   \n",
       "2     Short-term changes in plant growth rate mediat...   \n",
       "3     RNA is thought to have played an important rol...   \n",
       "4     An organism with a lobed thallus, rhizoids, an...   \n",
       "...                                                 ...   \n",
       "1495   For a long time commercial media have been us...   \n",
       "1496  The _________ model emphasizes that each phase...   \n",
       "1497   This is the unpaid peer-to-peer communication...   \n",
       "1498  Bollywood cinema aimed at audiences in the Ind...   \n",
       "1499   Segmentation by benefits sought is a type of ...   \n",
       "\n",
       "                                                context   \n",
       "0     Within volvocine algae three genes have been i...  \\\n",
       "1     According to the sliding filament theory, the ...   \n",
       "2     In a living plant, auxins and other plant horm...   \n",
       "3     The role of RNA in the origin of life is best ...   \n",
       "4     Rhizophysidae is a family of siphonophores in ...   \n",
       "...                                                 ...   \n",
       "1495  There are multiple techniques for achieving an...   \n",
       "1496  This term is used merely to describe the struc...   \n",
       "1497  To create successful viral marketing messages,...   \n",
       "1498  Bollywood has long influenced Indian society a...   \n",
       "1499  Marketers using benefit segmentation might dev...   \n",
       "\n",
       "                                                      A   \n",
       "0                                     RNA transcription  \\\n",
       "1     The ends of actin filaments move closer together.   \n",
       "2         loss of turgor pressure in the affected cells   \n",
       "3                                                I only   \n",
       "4                                                  moss   \n",
       "...                                                 ...   \n",
       "1495                                   Direct response.   \n",
       "1496                                           Circular   \n",
       "1497                                   Viral marketing.   \n",
       "1498                                   Group influence.   \n",
       "1499  To determine the marketer's effectiveness in s...   \n",
       "\n",
       "                                                      B   \n",
       "0                             Supercoiling of chromatin  \\\n",
       "1       The length of myosin filaments does not change.   \n",
       "2     increased extensibility of the walls of affect...   \n",
       "3                                               II only   \n",
       "4                                             liverwort   \n",
       "...                                                 ...   \n",
       "1495                              Behavioural response.   \n",
       "1496                                             Linear   \n",
       "1497                           Word-of-mouth marketing.   \n",
       "1498                                  Ethnic marketing.   \n",
       "1499  Used to create a psychographic profile of the ...   \n",
       "\n",
       "                                                      C   \n",
       "0          Chromosome replication without cell division  \\\n",
       "1     Calcium-troponin binding precedes actin-myosin...   \n",
       "2     suppression of metabolic activity in affected ...   \n",
       "3                                              III only   \n",
       "4                                                  fern   \n",
       "...                                                 ...   \n",
       "1495                              Attitudinal response.   \n",
       "1496                                            Two-way   \n",
       "1497                       Direct-response advertising.   \n",
       "1498                                      Social grade.   \n",
       "1499             As a post-consumption evaluation tool.   \n",
       "\n",
       "                                                      D   E answer  \n",
       "0                              Chromosome recombination NaN      C  \n",
       "1     Calcium-tropomyosin binding precedes actin-myo... NaN      D  \n",
       "2     cytoskeletal rearrangements in the affected cells NaN      B  \n",
       "3                                            II and III NaN      D  \n",
       "4                                              mushroom NaN      B  \n",
       "...                                                 ...  ..    ...  \n",
       "1495                          Call-to-action response . NaN      C  \n",
       "1496                                               AIDA NaN      B  \n",
       "1497                            Peer-to-peer marketing. NaN      A  \n",
       "1498                                         Lifestyle. NaN      B  \n",
       "1499  That focuses on the attributes that people see... NaN      D  \n",
       "\n",
       "[1500 rows x 8 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_data = pd.read_csv(\"dataset/val_context.csv\")\n",
    "\n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>context</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Which of the following is the most direct caus...</td>\n",
       "      <td>Within volvocine algae three genes have been i...</td>\n",
       "      <td>RNA transcription</td>\n",
       "      <td>Supercoiling of chromatin</td>\n",
       "      <td>Chromosome replication without cell division</td>\n",
       "      <td>Chromosome recombination</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>All of the following statements about muscle c...</td>\n",
       "      <td>According to the sliding filament theory, the ...</td>\n",
       "      <td>The ends of actin filaments move closer together.</td>\n",
       "      <td>The length of myosin filaments does not change.</td>\n",
       "      <td>Calcium-troponin binding precedes actin-myosin...</td>\n",
       "      <td>Calcium-tropomyosin binding precedes actin-myo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Short-term changes in plant growth rate mediat...</td>\n",
       "      <td>In a living plant, auxins and other plant horm...</td>\n",
       "      <td>loss of turgor pressure in the affected cells</td>\n",
       "      <td>increased extensibility of the walls of affect...</td>\n",
       "      <td>suppression of metabolic activity in affected ...</td>\n",
       "      <td>cytoskeletal rearrangements in the affected cells</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RNA is thought to have played an important rol...</td>\n",
       "      <td>The role of RNA in the origin of life is best ...</td>\n",
       "      <td>I only</td>\n",
       "      <td>II only</td>\n",
       "      <td>III only</td>\n",
       "      <td>II and III</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>An organism with a lobed thallus, rhizoids, an...</td>\n",
       "      <td>Rhizophysidae is a family of siphonophores in ...</td>\n",
       "      <td>moss</td>\n",
       "      <td>liverwort</td>\n",
       "      <td>fern</td>\n",
       "      <td>mushroom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>For a long time commercial media have been us...</td>\n",
       "      <td>There are multiple techniques for achieving an...</td>\n",
       "      <td>Direct response.</td>\n",
       "      <td>Behavioural response.</td>\n",
       "      <td>Attitudinal response.</td>\n",
       "      <td>Call-to-action response .</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>The _________ model emphasizes that each phase...</td>\n",
       "      <td>This term is used merely to describe the struc...</td>\n",
       "      <td>Circular</td>\n",
       "      <td>Linear</td>\n",
       "      <td>Two-way</td>\n",
       "      <td>AIDA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>This is the unpaid peer-to-peer communication...</td>\n",
       "      <td>To create successful viral marketing messages,...</td>\n",
       "      <td>Viral marketing.</td>\n",
       "      <td>Word-of-mouth marketing.</td>\n",
       "      <td>Direct-response advertising.</td>\n",
       "      <td>Peer-to-peer marketing.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>Bollywood cinema aimed at audiences in the Ind...</td>\n",
       "      <td>Bollywood has long influenced Indian society a...</td>\n",
       "      <td>Group influence.</td>\n",
       "      <td>Ethnic marketing.</td>\n",
       "      <td>Social grade.</td>\n",
       "      <td>Lifestyle.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>Segmentation by benefits sought is a type of ...</td>\n",
       "      <td>Marketers using benefit segmentation might dev...</td>\n",
       "      <td>To determine the marketer's effectiveness in s...</td>\n",
       "      <td>Used to create a psychographic profile of the ...</td>\n",
       "      <td>As a post-consumption evaluation tool.</td>\n",
       "      <td>That focuses on the attributes that people see...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 prompt   \n",
       "0     Which of the following is the most direct caus...  \\\n",
       "1     All of the following statements about muscle c...   \n",
       "2     Short-term changes in plant growth rate mediat...   \n",
       "3     RNA is thought to have played an important rol...   \n",
       "4     An organism with a lobed thallus, rhizoids, an...   \n",
       "...                                                 ...   \n",
       "1495   For a long time commercial media have been us...   \n",
       "1496  The _________ model emphasizes that each phase...   \n",
       "1497   This is the unpaid peer-to-peer communication...   \n",
       "1498  Bollywood cinema aimed at audiences in the Ind...   \n",
       "1499   Segmentation by benefits sought is a type of ...   \n",
       "\n",
       "                                                context   \n",
       "0     Within volvocine algae three genes have been i...  \\\n",
       "1     According to the sliding filament theory, the ...   \n",
       "2     In a living plant, auxins and other plant horm...   \n",
       "3     The role of RNA in the origin of life is best ...   \n",
       "4     Rhizophysidae is a family of siphonophores in ...   \n",
       "...                                                 ...   \n",
       "1495  There are multiple techniques for achieving an...   \n",
       "1496  This term is used merely to describe the struc...   \n",
       "1497  To create successful viral marketing messages,...   \n",
       "1498  Bollywood has long influenced Indian society a...   \n",
       "1499  Marketers using benefit segmentation might dev...   \n",
       "\n",
       "                                                      A   \n",
       "0                                     RNA transcription  \\\n",
       "1     The ends of actin filaments move closer together.   \n",
       "2         loss of turgor pressure in the affected cells   \n",
       "3                                                I only   \n",
       "4                                                  moss   \n",
       "...                                                 ...   \n",
       "1495                                   Direct response.   \n",
       "1496                                           Circular   \n",
       "1497                                   Viral marketing.   \n",
       "1498                                   Group influence.   \n",
       "1499  To determine the marketer's effectiveness in s...   \n",
       "\n",
       "                                                      B   \n",
       "0                             Supercoiling of chromatin  \\\n",
       "1       The length of myosin filaments does not change.   \n",
       "2     increased extensibility of the walls of affect...   \n",
       "3                                               II only   \n",
       "4                                             liverwort   \n",
       "...                                                 ...   \n",
       "1495                              Behavioural response.   \n",
       "1496                                             Linear   \n",
       "1497                           Word-of-mouth marketing.   \n",
       "1498                                  Ethnic marketing.   \n",
       "1499  Used to create a psychographic profile of the ...   \n",
       "\n",
       "                                                      C   \n",
       "0          Chromosome replication without cell division  \\\n",
       "1     Calcium-troponin binding precedes actin-myosin...   \n",
       "2     suppression of metabolic activity in affected ...   \n",
       "3                                              III only   \n",
       "4                                                  fern   \n",
       "...                                                 ...   \n",
       "1495                              Attitudinal response.   \n",
       "1496                                            Two-way   \n",
       "1497                       Direct-response advertising.   \n",
       "1498                                      Social grade.   \n",
       "1499             As a post-consumption evaluation tool.   \n",
       "\n",
       "                                                      D   E answer   \n",
       "0                              Chromosome recombination NaN      C  \\\n",
       "1     Calcium-tropomyosin binding precedes actin-myo... NaN      D   \n",
       "2     cytoskeletal rearrangements in the affected cells NaN      B   \n",
       "3                                            II and III NaN      D   \n",
       "4                                              mushroom NaN      B   \n",
       "...                                                 ...  ..    ...   \n",
       "1495                          Call-to-action response . NaN      C   \n",
       "1496                                               AIDA NaN      B   \n",
       "1497                            Peer-to-peer marketing. NaN      A   \n",
       "1498                                         Lifestyle. NaN      B   \n",
       "1499  That focuses on the attributes that people see... NaN      D   \n",
       "\n",
       "      answer_index  \n",
       "0                2  \n",
       "1                3  \n",
       "2                1  \n",
       "3                3  \n",
       "4                1  \n",
       "...            ...  \n",
       "1495             2  \n",
       "1496             1  \n",
       "1497             0  \n",
       "1498             1  \n",
       "1499             3  \n",
       "\n",
       "[1500 rows x 9 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options = 'ABCDE'\n",
    "answer_to_index = {options[i]: i for i in range(len(options))}\n",
    "answer_to_index\n",
    "\n",
    "\n",
    "# apply answer_to_index to the answer column\n",
    "df_data['answer_index'] = df_data['answer'].apply(lambda x: answer_to_index[x])\n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['predictions/hyc_val.csv',\n",
       "  'predictions/openbook_val.csv',\n",
       "  'predictions/itk_ob_val.csv',\n",
       "  'predictions/chris_val.csv',\n",
       "  'predictions/itk_awp_val.csv'],\n",
       " ['hyc_val', 'openbook_val', 'itk_ob_val', 'chris_val', 'itk_awp_val'])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "pred_files = os.listdir(\"predictions\")\n",
    "# remove if 'viktor' in name\n",
    "pred_files = [f for f in pred_files if 'viktor' not in f]\n",
    "\n",
    "pred_files_names = [f.split(\".\")[0] for f in pred_files]\n",
    "pred_files = [f\"predictions/{f}\" for f in pred_files]\n",
    "\n",
    "pred_files, pred_files_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hyc_val_A_prob</th>\n",
       "      <th>hyc_val_B_prob</th>\n",
       "      <th>hyc_val_C_prob</th>\n",
       "      <th>hyc_val_D_prob</th>\n",
       "      <th>hyc_val_E_prob</th>\n",
       "      <th>openbook_val_A_prob</th>\n",
       "      <th>openbook_val_B_prob</th>\n",
       "      <th>openbook_val_C_prob</th>\n",
       "      <th>openbook_val_D_prob</th>\n",
       "      <th>openbook_val_E_prob</th>\n",
       "      <th>...</th>\n",
       "      <th>chris_val_A_prob</th>\n",
       "      <th>chris_val_B_prob</th>\n",
       "      <th>chris_val_C_prob</th>\n",
       "      <th>chris_val_D_prob</th>\n",
       "      <th>chris_val_E_prob</th>\n",
       "      <th>itk_awp_val_A_prob</th>\n",
       "      <th>itk_awp_val_B_prob</th>\n",
       "      <th>itk_awp_val_C_prob</th>\n",
       "      <th>itk_awp_val_D_prob</th>\n",
       "      <th>itk_awp_val_E_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.04930</td>\n",
       "      <td>0.74660</td>\n",
       "      <td>0.033570</td>\n",
       "      <td>0.060500</td>\n",
       "      <td>0.11005</td>\n",
       "      <td>0.002998</td>\n",
       "      <td>0.16650</td>\n",
       "      <td>0.12570</td>\n",
       "      <td>0.013530</td>\n",
       "      <td>0.69140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031000</td>\n",
       "      <td>0.35960</td>\n",
       "      <td>0.467500</td>\n",
       "      <td>0.11290</td>\n",
       "      <td>0.028810</td>\n",
       "      <td>0.03375</td>\n",
       "      <td>0.3296</td>\n",
       "      <td>0.54300</td>\n",
       "      <td>0.05980</td>\n",
       "      <td>0.03375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.12440</td>\n",
       "      <td>0.18570</td>\n",
       "      <td>0.242400</td>\n",
       "      <td>0.252700</td>\n",
       "      <td>0.19480</td>\n",
       "      <td>0.201900</td>\n",
       "      <td>0.17930</td>\n",
       "      <td>0.14420</td>\n",
       "      <td>0.137500</td>\n",
       "      <td>0.33720</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149500</td>\n",
       "      <td>0.19260</td>\n",
       "      <td>0.334500</td>\n",
       "      <td>0.28250</td>\n",
       "      <td>0.041050</td>\n",
       "      <td>0.18590</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.15280</td>\n",
       "      <td>0.11395</td>\n",
       "      <td>0.18590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.16540</td>\n",
       "      <td>0.32200</td>\n",
       "      <td>0.036160</td>\n",
       "      <td>0.193800</td>\n",
       "      <td>0.28250</td>\n",
       "      <td>0.416500</td>\n",
       "      <td>0.37450</td>\n",
       "      <td>0.12006</td>\n",
       "      <td>0.020690</td>\n",
       "      <td>0.06805</td>\n",
       "      <td>...</td>\n",
       "      <td>0.263000</td>\n",
       "      <td>0.47630</td>\n",
       "      <td>0.144000</td>\n",
       "      <td>0.10840</td>\n",
       "      <td>0.008316</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.3418</td>\n",
       "      <td>0.05730</td>\n",
       "      <td>0.31200</td>\n",
       "      <td>0.14440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.11380</td>\n",
       "      <td>0.15890</td>\n",
       "      <td>0.177400</td>\n",
       "      <td>0.334000</td>\n",
       "      <td>0.21580</td>\n",
       "      <td>0.066300</td>\n",
       "      <td>0.10990</td>\n",
       "      <td>0.05905</td>\n",
       "      <td>0.398400</td>\n",
       "      <td>0.36650</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013725</td>\n",
       "      <td>0.01924</td>\n",
       "      <td>0.014980</td>\n",
       "      <td>0.61230</td>\n",
       "      <td>0.339600</td>\n",
       "      <td>0.28930</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.13120</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.28930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.04773</td>\n",
       "      <td>0.78170</td>\n",
       "      <td>0.067440</td>\n",
       "      <td>0.008350</td>\n",
       "      <td>0.09454</td>\n",
       "      <td>0.043430</td>\n",
       "      <td>0.14390</td>\n",
       "      <td>0.09100</td>\n",
       "      <td>0.117700</td>\n",
       "      <td>0.60400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097100</td>\n",
       "      <td>0.40670</td>\n",
       "      <td>0.322000</td>\n",
       "      <td>0.10360</td>\n",
       "      <td>0.070700</td>\n",
       "      <td>0.19500</td>\n",
       "      <td>0.3170</td>\n",
       "      <td>0.17990</td>\n",
       "      <td>0.11320</td>\n",
       "      <td>0.19500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>0.04007</td>\n",
       "      <td>0.01892</td>\n",
       "      <td>0.803700</td>\n",
       "      <td>0.005337</td>\n",
       "      <td>0.13200</td>\n",
       "      <td>0.041530</td>\n",
       "      <td>0.03415</td>\n",
       "      <td>0.42820</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.49170</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032500</td>\n",
       "      <td>0.08240</td>\n",
       "      <td>0.851000</td>\n",
       "      <td>0.01463</td>\n",
       "      <td>0.019240</td>\n",
       "      <td>0.13050</td>\n",
       "      <td>0.1771</td>\n",
       "      <td>0.50700</td>\n",
       "      <td>0.05518</td>\n",
       "      <td>0.13050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>0.09660</td>\n",
       "      <td>0.36430</td>\n",
       "      <td>0.178800</td>\n",
       "      <td>0.174100</td>\n",
       "      <td>0.18620</td>\n",
       "      <td>0.067300</td>\n",
       "      <td>0.18930</td>\n",
       "      <td>0.18960</td>\n",
       "      <td>0.173800</td>\n",
       "      <td>0.38010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038150</td>\n",
       "      <td>0.65700</td>\n",
       "      <td>0.103500</td>\n",
       "      <td>0.05710</td>\n",
       "      <td>0.143800</td>\n",
       "      <td>0.18250</td>\n",
       "      <td>0.1914</td>\n",
       "      <td>0.22730</td>\n",
       "      <td>0.21630</td>\n",
       "      <td>0.18250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>0.63600</td>\n",
       "      <td>0.08875</td>\n",
       "      <td>0.010155</td>\n",
       "      <td>0.146600</td>\n",
       "      <td>0.11820</td>\n",
       "      <td>0.704000</td>\n",
       "      <td>0.10614</td>\n",
       "      <td>0.02008</td>\n",
       "      <td>0.075130</td>\n",
       "      <td>0.09450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.893600</td>\n",
       "      <td>0.08870</td>\n",
       "      <td>0.000701</td>\n",
       "      <td>0.01630</td>\n",
       "      <td>0.000597</td>\n",
       "      <td>0.31740</td>\n",
       "      <td>0.0906</td>\n",
       "      <td>0.05743</td>\n",
       "      <td>0.21740</td>\n",
       "      <td>0.31740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>0.13370</td>\n",
       "      <td>0.14730</td>\n",
       "      <td>0.246500</td>\n",
       "      <td>0.123350</td>\n",
       "      <td>0.34900</td>\n",
       "      <td>0.042570</td>\n",
       "      <td>0.26540</td>\n",
       "      <td>0.04346</td>\n",
       "      <td>0.025150</td>\n",
       "      <td>0.62350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.126100</td>\n",
       "      <td>0.34230</td>\n",
       "      <td>0.067600</td>\n",
       "      <td>0.12780</td>\n",
       "      <td>0.336200</td>\n",
       "      <td>0.19100</td>\n",
       "      <td>0.2307</td>\n",
       "      <td>0.21100</td>\n",
       "      <td>0.17610</td>\n",
       "      <td>0.19100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>0.01475</td>\n",
       "      <td>0.66750</td>\n",
       "      <td>0.009990</td>\n",
       "      <td>0.130100</td>\n",
       "      <td>0.17750</td>\n",
       "      <td>0.008150</td>\n",
       "      <td>0.68850</td>\n",
       "      <td>0.04297</td>\n",
       "      <td>0.209000</td>\n",
       "      <td>0.05130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011925</td>\n",
       "      <td>0.13130</td>\n",
       "      <td>0.015205</td>\n",
       "      <td>0.80500</td>\n",
       "      <td>0.036250</td>\n",
       "      <td>0.03590</td>\n",
       "      <td>0.2473</td>\n",
       "      <td>0.08990</td>\n",
       "      <td>0.59100</td>\n",
       "      <td>0.03590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      hyc_val_A_prob  hyc_val_B_prob  hyc_val_C_prob  hyc_val_D_prob   \n",
       "0            0.04930         0.74660        0.033570        0.060500  \\\n",
       "1            0.12440         0.18570        0.242400        0.252700   \n",
       "2            0.16540         0.32200        0.036160        0.193800   \n",
       "3            0.11380         0.15890        0.177400        0.334000   \n",
       "4            0.04773         0.78170        0.067440        0.008350   \n",
       "...              ...             ...             ...             ...   \n",
       "1495         0.04007         0.01892        0.803700        0.005337   \n",
       "1496         0.09660         0.36430        0.178800        0.174100   \n",
       "1497         0.63600         0.08875        0.010155        0.146600   \n",
       "1498         0.13370         0.14730        0.246500        0.123350   \n",
       "1499         0.01475         0.66750        0.009990        0.130100   \n",
       "\n",
       "      hyc_val_E_prob  openbook_val_A_prob  openbook_val_B_prob   \n",
       "0            0.11005             0.002998              0.16650  \\\n",
       "1            0.19480             0.201900              0.17930   \n",
       "2            0.28250             0.416500              0.37450   \n",
       "3            0.21580             0.066300              0.10990   \n",
       "4            0.09454             0.043430              0.14390   \n",
       "...              ...                  ...                  ...   \n",
       "1495         0.13200             0.041530              0.03415   \n",
       "1496         0.18620             0.067300              0.18930   \n",
       "1497         0.11820             0.704000              0.10614   \n",
       "1498         0.34900             0.042570              0.26540   \n",
       "1499         0.17750             0.008150              0.68850   \n",
       "\n",
       "      openbook_val_C_prob  openbook_val_D_prob  openbook_val_E_prob  ...   \n",
       "0                 0.12570             0.013530              0.69140  ...  \\\n",
       "1                 0.14420             0.137500              0.33720  ...   \n",
       "2                 0.12006             0.020690              0.06805  ...   \n",
       "3                 0.05905             0.398400              0.36650  ...   \n",
       "4                 0.09100             0.117700              0.60400  ...   \n",
       "...                   ...                  ...                  ...  ...   \n",
       "1495              0.42820             0.004345              0.49170  ...   \n",
       "1496              0.18960             0.173800              0.38010  ...   \n",
       "1497              0.02008             0.075130              0.09450  ...   \n",
       "1498              0.04346             0.025150              0.62350  ...   \n",
       "1499              0.04297             0.209000              0.05130  ...   \n",
       "\n",
       "      chris_val_A_prob  chris_val_B_prob  chris_val_C_prob  chris_val_D_prob   \n",
       "0             0.031000           0.35960          0.467500           0.11290  \\\n",
       "1             0.149500           0.19260          0.334500           0.28250   \n",
       "2             0.263000           0.47630          0.144000           0.10840   \n",
       "3             0.013725           0.01924          0.014980           0.61230   \n",
       "4             0.097100           0.40670          0.322000           0.10360   \n",
       "...                ...               ...               ...               ...   \n",
       "1495          0.032500           0.08240          0.851000           0.01463   \n",
       "1496          0.038150           0.65700          0.103500           0.05710   \n",
       "1497          0.893600           0.08870          0.000701           0.01630   \n",
       "1498          0.126100           0.34230          0.067600           0.12780   \n",
       "1499          0.011925           0.13130          0.015205           0.80500   \n",
       "\n",
       "      chris_val_E_prob  itk_awp_val_A_prob  itk_awp_val_B_prob   \n",
       "0             0.028810             0.03375              0.3296  \\\n",
       "1             0.041050             0.18590              0.3613   \n",
       "2             0.008316             0.14440              0.3418   \n",
       "3             0.339600             0.28930              0.1430   \n",
       "4             0.070700             0.19500              0.3170   \n",
       "...                ...                 ...                 ...   \n",
       "1495          0.019240             0.13050              0.1771   \n",
       "1496          0.143800             0.18250              0.1914   \n",
       "1497          0.000597             0.31740              0.0906   \n",
       "1498          0.336200             0.19100              0.2307   \n",
       "1499          0.036250             0.03590              0.2473   \n",
       "\n",
       "      itk_awp_val_C_prob  itk_awp_val_D_prob  itk_awp_val_E_prob  \n",
       "0                0.54300             0.05980             0.03375  \n",
       "1                0.15280             0.11395             0.18590  \n",
       "2                0.05730             0.31200             0.14440  \n",
       "3                0.13120             0.14710             0.28930  \n",
       "4                0.17990             0.11320             0.19500  \n",
       "...                  ...                 ...                 ...  \n",
       "1495             0.50700             0.05518             0.13050  \n",
       "1496             0.22730             0.21630             0.18250  \n",
       "1497             0.05743             0.21740             0.31740  \n",
       "1498             0.21100             0.17610             0.19100  \n",
       "1499             0.08990             0.59100             0.03590  \n",
       "\n",
       "[1500 rows x 25 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean(df_input, indx):\n",
    "    # remove \"Unnamed: 0\" column\n",
    "    df_input = df_input.drop(columns=[\"Unnamed: 0\"])\n",
    "    # loop over columns and remane as \"{pred_files_names[0]}_{columns_name}\"\n",
    "    for col in df_input.columns:\n",
    "        df_input = df_input.rename(columns={col: f\"{pred_files_names[indx]}_{col}\"})\n",
    "    return df_input\n",
    "    \n",
    "df_preds = pd.read_csv(pred_files[0]).reset_index(drop=True)\n",
    "df_preds = clean(df_preds, 0)\n",
    "\n",
    "\n",
    "for i in range(1, len(pred_files)):\n",
    "    df_tmp = pd.read_csv(pred_files[i]).reset_index(drop=True)\n",
    "    df_tmp = clean(df_tmp, i)\n",
    "    \n",
    "    df_preds = pd.concat([df_preds, df_tmp], axis=1)\n",
    "    \n",
    "\n",
    "\n",
    "df_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preds.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0493  , 0.7466  , 0.03357 , ..., 0.543   , 0.0598  , 0.03375 ],\n",
       "       [0.1244  , 0.1857  , 0.2424  , ..., 0.1528  , 0.11395 , 0.1859  ],\n",
       "       [0.1654  , 0.322   , 0.03616 , ..., 0.0573  , 0.312   , 0.1444  ],\n",
       "       ...,\n",
       "       [0.636   , 0.08875 , 0.010155, ..., 0.05743 , 0.2174  , 0.3174  ],\n",
       "       [0.1337  , 0.1473  , 0.2465  , ..., 0.211   , 0.1761  , 0.191   ],\n",
       "       [0.01475 , 0.6675  , 0.00999 , ..., 0.0899  , 0.591   , 0.0359  ]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_preds.values\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 1, ..., 0, 1, 3])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df_data['answer_index'].values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-merror:0.32583\teval-merror:0.54333\n",
      "[1]\ttrain-merror:0.24667\teval-merror:0.51000\n",
      "[2]\ttrain-merror:0.19333\teval-merror:0.51667\n",
      "[3]\ttrain-merror:0.16917\teval-merror:0.54667\n",
      "[4]\ttrain-merror:0.14167\teval-merror:0.53667\n",
      "[5]\ttrain-merror:0.12250\teval-merror:0.52333\n",
      "[6]\ttrain-merror:0.09667\teval-merror:0.51667\n",
      "[7]\ttrain-merror:0.08333\teval-merror:0.50333\n",
      "[8]\ttrain-merror:0.07500\teval-merror:0.51667\n",
      "[9]\ttrain-merror:0.06167\teval-merror:0.49667\n",
      "[10]\ttrain-merror:0.05500\teval-merror:0.50000\n",
      "[11]\ttrain-merror:0.04833\teval-merror:0.50333\n",
      "[12]\ttrain-merror:0.03333\teval-merror:0.51667\n",
      "[13]\ttrain-merror:0.02500\teval-merror:0.50667\n",
      "[14]\ttrain-merror:0.01833\teval-merror:0.50333\n",
      "[15]\ttrain-merror:0.01417\teval-merror:0.50333\n",
      "[16]\ttrain-merror:0.01167\teval-merror:0.50000\n",
      "[17]\ttrain-merror:0.00750\teval-merror:0.49667\n",
      "[18]\ttrain-merror:0.00667\teval-merror:0.50000\n",
      "[19]\ttrain-merror:0.00500\teval-merror:0.50000\n",
      "[20]\ttrain-merror:0.00250\teval-merror:0.49333\n",
      "[21]\ttrain-merror:0.00167\teval-merror:0.50000\n",
      "[22]\ttrain-merror:0.00083\teval-merror:0.50000\n",
      "[23]\ttrain-merror:0.00083\teval-merror:0.50333\n",
      "[24]\ttrain-merror:0.00083\teval-merror:0.51333\n",
      "[25]\ttrain-merror:0.00083\teval-merror:0.50000\n",
      "[26]\ttrain-merror:0.00083\teval-merror:0.48333\n",
      "[27]\ttrain-merror:0.00000\teval-merror:0.50667\n",
      "[28]\ttrain-merror:0.00083\teval-merror:0.49667\n",
      "[29]\ttrain-merror:0.00083\teval-merror:0.50000\n",
      "[30]\ttrain-merror:0.00000\teval-merror:0.50000\n",
      "[31]\ttrain-merror:0.00000\teval-merror:0.50000\n",
      "[32]\ttrain-merror:0.00000\teval-merror:0.50667\n",
      "[33]\ttrain-merror:0.00000\teval-merror:0.50667\n",
      "[34]\ttrain-merror:0.00000\teval-merror:0.50000\n",
      "[35]\ttrain-merror:0.00000\teval-merror:0.50667\n",
      "[36]\ttrain-merror:0.00000\teval-merror:0.50000\n",
      "[37]\ttrain-merror:0.00000\teval-merror:0.51000\n",
      "[38]\ttrain-merror:0.00000\teval-merror:0.49000\n",
      "[39]\ttrain-merror:0.00000\teval-merror:0.49333\n",
      "[40]\ttrain-merror:0.00000\teval-merror:0.51000\n",
      "[41]\ttrain-merror:0.00000\teval-merror:0.49667\n",
      "[42]\ttrain-merror:0.00000\teval-merror:0.50333\n",
      "[43]\ttrain-merror:0.00000\teval-merror:0.50333\n",
      "[44]\ttrain-merror:0.00000\teval-merror:0.50333\n",
      "[45]\ttrain-merror:0.00000\teval-merror:0.50000\n",
      "[46]\ttrain-merror:0.00000\teval-merror:0.50333\n",
      "[47]\ttrain-merror:0.00000\teval-merror:0.51333\n",
      "[48]\ttrain-merror:0.00000\teval-merror:0.51667\n",
      "[49]\ttrain-merror:0.00000\teval-merror:0.52000\n",
      "[50]\ttrain-merror:0.00000\teval-merror:0.51667\n",
      "[51]\ttrain-merror:0.00000\teval-merror:0.51000\n",
      "[52]\ttrain-merror:0.00000\teval-merror:0.51333\n",
      "[53]\ttrain-merror:0.00000\teval-merror:0.51000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/viktor/miniconda3/envs/torch-env/lib/python3.9/site-packages/xgboost/core.py:726: FutureWarning: Pass `evals` as keyword args.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[54]\ttrain-merror:0.00000\teval-merror:0.51667\n",
      "[55]\ttrain-merror:0.00000\teval-merror:0.51000\n",
      "[56]\ttrain-merror:0.00000\teval-merror:0.51000\n",
      "[57]\ttrain-merror:0.00000\teval-merror:0.51000\n",
      "[58]\ttrain-merror:0.00000\teval-merror:0.51000\n",
      "[59]\ttrain-merror:0.00000\teval-merror:0.51667\n",
      "[60]\ttrain-merror:0.00000\teval-merror:0.51333\n",
      "[61]\ttrain-merror:0.00000\teval-merror:0.51000\n",
      "[62]\ttrain-merror:0.00000\teval-merror:0.51667\n",
      "[63]\ttrain-merror:0.00000\teval-merror:0.51000\n",
      "[64]\ttrain-merror:0.00000\teval-merror:0.50667\n",
      "[65]\ttrain-merror:0.00000\teval-merror:0.50667\n",
      "[66]\ttrain-merror:0.00000\teval-merror:0.50333\n",
      "[67]\ttrain-merror:0.00000\teval-merror:0.51000\n",
      "[68]\ttrain-merror:0.00000\teval-merror:0.49333\n",
      "[69]\ttrain-merror:0.00000\teval-merror:0.51000\n",
      "[70]\ttrain-merror:0.00000\teval-merror:0.51333\n",
      "[71]\ttrain-merror:0.00000\teval-merror:0.51000\n",
      "[72]\ttrain-merror:0.00000\teval-merror:0.51000\n",
      "[73]\ttrain-merror:0.00000\teval-merror:0.51333\n",
      "[74]\ttrain-merror:0.00000\teval-merror:0.51667\n",
      "[75]\ttrain-merror:0.00000\teval-merror:0.52000\n",
      "[76]\ttrain-merror:0.00000\teval-merror:0.52333\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "watchlist = [(dtrain, 'train'), (dtest, 'eval')]\n",
    "\n",
    "params = {\n",
    "    'objective': 'multi:softprob',\n",
    "    'num_class': 5,  # number of classes\n",
    "    'booster': 'gbtree',\n",
    "    'n_jobs': -1,    # use all CPU cores\n",
    "    'random_state': 42,\n",
    "    'eval_metric': 'merror'  # error rate for multi-class classification\n",
    "}\n",
    "\n",
    "\n",
    "num_round = 80  # number of boosting rounds, you can adjust this based on your needs\n",
    "clf = xgb.train(params, \n",
    "                dtrain,\n",
    "                num_round, \n",
    "                watchlist, \n",
    "                early_stopping_rounds=50, \n",
    "                verbose_eval=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04634811, 0.00998932, 0.797225  , 0.14555168, 0.00088584],\n",
       "       [0.03867351, 0.03039374, 0.05760349, 0.8723642 , 0.0009651 ],\n",
       "       [0.16313344, 0.3064124 , 0.17949206, 0.34891105, 0.00205107],\n",
       "       ...,\n",
       "       [0.3142134 , 0.08534947, 0.5933856 , 0.00598921, 0.0010623 ],\n",
       "       [0.13545167, 0.10341583, 0.07655868, 0.68328655, 0.00128722],\n",
       "       [0.03096482, 0.6247084 , 0.23203962, 0.11067588, 0.00161131]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities = clf.predict(dtest)\n",
    "probabilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 3, 0, 2, 1, 1, 2, 2, 1, 1, 1, 3, 2, 0, 2, 2, 2, 1, 2, 0, 2,\n",
       "       2, 3, 2, 0, 2, 1, 0, 0, 1, 1, 1, 3, 0, 0, 2, 1, 3, 0, 2, 0, 3, 1,\n",
       "       3, 2, 3, 2, 0, 0, 3, 2, 2, 2, 0, 2, 0, 2, 1, 1, 0, 2, 0, 2, 3, 3,\n",
       "       3, 2, 0, 3, 0, 1, 3, 2, 3, 1, 3, 3, 0, 3, 0, 0, 1, 2, 1, 0, 1, 0,\n",
       "       0, 3, 2, 1, 0, 0, 2, 3, 3, 0, 1, 3, 1, 0, 2, 2, 2, 1, 0, 3, 2, 2,\n",
       "       3, 0, 2, 0, 2, 0, 1, 0, 3, 2, 2, 1, 2, 0, 2, 1, 2, 3, 2, 1, 3, 3,\n",
       "       3, 2, 3, 1, 2, 0, 2, 0, 0, 0, 0, 1, 0, 1, 2, 2, 2, 1, 2, 0, 1, 3,\n",
       "       1, 2, 1, 3, 2, 2, 2, 3, 1, 0, 0, 0, 0, 0, 3, 2, 1, 2, 2, 1, 1, 3,\n",
       "       2, 0, 0, 1, 0, 3, 2, 2, 1, 0, 2, 3, 2, 3, 1, 2, 3, 1, 0, 3, 1, 0,\n",
       "       3, 1, 0, 0, 3, 3, 1, 3, 2, 1, 3, 1, 1, 2, 3, 2, 2, 0, 2, 2, 2, 2,\n",
       "       1, 3, 3, 2, 0, 0, 0, 2, 0, 1, 1, 3, 2, 0, 1, 0, 0, 3, 3, 0, 1, 1,\n",
       "       2, 2, 2, 0, 1, 1, 2, 3, 3, 2, 0, 3, 2, 2, 0, 1, 0, 1, 3, 0, 3, 3,\n",
       "       2, 3, 0, 1, 1, 3, 1, 2, 2, 1, 2, 2, 1, 1, 3, 2, 3, 2, 1, 0, 2, 1,\n",
       "       3, 1, 2, 1, 2, 2, 0, 0, 0, 1, 2, 2, 3, 1])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "predicted_labels = np.argmax(probabilities, axis=1)\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5266666666666666"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(y_test == predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class CombineScoresNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(CombineScoresNet, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_torch = torch.LongTensor(y_train)  # Assuming y_train is a pandas series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_torch = torch.LongTensor(y_test)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, inputs, labels):\n",
    "    outputs = model(inputs)\n",
    "    _, predicted = torch.max(outputs, 1)  # get the index of the max probability\n",
    "    total = labels.size(0)\n",
    "    correct = (predicted == labels).sum().item()\n",
    "    return 100 * correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/2000], Loss: 1.7806, Train Accuracy: 39.17%, Test Accuracy: 35.00%\n",
      "Epoch [10/2000], Loss: 1.7631, Train Accuracy: 46.58%, Test Accuracy: 44.33%\n",
      "Epoch [15/2000], Loss: 1.7394, Train Accuracy: 47.42%, Test Accuracy: 45.00%\n",
      "Epoch [20/2000], Loss: 1.7111, Train Accuracy: 49.75%, Test Accuracy: 46.00%\n",
      "Epoch [25/2000], Loss: 1.6814, Train Accuracy: 51.42%, Test Accuracy: 47.33%\n",
      "Epoch [30/2000], Loss: 1.6481, Train Accuracy: 52.08%, Test Accuracy: 49.00%\n",
      "Epoch [35/2000], Loss: 1.6118, Train Accuracy: 51.58%, Test Accuracy: 48.33%\n",
      "Epoch [40/2000], Loss: 1.5756, Train Accuracy: 52.42%, Test Accuracy: 48.33%\n",
      "Epoch [45/2000], Loss: 1.5468, Train Accuracy: 52.25%, Test Accuracy: 48.67%\n",
      "Epoch [50/2000], Loss: 1.5276, Train Accuracy: 52.08%, Test Accuracy: 48.67%\n",
      "Epoch [55/2000], Loss: 1.5162, Train Accuracy: 52.17%, Test Accuracy: 49.33%\n",
      "Epoch [60/2000], Loss: 1.5094, Train Accuracy: 52.50%, Test Accuracy: 49.00%\n",
      "Epoch [65/2000], Loss: 1.5049, Train Accuracy: 52.50%, Test Accuracy: 48.00%\n",
      "Epoch [70/2000], Loss: 1.5014, Train Accuracy: 52.67%, Test Accuracy: 47.33%\n",
      "Epoch [75/2000], Loss: 1.4984, Train Accuracy: 53.17%, Test Accuracy: 47.33%\n",
      "Epoch [80/2000], Loss: 1.4954, Train Accuracy: 53.75%, Test Accuracy: 46.33%\n",
      "Epoch [85/2000], Loss: 1.4925, Train Accuracy: 54.00%, Test Accuracy: 45.67%\n",
      "Epoch [90/2000], Loss: 1.4897, Train Accuracy: 54.33%, Test Accuracy: 46.00%\n",
      "Epoch [95/2000], Loss: 1.4872, Train Accuracy: 54.83%, Test Accuracy: 45.67%\n",
      "Epoch [100/2000], Loss: 1.4848, Train Accuracy: 54.83%, Test Accuracy: 45.67%\n",
      "Epoch [105/2000], Loss: 1.4826, Train Accuracy: 55.17%, Test Accuracy: 46.33%\n",
      "Epoch [110/2000], Loss: 1.4806, Train Accuracy: 55.75%, Test Accuracy: 46.33%\n",
      "Epoch [115/2000], Loss: 1.4786, Train Accuracy: 56.33%, Test Accuracy: 46.00%\n",
      "Epoch [120/2000], Loss: 1.4768, Train Accuracy: 56.42%, Test Accuracy: 45.33%\n",
      "Epoch [125/2000], Loss: 1.4750, Train Accuracy: 56.58%, Test Accuracy: 45.00%\n",
      "Epoch [130/2000], Loss: 1.4734, Train Accuracy: 56.75%, Test Accuracy: 44.67%\n",
      "Epoch [135/2000], Loss: 1.4718, Train Accuracy: 56.67%, Test Accuracy: 44.67%\n",
      "Epoch [140/2000], Loss: 1.4702, Train Accuracy: 56.75%, Test Accuracy: 44.67%\n",
      "Epoch [145/2000], Loss: 1.4686, Train Accuracy: 56.92%, Test Accuracy: 45.00%\n",
      "Epoch [150/2000], Loss: 1.4672, Train Accuracy: 57.42%, Test Accuracy: 45.00%\n",
      "Epoch [155/2000], Loss: 1.4658, Train Accuracy: 57.50%, Test Accuracy: 44.33%\n",
      "Epoch [160/2000], Loss: 1.4645, Train Accuracy: 58.00%, Test Accuracy: 44.00%\n",
      "Epoch [165/2000], Loss: 1.4633, Train Accuracy: 58.42%, Test Accuracy: 43.67%\n",
      "Epoch [170/2000], Loss: 1.4620, Train Accuracy: 58.25%, Test Accuracy: 43.67%\n",
      "Epoch [175/2000], Loss: 1.4608, Train Accuracy: 58.50%, Test Accuracy: 43.67%\n",
      "Epoch [180/2000], Loss: 1.4596, Train Accuracy: 58.75%, Test Accuracy: 43.33%\n",
      "Epoch [185/2000], Loss: 1.4583, Train Accuracy: 59.00%, Test Accuracy: 44.00%\n",
      "Epoch [190/2000], Loss: 1.4570, Train Accuracy: 59.42%, Test Accuracy: 44.00%\n",
      "Epoch [195/2000], Loss: 1.4556, Train Accuracy: 59.50%, Test Accuracy: 44.00%\n",
      "Epoch [200/2000], Loss: 1.4542, Train Accuracy: 59.83%, Test Accuracy: 44.00%\n",
      "Epoch [205/2000], Loss: 1.4528, Train Accuracy: 60.25%, Test Accuracy: 43.67%\n",
      "Epoch [210/2000], Loss: 1.4514, Train Accuracy: 60.25%, Test Accuracy: 43.33%\n",
      "Epoch [215/2000], Loss: 1.4500, Train Accuracy: 60.42%, Test Accuracy: 44.00%\n",
      "Epoch [220/2000], Loss: 1.4485, Train Accuracy: 60.33%, Test Accuracy: 44.00%\n",
      "Epoch [225/2000], Loss: 1.4470, Train Accuracy: 60.75%, Test Accuracy: 44.67%\n",
      "Epoch [230/2000], Loss: 1.4456, Train Accuracy: 61.17%, Test Accuracy: 44.67%\n",
      "Epoch [235/2000], Loss: 1.4441, Train Accuracy: 61.42%, Test Accuracy: 44.67%\n",
      "Epoch [240/2000], Loss: 1.4427, Train Accuracy: 61.50%, Test Accuracy: 44.67%\n",
      "Epoch [245/2000], Loss: 1.4413, Train Accuracy: 61.67%, Test Accuracy: 44.67%\n",
      "Epoch [250/2000], Loss: 1.4399, Train Accuracy: 61.92%, Test Accuracy: 45.00%\n",
      "Epoch [255/2000], Loss: 1.4384, Train Accuracy: 62.17%, Test Accuracy: 45.00%\n",
      "Epoch [260/2000], Loss: 1.4371, Train Accuracy: 62.17%, Test Accuracy: 45.67%\n",
      "Epoch [265/2000], Loss: 1.4357, Train Accuracy: 62.25%, Test Accuracy: 45.67%\n",
      "Epoch [270/2000], Loss: 1.4343, Train Accuracy: 62.42%, Test Accuracy: 45.33%\n",
      "Epoch [275/2000], Loss: 1.4330, Train Accuracy: 62.58%, Test Accuracy: 45.67%\n",
      "Epoch [280/2000], Loss: 1.4317, Train Accuracy: 62.67%, Test Accuracy: 45.67%\n",
      "Epoch [285/2000], Loss: 1.4304, Train Accuracy: 62.75%, Test Accuracy: 45.67%\n",
      "Epoch [290/2000], Loss: 1.4292, Train Accuracy: 62.75%, Test Accuracy: 45.67%\n",
      "Epoch [295/2000], Loss: 1.4279, Train Accuracy: 63.00%, Test Accuracy: 46.00%\n",
      "Epoch [300/2000], Loss: 1.4267, Train Accuracy: 63.00%, Test Accuracy: 45.67%\n",
      "Epoch [305/2000], Loss: 1.4254, Train Accuracy: 63.17%, Test Accuracy: 45.67%\n",
      "Epoch [310/2000], Loss: 1.4242, Train Accuracy: 63.42%, Test Accuracy: 45.33%\n",
      "Epoch [315/2000], Loss: 1.4230, Train Accuracy: 63.67%, Test Accuracy: 45.33%\n",
      "Epoch [320/2000], Loss: 1.4218, Train Accuracy: 63.67%, Test Accuracy: 45.00%\n",
      "Epoch [325/2000], Loss: 1.4206, Train Accuracy: 63.92%, Test Accuracy: 45.00%\n",
      "Epoch [330/2000], Loss: 1.4195, Train Accuracy: 64.00%, Test Accuracy: 45.00%\n",
      "Epoch [335/2000], Loss: 1.4183, Train Accuracy: 64.00%, Test Accuracy: 45.00%\n",
      "Epoch [340/2000], Loss: 1.4173, Train Accuracy: 64.00%, Test Accuracy: 45.00%\n",
      "Epoch [345/2000], Loss: 1.4163, Train Accuracy: 64.08%, Test Accuracy: 45.33%\n",
      "Epoch [350/2000], Loss: 1.4153, Train Accuracy: 64.08%, Test Accuracy: 45.00%\n",
      "Epoch [355/2000], Loss: 1.4144, Train Accuracy: 64.08%, Test Accuracy: 45.00%\n",
      "Epoch [360/2000], Loss: 1.4135, Train Accuracy: 64.00%, Test Accuracy: 45.33%\n",
      "Epoch [365/2000], Loss: 1.4127, Train Accuracy: 64.00%, Test Accuracy: 45.33%\n",
      "Epoch [370/2000], Loss: 1.4118, Train Accuracy: 64.17%, Test Accuracy: 45.00%\n",
      "Epoch [375/2000], Loss: 1.4112, Train Accuracy: 64.17%, Test Accuracy: 45.00%\n",
      "Epoch [380/2000], Loss: 1.4103, Train Accuracy: 64.17%, Test Accuracy: 45.00%\n",
      "Epoch [385/2000], Loss: 1.4094, Train Accuracy: 64.17%, Test Accuracy: 45.00%\n",
      "Epoch [390/2000], Loss: 1.4086, Train Accuracy: 64.33%, Test Accuracy: 45.00%\n",
      "Epoch [395/2000], Loss: 1.4078, Train Accuracy: 64.42%, Test Accuracy: 45.00%\n",
      "Epoch [400/2000], Loss: 1.4070, Train Accuracy: 64.58%, Test Accuracy: 44.67%\n",
      "Epoch [405/2000], Loss: 1.4063, Train Accuracy: 64.75%, Test Accuracy: 44.67%\n",
      "Epoch [410/2000], Loss: 1.4055, Train Accuracy: 64.58%, Test Accuracy: 44.33%\n",
      "Epoch [415/2000], Loss: 1.4049, Train Accuracy: 64.58%, Test Accuracy: 45.00%\n",
      "Epoch [420/2000], Loss: 1.4041, Train Accuracy: 64.67%, Test Accuracy: 44.67%\n",
      "Epoch [425/2000], Loss: 1.4033, Train Accuracy: 64.92%, Test Accuracy: 44.67%\n",
      "Epoch [430/2000], Loss: 1.4027, Train Accuracy: 65.00%, Test Accuracy: 44.67%\n",
      "Epoch [435/2000], Loss: 1.4020, Train Accuracy: 65.08%, Test Accuracy: 44.67%\n",
      "Epoch [440/2000], Loss: 1.4012, Train Accuracy: 65.08%, Test Accuracy: 45.00%\n",
      "Epoch [445/2000], Loss: 1.4006, Train Accuracy: 65.17%, Test Accuracy: 45.00%\n",
      "Epoch [450/2000], Loss: 1.3999, Train Accuracy: 65.33%, Test Accuracy: 44.67%\n",
      "Epoch [455/2000], Loss: 1.3993, Train Accuracy: 65.33%, Test Accuracy: 44.67%\n",
      "Epoch [460/2000], Loss: 1.3987, Train Accuracy: 65.42%, Test Accuracy: 44.00%\n",
      "Epoch [465/2000], Loss: 1.3981, Train Accuracy: 65.42%, Test Accuracy: 44.00%\n",
      "Epoch [470/2000], Loss: 1.3974, Train Accuracy: 65.58%, Test Accuracy: 44.00%\n",
      "Epoch [475/2000], Loss: 1.3966, Train Accuracy: 65.50%, Test Accuracy: 44.00%\n",
      "Epoch [480/2000], Loss: 1.3958, Train Accuracy: 65.58%, Test Accuracy: 44.00%\n",
      "Epoch [485/2000], Loss: 1.3951, Train Accuracy: 65.83%, Test Accuracy: 44.00%\n",
      "Epoch [490/2000], Loss: 1.3944, Train Accuracy: 65.92%, Test Accuracy: 44.00%\n",
      "Epoch [495/2000], Loss: 1.3938, Train Accuracy: 65.83%, Test Accuracy: 44.00%\n",
      "Epoch [500/2000], Loss: 1.3932, Train Accuracy: 66.00%, Test Accuracy: 44.00%\n",
      "Epoch [505/2000], Loss: 1.3928, Train Accuracy: 65.92%, Test Accuracy: 44.00%\n",
      "Epoch [510/2000], Loss: 1.3920, Train Accuracy: 65.92%, Test Accuracy: 44.00%\n",
      "Epoch [515/2000], Loss: 1.3915, Train Accuracy: 66.00%, Test Accuracy: 44.00%\n",
      "Epoch [520/2000], Loss: 1.3909, Train Accuracy: 66.00%, Test Accuracy: 44.00%\n",
      "Epoch [525/2000], Loss: 1.3904, Train Accuracy: 66.17%, Test Accuracy: 44.00%\n",
      "Epoch [530/2000], Loss: 1.3900, Train Accuracy: 66.17%, Test Accuracy: 44.00%\n",
      "Epoch [535/2000], Loss: 1.3895, Train Accuracy: 66.08%, Test Accuracy: 44.00%\n",
      "Epoch [540/2000], Loss: 1.3892, Train Accuracy: 66.17%, Test Accuracy: 44.00%\n",
      "Epoch [545/2000], Loss: 1.3889, Train Accuracy: 66.25%, Test Accuracy: 44.00%\n",
      "Epoch [550/2000], Loss: 1.3885, Train Accuracy: 66.17%, Test Accuracy: 44.00%\n",
      "Epoch [555/2000], Loss: 1.3881, Train Accuracy: 66.17%, Test Accuracy: 44.00%\n",
      "Epoch [560/2000], Loss: 1.3878, Train Accuracy: 66.25%, Test Accuracy: 44.00%\n",
      "Epoch [565/2000], Loss: 1.3875, Train Accuracy: 66.25%, Test Accuracy: 44.00%\n",
      "Epoch [570/2000], Loss: 1.3871, Train Accuracy: 66.25%, Test Accuracy: 44.00%\n",
      "Epoch [575/2000], Loss: 1.3868, Train Accuracy: 66.33%, Test Accuracy: 44.00%\n",
      "Epoch [580/2000], Loss: 1.3865, Train Accuracy: 66.33%, Test Accuracy: 44.00%\n",
      "Epoch [585/2000], Loss: 1.3862, Train Accuracy: 66.33%, Test Accuracy: 44.33%\n",
      "Epoch [590/2000], Loss: 1.3859, Train Accuracy: 66.33%, Test Accuracy: 43.67%\n",
      "Epoch [595/2000], Loss: 1.3860, Train Accuracy: 66.08%, Test Accuracy: 44.33%\n",
      "Epoch [600/2000], Loss: 1.3855, Train Accuracy: 66.25%, Test Accuracy: 44.33%\n",
      "Epoch [605/2000], Loss: 1.3851, Train Accuracy: 66.33%, Test Accuracy: 44.33%\n",
      "Epoch [610/2000], Loss: 1.3849, Train Accuracy: 66.33%, Test Accuracy: 44.33%\n",
      "Epoch [615/2000], Loss: 1.3846, Train Accuracy: 66.33%, Test Accuracy: 44.00%\n",
      "Epoch [620/2000], Loss: 1.3844, Train Accuracy: 66.33%, Test Accuracy: 43.67%\n",
      "Epoch [625/2000], Loss: 1.3842, Train Accuracy: 66.33%, Test Accuracy: 43.67%\n",
      "Epoch [630/2000], Loss: 1.3839, Train Accuracy: 66.33%, Test Accuracy: 43.67%\n",
      "Epoch [635/2000], Loss: 1.3837, Train Accuracy: 66.33%, Test Accuracy: 43.67%\n",
      "Epoch [640/2000], Loss: 1.3835, Train Accuracy: 66.42%, Test Accuracy: 43.67%\n",
      "Epoch [645/2000], Loss: 1.3833, Train Accuracy: 66.42%, Test Accuracy: 44.00%\n",
      "Epoch [650/2000], Loss: 1.3831, Train Accuracy: 66.42%, Test Accuracy: 44.00%\n",
      "Epoch [655/2000], Loss: 1.3829, Train Accuracy: 66.42%, Test Accuracy: 44.00%\n",
      "Epoch [660/2000], Loss: 1.3827, Train Accuracy: 66.42%, Test Accuracy: 44.00%\n",
      "Epoch [665/2000], Loss: 1.3825, Train Accuracy: 66.42%, Test Accuracy: 44.00%\n",
      "Epoch [670/2000], Loss: 1.3824, Train Accuracy: 66.42%, Test Accuracy: 44.00%\n",
      "Epoch [675/2000], Loss: 1.3822, Train Accuracy: 66.50%, Test Accuracy: 44.00%\n",
      "Epoch [680/2000], Loss: 1.3820, Train Accuracy: 66.42%, Test Accuracy: 44.00%\n",
      "Epoch [685/2000], Loss: 1.3818, Train Accuracy: 66.42%, Test Accuracy: 44.00%\n",
      "Epoch [690/2000], Loss: 1.3817, Train Accuracy: 66.50%, Test Accuracy: 44.00%\n",
      "Epoch [695/2000], Loss: 1.3815, Train Accuracy: 66.50%, Test Accuracy: 44.00%\n",
      "Epoch [700/2000], Loss: 1.3813, Train Accuracy: 66.42%, Test Accuracy: 44.00%\n",
      "Epoch [705/2000], Loss: 1.3812, Train Accuracy: 66.50%, Test Accuracy: 44.00%\n",
      "Epoch [710/2000], Loss: 1.3810, Train Accuracy: 66.50%, Test Accuracy: 44.00%\n",
      "Epoch [715/2000], Loss: 1.3808, Train Accuracy: 66.50%, Test Accuracy: 44.00%\n",
      "Epoch [720/2000], Loss: 1.3808, Train Accuracy: 66.50%, Test Accuracy: 44.33%\n",
      "Epoch [725/2000], Loss: 1.3805, Train Accuracy: 66.50%, Test Accuracy: 44.00%\n",
      "Epoch [730/2000], Loss: 1.3804, Train Accuracy: 66.58%, Test Accuracy: 44.00%\n",
      "Epoch [735/2000], Loss: 1.3802, Train Accuracy: 66.67%, Test Accuracy: 44.00%\n",
      "Epoch [740/2000], Loss: 1.3802, Train Accuracy: 66.58%, Test Accuracy: 44.33%\n",
      "Epoch [745/2000], Loss: 1.3799, Train Accuracy: 66.75%, Test Accuracy: 44.33%\n",
      "Epoch [750/2000], Loss: 1.3798, Train Accuracy: 66.75%, Test Accuracy: 44.00%\n",
      "Epoch [755/2000], Loss: 1.3796, Train Accuracy: 66.75%, Test Accuracy: 44.33%\n",
      "Epoch [760/2000], Loss: 1.3795, Train Accuracy: 66.75%, Test Accuracy: 44.33%\n",
      "Epoch [765/2000], Loss: 1.3794, Train Accuracy: 66.67%, Test Accuracy: 44.00%\n",
      "Epoch [770/2000], Loss: 1.3793, Train Accuracy: 66.75%, Test Accuracy: 44.33%\n",
      "Epoch [775/2000], Loss: 1.3790, Train Accuracy: 66.75%, Test Accuracy: 44.33%\n",
      "Epoch [780/2000], Loss: 1.3790, Train Accuracy: 66.83%, Test Accuracy: 44.67%\n",
      "Epoch [785/2000], Loss: 1.3787, Train Accuracy: 66.83%, Test Accuracy: 44.33%\n",
      "Epoch [790/2000], Loss: 1.3787, Train Accuracy: 66.92%, Test Accuracy: 45.00%\n",
      "Epoch [795/2000], Loss: 1.3783, Train Accuracy: 66.92%, Test Accuracy: 44.00%\n",
      "Epoch [800/2000], Loss: 1.3783, Train Accuracy: 66.92%, Test Accuracy: 44.33%\n",
      "Epoch [805/2000], Loss: 1.3780, Train Accuracy: 66.92%, Test Accuracy: 44.33%\n",
      "Epoch [810/2000], Loss: 1.3780, Train Accuracy: 66.92%, Test Accuracy: 44.33%\n",
      "Epoch [815/2000], Loss: 1.3777, Train Accuracy: 66.92%, Test Accuracy: 44.33%\n",
      "Epoch [820/2000], Loss: 1.3776, Train Accuracy: 66.92%, Test Accuracy: 44.67%\n",
      "Epoch [825/2000], Loss: 1.3774, Train Accuracy: 67.00%, Test Accuracy: 44.33%\n",
      "Epoch [830/2000], Loss: 1.3773, Train Accuracy: 66.92%, Test Accuracy: 45.00%\n",
      "Epoch [835/2000], Loss: 1.3772, Train Accuracy: 67.00%, Test Accuracy: 45.00%\n",
      "Epoch [840/2000], Loss: 1.3771, Train Accuracy: 67.00%, Test Accuracy: 44.67%\n",
      "Epoch [845/2000], Loss: 1.3769, Train Accuracy: 67.00%, Test Accuracy: 45.00%\n",
      "Epoch [850/2000], Loss: 1.3768, Train Accuracy: 67.00%, Test Accuracy: 45.00%\n",
      "Epoch [855/2000], Loss: 1.3767, Train Accuracy: 67.00%, Test Accuracy: 45.00%\n",
      "Epoch [860/2000], Loss: 1.3766, Train Accuracy: 67.00%, Test Accuracy: 45.00%\n",
      "Epoch [865/2000], Loss: 1.3765, Train Accuracy: 67.00%, Test Accuracy: 45.00%\n",
      "Epoch [870/2000], Loss: 1.3764, Train Accuracy: 67.00%, Test Accuracy: 45.00%\n",
      "Epoch [875/2000], Loss: 1.3763, Train Accuracy: 67.00%, Test Accuracy: 45.00%\n",
      "Epoch [880/2000], Loss: 1.3762, Train Accuracy: 67.00%, Test Accuracy: 45.00%\n",
      "Epoch [885/2000], Loss: 1.3761, Train Accuracy: 67.00%, Test Accuracy: 45.00%\n",
      "Epoch [890/2000], Loss: 1.3760, Train Accuracy: 67.00%, Test Accuracy: 45.00%\n",
      "Epoch [895/2000], Loss: 1.3759, Train Accuracy: 67.00%, Test Accuracy: 45.00%\n",
      "Epoch [900/2000], Loss: 1.3758, Train Accuracy: 67.00%, Test Accuracy: 45.00%\n",
      "Epoch [905/2000], Loss: 1.3757, Train Accuracy: 67.00%, Test Accuracy: 45.00%\n",
      "Epoch [910/2000], Loss: 1.3756, Train Accuracy: 67.00%, Test Accuracy: 45.00%\n",
      "Epoch [915/2000], Loss: 1.3755, Train Accuracy: 67.00%, Test Accuracy: 45.00%\n",
      "Epoch [920/2000], Loss: 1.3755, Train Accuracy: 67.00%, Test Accuracy: 45.00%\n",
      "Epoch [925/2000], Loss: 1.3754, Train Accuracy: 67.00%, Test Accuracy: 45.00%\n",
      "Epoch [930/2000], Loss: 1.3753, Train Accuracy: 67.00%, Test Accuracy: 45.00%\n",
      "Epoch [935/2000], Loss: 1.3752, Train Accuracy: 67.00%, Test Accuracy: 45.00%\n",
      "Epoch [940/2000], Loss: 1.3751, Train Accuracy: 67.00%, Test Accuracy: 45.00%\n",
      "Epoch [945/2000], Loss: 1.3751, Train Accuracy: 67.00%, Test Accuracy: 45.00%\n",
      "Epoch [950/2000], Loss: 1.3750, Train Accuracy: 67.00%, Test Accuracy: 45.00%\n",
      "Epoch [955/2000], Loss: 1.3749, Train Accuracy: 67.00%, Test Accuracy: 45.00%\n",
      "Epoch [960/2000], Loss: 1.3749, Train Accuracy: 67.00%, Test Accuracy: 45.00%\n",
      "Epoch [965/2000], Loss: 1.3748, Train Accuracy: 67.00%, Test Accuracy: 45.00%\n",
      "Epoch [970/2000], Loss: 1.3747, Train Accuracy: 67.00%, Test Accuracy: 45.00%\n",
      "Epoch [975/2000], Loss: 1.3747, Train Accuracy: 67.00%, Test Accuracy: 45.00%\n",
      "Epoch [980/2000], Loss: 1.3746, Train Accuracy: 67.00%, Test Accuracy: 44.67%\n",
      "Epoch [985/2000], Loss: 1.3745, Train Accuracy: 67.00%, Test Accuracy: 45.00%\n",
      "Epoch [990/2000], Loss: 1.3745, Train Accuracy: 67.00%, Test Accuracy: 45.00%\n",
      "Epoch [995/2000], Loss: 1.3744, Train Accuracy: 67.00%, Test Accuracy: 44.67%\n",
      "Epoch [1000/2000], Loss: 1.3744, Train Accuracy: 67.00%, Test Accuracy: 44.67%\n",
      "Epoch [1005/2000], Loss: 1.3743, Train Accuracy: 67.00%, Test Accuracy: 45.00%\n",
      "Epoch [1010/2000], Loss: 1.3742, Train Accuracy: 67.00%, Test Accuracy: 45.00%\n",
      "Epoch [1015/2000], Loss: 1.3742, Train Accuracy: 67.00%, Test Accuracy: 44.67%\n",
      "Epoch [1020/2000], Loss: 1.3741, Train Accuracy: 67.00%, Test Accuracy: 45.00%\n",
      "Epoch [1025/2000], Loss: 1.3741, Train Accuracy: 67.00%, Test Accuracy: 45.00%\n",
      "Epoch [1030/2000], Loss: 1.3740, Train Accuracy: 67.00%, Test Accuracy: 44.67%\n",
      "Epoch [1035/2000], Loss: 1.3740, Train Accuracy: 67.00%, Test Accuracy: 44.67%\n",
      "Epoch [1040/2000], Loss: 1.3739, Train Accuracy: 67.00%, Test Accuracy: 45.00%\n",
      "Epoch [1045/2000], Loss: 1.3739, Train Accuracy: 67.00%, Test Accuracy: 45.00%\n",
      "Epoch [1050/2000], Loss: 1.3738, Train Accuracy: 67.00%, Test Accuracy: 45.00%\n",
      "Epoch [1055/2000], Loss: 1.3738, Train Accuracy: 67.00%, Test Accuracy: 45.00%\n",
      "Epoch [1060/2000], Loss: 1.3737, Train Accuracy: 67.00%, Test Accuracy: 45.00%\n",
      "Epoch [1065/2000], Loss: 1.3737, Train Accuracy: 67.00%, Test Accuracy: 45.00%\n",
      "Epoch [1070/2000], Loss: 1.3736, Train Accuracy: 67.00%, Test Accuracy: 45.00%\n",
      "Epoch [1075/2000], Loss: 1.3736, Train Accuracy: 67.00%, Test Accuracy: 45.00%\n",
      "Epoch [1080/2000], Loss: 1.3735, Train Accuracy: 67.08%, Test Accuracy: 45.00%\n",
      "Epoch [1085/2000], Loss: 1.3734, Train Accuracy: 67.08%, Test Accuracy: 45.00%\n",
      "Epoch [1090/2000], Loss: 1.3734, Train Accuracy: 67.08%, Test Accuracy: 45.00%\n",
      "Epoch [1095/2000], Loss: 1.3733, Train Accuracy: 67.08%, Test Accuracy: 45.00%\n",
      "Epoch [1100/2000], Loss: 1.3732, Train Accuracy: 67.08%, Test Accuracy: 45.00%\n",
      "Epoch [1105/2000], Loss: 1.3732, Train Accuracy: 67.08%, Test Accuracy: 45.00%\n",
      "Epoch [1110/2000], Loss: 1.3731, Train Accuracy: 67.08%, Test Accuracy: 45.00%\n",
      "Epoch [1115/2000], Loss: 1.3730, Train Accuracy: 67.08%, Test Accuracy: 45.00%\n",
      "Epoch [1120/2000], Loss: 1.3729, Train Accuracy: 67.08%, Test Accuracy: 45.00%\n",
      "Epoch [1125/2000], Loss: 1.3729, Train Accuracy: 67.08%, Test Accuracy: 45.00%\n",
      "Epoch [1130/2000], Loss: 1.3729, Train Accuracy: 67.08%, Test Accuracy: 45.00%\n",
      "Epoch [1135/2000], Loss: 1.3737, Train Accuracy: 67.17%, Test Accuracy: 45.00%\n",
      "Epoch [1140/2000], Loss: 1.3731, Train Accuracy: 67.17%, Test Accuracy: 45.00%\n",
      "Epoch [1145/2000], Loss: 1.3729, Train Accuracy: 67.08%, Test Accuracy: 45.00%\n",
      "Epoch [1150/2000], Loss: 1.3728, Train Accuracy: 67.08%, Test Accuracy: 45.00%\n",
      "Epoch [1155/2000], Loss: 1.3726, Train Accuracy: 67.08%, Test Accuracy: 45.00%\n",
      "Epoch [1160/2000], Loss: 1.3723, Train Accuracy: 67.17%, Test Accuracy: 45.33%\n",
      "Epoch [1165/2000], Loss: 1.3720, Train Accuracy: 67.17%, Test Accuracy: 45.33%\n",
      "Epoch [1170/2000], Loss: 1.3718, Train Accuracy: 67.17%, Test Accuracy: 45.67%\n",
      "Epoch [1175/2000], Loss: 1.3717, Train Accuracy: 67.17%, Test Accuracy: 45.67%\n",
      "Epoch [1180/2000], Loss: 1.3716, Train Accuracy: 67.25%, Test Accuracy: 45.67%\n",
      "Epoch [1185/2000], Loss: 1.3716, Train Accuracy: 67.25%, Test Accuracy: 45.67%\n",
      "Epoch [1190/2000], Loss: 1.3715, Train Accuracy: 67.25%, Test Accuracy: 45.67%\n",
      "Epoch [1195/2000], Loss: 1.3715, Train Accuracy: 67.25%, Test Accuracy: 45.67%\n",
      "Epoch [1200/2000], Loss: 1.3714, Train Accuracy: 67.25%, Test Accuracy: 45.67%\n",
      "Epoch [1205/2000], Loss: 1.3713, Train Accuracy: 67.25%, Test Accuracy: 45.67%\n",
      "Epoch [1210/2000], Loss: 1.3713, Train Accuracy: 67.25%, Test Accuracy: 45.67%\n",
      "Epoch [1215/2000], Loss: 1.3713, Train Accuracy: 67.25%, Test Accuracy: 45.67%\n",
      "Epoch [1220/2000], Loss: 1.3712, Train Accuracy: 67.25%, Test Accuracy: 45.67%\n",
      "Epoch [1225/2000], Loss: 1.3712, Train Accuracy: 67.25%, Test Accuracy: 45.67%\n",
      "Epoch [1230/2000], Loss: 1.3711, Train Accuracy: 67.25%, Test Accuracy: 45.67%\n",
      "Epoch [1235/2000], Loss: 1.3711, Train Accuracy: 67.25%, Test Accuracy: 45.67%\n",
      "Epoch [1240/2000], Loss: 1.3711, Train Accuracy: 67.25%, Test Accuracy: 45.67%\n",
      "Epoch [1245/2000], Loss: 1.3711, Train Accuracy: 67.25%, Test Accuracy: 45.67%\n",
      "Epoch [1250/2000], Loss: 1.3710, Train Accuracy: 67.25%, Test Accuracy: 45.67%\n",
      "Epoch [1255/2000], Loss: 1.3710, Train Accuracy: 67.25%, Test Accuracy: 45.67%\n",
      "Epoch [1260/2000], Loss: 1.3709, Train Accuracy: 67.25%, Test Accuracy: 45.67%\n",
      "Epoch [1265/2000], Loss: 1.3709, Train Accuracy: 67.25%, Test Accuracy: 45.33%\n",
      "Epoch [1270/2000], Loss: 1.3709, Train Accuracy: 67.25%, Test Accuracy: 45.67%\n",
      "Epoch [1275/2000], Loss: 1.3708, Train Accuracy: 67.25%, Test Accuracy: 45.67%\n",
      "Epoch [1280/2000], Loss: 1.3708, Train Accuracy: 67.25%, Test Accuracy: 45.33%\n",
      "Epoch [1285/2000], Loss: 1.3708, Train Accuracy: 67.25%, Test Accuracy: 45.67%\n",
      "Epoch [1290/2000], Loss: 1.3707, Train Accuracy: 67.25%, Test Accuracy: 45.33%\n",
      "Epoch [1295/2000], Loss: 1.3707, Train Accuracy: 67.25%, Test Accuracy: 45.67%\n",
      "Epoch [1300/2000], Loss: 1.3707, Train Accuracy: 67.25%, Test Accuracy: 45.33%\n",
      "Epoch [1305/2000], Loss: 1.3707, Train Accuracy: 67.25%, Test Accuracy: 45.33%\n",
      "Epoch [1310/2000], Loss: 1.3706, Train Accuracy: 67.25%, Test Accuracy: 44.67%\n",
      "Epoch [1315/2000], Loss: 1.3706, Train Accuracy: 67.25%, Test Accuracy: 44.67%\n",
      "Epoch [1320/2000], Loss: 1.3706, Train Accuracy: 67.25%, Test Accuracy: 44.67%\n",
      "Epoch [1325/2000], Loss: 1.3705, Train Accuracy: 67.25%, Test Accuracy: 44.67%\n",
      "Epoch [1330/2000], Loss: 1.3704, Train Accuracy: 67.25%, Test Accuracy: 44.67%\n",
      "Epoch [1335/2000], Loss: 1.3702, Train Accuracy: 67.33%, Test Accuracy: 44.67%\n",
      "Epoch [1340/2000], Loss: 1.3700, Train Accuracy: 67.33%, Test Accuracy: 45.00%\n",
      "Epoch [1345/2000], Loss: 1.3702, Train Accuracy: 67.33%, Test Accuracy: 44.67%\n",
      "Epoch [1350/2000], Loss: 1.3700, Train Accuracy: 67.33%, Test Accuracy: 45.00%\n",
      "Epoch [1355/2000], Loss: 1.3699, Train Accuracy: 67.33%, Test Accuracy: 44.67%\n",
      "Epoch [1360/2000], Loss: 1.3698, Train Accuracy: 67.33%, Test Accuracy: 44.67%\n",
      "Epoch [1365/2000], Loss: 1.3697, Train Accuracy: 67.33%, Test Accuracy: 44.67%\n",
      "Epoch [1370/2000], Loss: 1.3697, Train Accuracy: 67.33%, Test Accuracy: 44.67%\n",
      "Epoch [1375/2000], Loss: 1.3696, Train Accuracy: 67.33%, Test Accuracy: 44.67%\n",
      "Epoch [1380/2000], Loss: 1.3696, Train Accuracy: 67.33%, Test Accuracy: 44.67%\n",
      "Epoch [1385/2000], Loss: 1.3696, Train Accuracy: 67.33%, Test Accuracy: 44.67%\n",
      "Epoch [1390/2000], Loss: 1.3695, Train Accuracy: 67.33%, Test Accuracy: 44.67%\n",
      "Epoch [1395/2000], Loss: 1.3695, Train Accuracy: 67.33%, Test Accuracy: 44.67%\n",
      "Epoch [1400/2000], Loss: 1.3695, Train Accuracy: 67.33%, Test Accuracy: 44.67%\n",
      "Epoch [1405/2000], Loss: 1.3694, Train Accuracy: 67.33%, Test Accuracy: 44.67%\n",
      "Epoch [1410/2000], Loss: 1.3694, Train Accuracy: 67.33%, Test Accuracy: 44.67%\n",
      "Epoch [1415/2000], Loss: 1.3694, Train Accuracy: 67.33%, Test Accuracy: 44.67%\n",
      "Epoch [1420/2000], Loss: 1.3694, Train Accuracy: 67.33%, Test Accuracy: 44.67%\n",
      "Epoch [1425/2000], Loss: 1.3694, Train Accuracy: 67.33%, Test Accuracy: 44.67%\n",
      "Epoch [1430/2000], Loss: 1.3693, Train Accuracy: 67.33%, Test Accuracy: 44.67%\n",
      "Epoch [1435/2000], Loss: 1.3693, Train Accuracy: 67.33%, Test Accuracy: 44.67%\n",
      "Epoch [1440/2000], Loss: 1.3693, Train Accuracy: 67.33%, Test Accuracy: 44.67%\n",
      "Epoch [1445/2000], Loss: 1.3693, Train Accuracy: 67.33%, Test Accuracy: 45.00%\n",
      "Epoch [1450/2000], Loss: 1.3693, Train Accuracy: 67.33%, Test Accuracy: 45.00%\n",
      "Epoch [1455/2000], Loss: 1.3692, Train Accuracy: 67.33%, Test Accuracy: 44.67%\n",
      "Epoch [1460/2000], Loss: 1.3692, Train Accuracy: 67.33%, Test Accuracy: 45.00%\n",
      "Epoch [1465/2000], Loss: 1.3692, Train Accuracy: 67.33%, Test Accuracy: 45.00%\n",
      "Epoch [1470/2000], Loss: 1.3692, Train Accuracy: 67.33%, Test Accuracy: 45.00%\n",
      "Epoch [1475/2000], Loss: 1.3692, Train Accuracy: 67.33%, Test Accuracy: 45.00%\n",
      "Epoch [1480/2000], Loss: 1.3691, Train Accuracy: 67.33%, Test Accuracy: 44.67%\n",
      "Epoch [1485/2000], Loss: 1.3691, Train Accuracy: 67.33%, Test Accuracy: 44.67%\n",
      "Epoch [1490/2000], Loss: 1.3691, Train Accuracy: 67.33%, Test Accuracy: 44.67%\n",
      "Epoch [1495/2000], Loss: 1.3691, Train Accuracy: 67.33%, Test Accuracy: 44.67%\n",
      "Epoch [1500/2000], Loss: 1.3691, Train Accuracy: 67.33%, Test Accuracy: 44.67%\n",
      "Epoch [1505/2000], Loss: 1.3691, Train Accuracy: 67.33%, Test Accuracy: 44.67%\n",
      "Epoch [1510/2000], Loss: 1.3690, Train Accuracy: 67.33%, Test Accuracy: 44.67%\n",
      "Epoch [1515/2000], Loss: 1.3690, Train Accuracy: 67.33%, Test Accuracy: 44.67%\n",
      "Epoch [1520/2000], Loss: 1.3690, Train Accuracy: 67.33%, Test Accuracy: 44.67%\n",
      "Epoch [1525/2000], Loss: 1.3690, Train Accuracy: 67.33%, Test Accuracy: 44.67%\n",
      "Epoch [1530/2000], Loss: 1.3690, Train Accuracy: 67.33%, Test Accuracy: 44.67%\n",
      "Epoch [1535/2000], Loss: 1.3690, Train Accuracy: 67.33%, Test Accuracy: 44.67%\n",
      "Epoch [1540/2000], Loss: 1.3689, Train Accuracy: 67.33%, Test Accuracy: 44.67%\n",
      "Epoch [1545/2000], Loss: 1.3689, Train Accuracy: 67.33%, Test Accuracy: 44.67%\n",
      "Epoch [1550/2000], Loss: 1.3689, Train Accuracy: 67.33%, Test Accuracy: 44.67%\n",
      "Epoch [1555/2000], Loss: 1.3689, Train Accuracy: 67.33%, Test Accuracy: 44.67%\n",
      "Epoch [1560/2000], Loss: 1.3686, Train Accuracy: 67.42%, Test Accuracy: 45.00%\n",
      "Epoch [1565/2000], Loss: 1.3683, Train Accuracy: 67.42%, Test Accuracy: 45.00%\n",
      "Epoch [1570/2000], Loss: 1.3683, Train Accuracy: 67.42%, Test Accuracy: 45.00%\n",
      "Epoch [1575/2000], Loss: 1.3682, Train Accuracy: 67.42%, Test Accuracy: 45.00%\n",
      "Epoch [1580/2000], Loss: 1.3682, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1585/2000], Loss: 1.3681, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1590/2000], Loss: 1.3681, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1595/2000], Loss: 1.3681, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1600/2000], Loss: 1.3680, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1605/2000], Loss: 1.3680, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1610/2000], Loss: 1.3680, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1615/2000], Loss: 1.3680, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1620/2000], Loss: 1.3680, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1625/2000], Loss: 1.3680, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1630/2000], Loss: 1.3679, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1635/2000], Loss: 1.3679, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1640/2000], Loss: 1.3679, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1645/2000], Loss: 1.3679, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1650/2000], Loss: 1.3679, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1655/2000], Loss: 1.3679, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1660/2000], Loss: 1.3679, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1665/2000], Loss: 1.3679, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1670/2000], Loss: 1.3679, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1675/2000], Loss: 1.3678, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1680/2000], Loss: 1.3678, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1685/2000], Loss: 1.3678, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1690/2000], Loss: 1.3678, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1695/2000], Loss: 1.3678, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1700/2000], Loss: 1.3678, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1705/2000], Loss: 1.3678, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1710/2000], Loss: 1.3678, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1715/2000], Loss: 1.3677, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1720/2000], Loss: 1.3677, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1725/2000], Loss: 1.3677, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1730/2000], Loss: 1.3677, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1735/2000], Loss: 1.3677, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1740/2000], Loss: 1.3677, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1745/2000], Loss: 1.3677, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1750/2000], Loss: 1.3677, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1755/2000], Loss: 1.3677, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1760/2000], Loss: 1.3677, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1765/2000], Loss: 1.3677, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1770/2000], Loss: 1.3676, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1775/2000], Loss: 1.3676, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1780/2000], Loss: 1.3676, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1785/2000], Loss: 1.3676, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1790/2000], Loss: 1.3676, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1795/2000], Loss: 1.3676, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1800/2000], Loss: 1.3676, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1805/2000], Loss: 1.3676, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1810/2000], Loss: 1.3676, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1815/2000], Loss: 1.3676, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1820/2000], Loss: 1.3676, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1825/2000], Loss: 1.3676, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1830/2000], Loss: 1.3675, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1835/2000], Loss: 1.3675, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1840/2000], Loss: 1.3676, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1845/2000], Loss: 1.3675, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1850/2000], Loss: 1.3675, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1855/2000], Loss: 1.3675, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1860/2000], Loss: 1.3675, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1865/2000], Loss: 1.3675, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1870/2000], Loss: 1.3675, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1875/2000], Loss: 1.3675, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1880/2000], Loss: 1.3675, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1885/2000], Loss: 1.3675, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1890/2000], Loss: 1.3675, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1895/2000], Loss: 1.3675, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1900/2000], Loss: 1.3675, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1905/2000], Loss: 1.3675, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1910/2000], Loss: 1.3674, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1915/2000], Loss: 1.3674, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1920/2000], Loss: 1.3674, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1925/2000], Loss: 1.3674, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1930/2000], Loss: 1.3674, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1935/2000], Loss: 1.3674, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1940/2000], Loss: 1.3674, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1945/2000], Loss: 1.3672, Train Accuracy: 67.50%, Test Accuracy: 44.33%\n",
      "Epoch [1950/2000], Loss: 1.3680, Train Accuracy: 67.50%, Test Accuracy: 45.00%\n",
      "Epoch [1955/2000], Loss: 1.3722, Train Accuracy: 66.33%, Test Accuracy: 45.33%\n",
      "Epoch [1960/2000], Loss: 1.3755, Train Accuracy: 66.92%, Test Accuracy: 46.33%\n",
      "Epoch [1965/2000], Loss: 1.3688, Train Accuracy: 67.17%, Test Accuracy: 45.33%\n",
      "Epoch [1970/2000], Loss: 1.3726, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1975/2000], Loss: 1.3684, Train Accuracy: 67.42%, Test Accuracy: 44.67%\n",
      "Epoch [1980/2000], Loss: 1.3683, Train Accuracy: 67.42%, Test Accuracy: 45.00%\n",
      "Epoch [1985/2000], Loss: 1.3683, Train Accuracy: 67.50%, Test Accuracy: 44.00%\n",
      "Epoch [1990/2000], Loss: 1.3674, Train Accuracy: 67.50%, Test Accuracy: 44.33%\n",
      "Epoch [1995/2000], Loss: 1.3673, Train Accuracy: 67.50%, Test Accuracy: 45.33%\n",
      "Epoch [2000/2000], Loss: 1.3670, Train Accuracy: 67.50%, Test Accuracy: 44.67%\n"
     ]
    }
   ],
   "source": [
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_torch = torch.LongTensor(y_train)  # Assuming y_train is a pandas series\n",
    "\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_torch = torch.LongTensor(y_test)  # Assuming y_test is a pandas series\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 128\n",
    "num_classes = 6\n",
    "num_epochs = 2000\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model, Loss, and Optimizer\n",
    "model = CombineScoresNet(input_size, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_torch)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 5 == 0:\n",
    "        train_accuracy = compute_accuracy(model, X_train_tensor, y_train_torch)\n",
    "        test_accuracy = compute_accuracy(model, X_test_tensor, y_test_torch)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Train Accuracy: {train_accuracy:.2f}%, Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
